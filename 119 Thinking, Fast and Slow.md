# Cortex - 119: Thinking, Fast and Slow
**00:00:00** Don't be alarmed, Mike.

**00:00:01** Oh, that's a terrible way to start.

**00:00:03** I don't like this.

**00:00:04** I am recording this podcast on the beta, on the Mac beta.

**00:00:10** Why?

**00:00:11** Now, just to be clear, not because I want to, but because I have to.

**00:00:16** You don't have to.

**00:00:17** No, I do have to.

**00:00:18** Why?

**00:00:18** All right.

**00:00:19** So-

**00:00:19** Why do you cause me so much stress?

**00:00:21** I don't, I'm not causing, I'm letting you know that you don't have to be stressed,

**00:00:25** because I have the backup recording going, which is physically separate from everything else.

**00:00:31** And it will be perfectly fine as long as the batteries don't run down,

**00:00:34** but I'd freshly changed them just before this episode.

**00:00:37** So the chance of that happening is low, assuming I picked the right batteries

**00:00:42** and didn't pick dead batteries.

**00:00:43** Just keep your eye on the recorder would be my request.

**00:00:46** I'm sure it gives you some kind of indication when the battery's low and going low.

**00:00:49** Yeah, it turns off.

**00:00:50** So I've got it right in front of me so I can see if that happens.

**00:00:54** But no, I'm not, I'm not doing this on purpose.

**00:00:56** This isn't fun levels, levels shenanigans.

**00:00:59** What happened is my writing computer has become, I guess the way to describe it is

**00:01:09** it has a like a synchronization corruption in Dropbox that is causing me problems.

**00:01:17** And so I have had to quarantine my writing computer from any kind of network access.

**00:01:25** So that's all of the work that I do doesn't get messed up.

**00:01:29** The only other computers that I have are the laptops where I'm running the betas.

**00:01:33** And so that's why I'm talking to you from the beta right now.

**00:01:39** I'll allow it.

**00:01:40** Okay, he'll allow it.

**00:01:41** I'll allow it.

**00:01:42** Thank you.

**00:01:42** I mean, I know you're having this problem because sometimes,

**00:01:46** like after our last episode, I had to text you and be like, where's the file?

**00:01:50** So was it happening from then?

**00:01:52** Yes.

**00:01:52** So our last episode was the thing that finally clued me into,

**00:01:58** hey buddy, you've got some problem in your system.

**00:02:01** And you kept texting me like, where's the file?

**00:02:04** And I kept checking on my writing computer, which is also the podcasting computer,

**00:02:08** and seeing files there.

**00:02:10** It says uploading.

**00:02:11** It should be with you any moment, Mike.

**00:02:13** But as your increasingly frequent messages conveyed to me, the file was not showing up.

**00:02:19** This always happens when I need the file quickly.

**00:02:22** And I did need the file quickly after our last episode,

**00:02:25** because we were under a bit of a time crunch.

**00:02:27** Because I wanted to start editing straight away,

**00:02:29** which is one of the worst things you can do.

**00:02:31** Like with the way that I edit this show,

**00:02:33** that we just had the conversation now.

**00:02:34** I'm literally going to listen back to all of it immediately, which is like...

**00:02:38** Oh, that's the worst.

**00:02:39** That sucks.

**00:02:40** It's nice to have a couple of days at least.

**00:02:43** Which I also, I will say, I feel like it fits better.

**00:02:46** After a couple of days when I come back to that episode,

**00:02:48** not only is some of it refreshing,

**00:02:51** I've also, my brain has been working on it a little bit more.

**00:02:54** And I find that I'm able to immediately notice the parts that I know didn't work

**00:02:59** in a way that I don't get that after for some reason.

**00:03:02** Like if I go straight into the episode,

**00:03:04** there's this weird thing where I feel like my brain is just chewing on the conversation a bit,

**00:03:09** which is interesting.

**00:03:10** But yeah, so I always, this always happens.

**00:03:13** And this might be like a selection effect kind of thing.

**00:03:16** Like maybe there's always problems,

**00:03:17** but I don't usually notice them because they resolve themselves after a day or two.

**00:03:21** Usually my fault is just simply not turning back on Dropbox

**00:03:25** after the conversation is over.

**00:03:26** That's the fault most of the times.

**00:03:28** But this time, okay, so what is happening?

**00:03:31** I don't think it's Dropbox's fault.

**00:03:34** I do think it's ultimately my fault,

**00:03:37** but what is the chain of events?

**00:03:40** As far as I was able to reconstruct this incident

**00:03:43** is that I keep a local copy of all of my Dropbox files on one of these

**00:03:50** giant Pegasus drive things that's under my desk.

**00:03:54** You know, one of these like we can hold 50 terabytes of data kind of drives.

**00:03:57** They spin in hard drives or use SSDs in there.

**00:03:59** I don't know.

**00:04:00** They're those funny shaped other drives that I only see in server stuff.

**00:04:04** I don't actually know if they're spinning drives on the inside or if their SSDs on the inside.

**00:04:07** Does your Pegasus thing make any noise?

**00:04:10** It does, but it doesn't make spinny hard drive kinds of noises.

**00:04:13** It makes electrical kind of noises.

**00:04:15** So I'm going to guess their solid state, but I don't know for sure.

**00:04:18** But so precisely you've now identified where does the problem begin because it does make noise.

**00:04:22** I've wanted to have it outside the acoustically separated writing computer,

**00:04:27** which means that I need to run a wire from the Pegasus to the computer.

**00:04:33** But of course, I also have a standing desk.

**00:04:36** And so I set up the situation so that the wire could

**00:04:40** just reach when the standing desk was at the highest level.

**00:04:45** Great. Really good. Really good stuff.

**00:04:50** And this way, the Pegasus drive could be outside the little recording booth that I've made.

**00:04:55** And also I could raise and lower the standing desk as long as I was using the preset memory

**00:05:03** heights for the standing desk.

**00:05:04** Hey, what's the issue with having your massive storage solution,

**00:05:08** the cable for it just under slight tension constantly?

**00:05:11** What's the issue with that? No problem. That wouldn't cause any issues.

**00:05:14** Well, and it hasn't caused any issues for my entire quarantine year.

**00:05:18** Yeah.

**00:05:19** But just before our last recording, I was attempting to redo some of the wires behind my desk.

**00:05:29** And while I was doing that, I thought, oh, I need a little bit more space

**00:05:33** under the standing desk while I'm working here. And so I pressed the up button and right out popped

**00:05:39** the cable. Now Dropbox was running at the time. And just to give people a sense of the scale of

**00:05:46** the thing, I checked this morning and I have 20 terabytes of data in about a million files

**00:05:54** in my Dropbox system. That's a lot of terabytes.

**00:05:57** It's a lot of terabytes. It's a lot of files. Obviously, that's partly because I'm

**00:06:02** sharing documents with a bunch of people and like people that I work with. And so there's like,

**00:06:06** there's just a ton of stuff in there. But what happened is after the cable got pulled out,

**00:06:13** well, I plugged it back in and I thought, hopefully nothing bad happened.

**00:06:19** But obviously something bad did happen because Dropbox started to re-index the entirety of those

**00:06:26** 20 terabytes and million files.

**00:06:28** Wait, so do you have, I just want to make sure I've got this right. So you have all this stuff

**00:06:33** on the Pegasus Drive. Yeah.

**00:06:35** And that's going up to Dropbox as well.

**00:06:37** No, it's all in Dropbox.

**00:06:38** What's on the Pegasus Drive?

**00:06:40** The Pegasus Drive is where I have my Dropbox folder.

**00:06:44** And I've told that Dropbox folder, keep everything saved locally.

**00:06:48** Right. Okay. And then on your other machines, you're doing that, like,

**00:06:51** download it when you need it.

**00:06:52** I'm doing the selective syncing thing.

**00:06:54** Do you use Selective Sync or do you use the Smart Sync thing?

**00:06:58** So I've, well, I'm slightly changing the way I work now because of this very problem.

**00:07:03** But previously I was using Selective Sync where you can tell it just pretend like these folders

**00:07:08** don't exist. And part of the reason I was doing that is because every time I would install

**00:07:12** Dropbox on a new computer, it would give me this message that said, hey buddy, you have more than

**00:07:17** 500,000 files in your system. We strongly recommend you don't try to synchronize all of this,

**00:07:22** like just use Selective Sync for what you need. And since the laptops only have a terabyte of

**00:07:27** data or whatever, I would have to do that. And also the old way they used to work about

**00:07:33** selecting files to be local or not local, but still visible used to not work with Time Machine,

**00:07:38** but they seem to have fixed that. It does seem to work with Time Machine now.

**00:07:41** I think I'm having an issue with Dropbox and Time Machine.

**00:07:43** Okay. Yeah. What do you mean?

**00:07:44** My Time Machine backup keeps failing and it's telling me it needs to back up four terabytes

**00:07:50** of stuff. My iMac has a one terabyte SSD in it. So I don't know where it's drawing four terabytes

**00:07:56** of stuff from. Yeah. This is the kind of thing that you can run into.

**00:07:59** My Time Machine isn't working and I think it's related to my Dropbox because I have like three

**00:08:05** terabytes of stuff in Dropbox. Yeah. I'd easily bet that that's what this is. Ideally, the Dropbox

**00:08:11** should register all of the files that aren't there locally as zero bytes in size. But there are funny

**00:08:18** things that can happen when you have files that reference other files and like all sorts of

**00:08:22** complications there. I think maybe Time Machine is actually not a thing I can do anymore.

**00:08:26** That's the situation. Everything was local on the Pegasus and then it got disconnected

**00:08:31** while Dropbox was running and then Dropbox attempted to re-index things. It seemed to be

**00:08:38** going fine until Dropbox. So the number is dropping like indexing a million files,

**00:08:45** indexing 900,000 files. I was like, okay, well, this will just take a while.

**00:08:50** But it got down to about 400,000 files and then just stopped. And so Dropbox kept saying indexing

**00:08:58** 400,000 files and also had this hilarious like uploading 200,000 files. But I could check the

**00:09:05** network access and see you're not doing anything Dropbox. There's no data coming out of this

**00:09:10** computer. You're stuck. You're just stuck in this current position. So I thought, well,

**00:09:17** these are a lot of files. It's a lot of data. I'm sure I can just wait long enough and this

**00:09:22** problem will sort itself out. No, it didn't. I've been waiting five, six weeks now and it just

**00:09:30** didn't move at all. But what was happening is that my writing computer was downloading new stuff

**00:09:37** from my other computers that I was working on. And so I thought, oh, okay, I guess everything

**00:09:44** is staying in sync. But I've only just noticed in the past couple of days that it isn't that the

**00:09:52** newer computers keep trying to revert to the way Dropbox was like a month ago before this happened

**00:09:59** and disappearing stuff that I've worked on. So I was like, so I realized, oh, no, indeed is what I

**00:10:10** realized. I thought, okay, writing computer shut down immediately. Goodbye. Like you are not getting

**00:10:18** network access ever again until I can create some kind of Faraday cage around it to boot it up and

**00:10:25** probably just like wipe the whole machine and start over if I'm going to use it for something else.

**00:10:28** I think that this situation compounded with other problems you have have a Dropbox in the past,

**00:10:35** I think is suggesting that you need a slightly different system than the one that you're using.

**00:10:40** I feel like there needs to be like a cold storage, which isn't connected to Dropbox anymore. I think

**00:10:47** 20 terabytes is too much to put in Dropbox. Yeah. I think you're always going to have these problems.

**00:10:53** I mean, maybe I would be curious to know among Dropbox's enterprise users, where do I rank

**00:10:58** percentage wise in terms of amount of data that is being used? I feel like there have to be

**00:11:01** organizations that are using 100 times more than I'm using for Dropbox. Yeah, but it's probably all

**00:11:06** not for one person. Right. But this is where it's coming from. It's team stuff as well,

**00:11:11** or working with other people, like even between the two of us, right? Like we both have access to

**00:11:15** a copy of all of the Cortex files. Yeah, but I don't think that people even in like large

**00:11:20** organizations are sharing that volume of data between them, right? Like maybe an organization

**00:11:25** has 20 terabytes, but each individual only has a small percentage of that overall thing.

**00:11:31** Right. That they have, I see what you're saying that they have access to, that there's not any

**00:11:34** individual who's trying to keep on top of 20 terabytes worth of stuff. Okay. I see what you're

**00:11:40** saying. I think particularly the way that you're doing it of like 20 terabytes of external storage,

**00:11:46** I think that that's like compounding the potential risk. What could go wrong? All of the data being

**00:11:53** stored in that one place is probably a bad, bad idea. Right. Well, I mean, part of the reason why

**00:11:58** I wanted a local copy of all of it is so that I can make my own backups and not just trust Dropbox

**00:12:03** to have all of the files all the time. Like that's what I'm just trying to do there. But no,

**00:12:08** I completely understand why you would do that. If I was okay, like this isn't necessarily helpful

**00:12:13** as such, because it's like so hard to get to this. But like, if I was going to re-architect what

**00:12:18** you're doing, I think you need to have like, there's the Dropbox active storage. And then there

**00:12:24** isn't like an away from Dropbox storage, which is physically on a thing that you have in your home.

**00:12:32** But then that is also backed up to another service like, like Backblaze or something. So

**00:12:38** you have an online backup for it. And it's accessible to you, but not constantly like

**00:12:43** Dropbox is I don't think you need that, I would expect, like at your fingertips on every single

**00:12:49** computer that you have at any moment. Like really easy. Yeah, because like even with Backblaze,

**00:12:54** if you had it all there, you could log into Backblaze and download that data on any machine,

**00:13:00** but it's not like in a folder structure in Finder. I think you're putting too much stuff

**00:13:07** through that system. Yes. I mean, possibly precisely because of the moment that we're in

**00:13:13** right now. I think you've proven it because I think millions of files up and down, it's just,

**00:13:19** it's going to get like, you only need one out of a million to have some kind of weirdness to it.

**00:13:27** Yes. And then you're in this situation and I don't know how you wouldn't just

**00:13:31** continue to get in these situations forever unless you change something about the way you store files.

**00:13:36** Yeah, I mean, that is my suspicion is that some file has become corrupted in an odd way

**00:13:42** that doesn't allow Dropbox to continue to index it. And that's where the system is just getting

**00:13:46** stuck. And then it's trying to keep track of new things, but also the synchronization status

**00:13:52** keeps seeming like, oh no, these new files don't exist because I'm currently running on this

**00:13:57** machine where they don't. And this machine is up to date because I haven't finished the indexing.

**00:14:02** I noticed it because I was working on a, like a, just a really dumb little vlog. And then I went

**00:14:06** on my laptop to go edit it. And it's like, this file doesn't exist. This entire Final Cut project

**00:14:12** that's gigabytes in size, never heard of it. It's like, oh God.

**00:14:18** It's kind of funny to me because like on Twitter, a couple of days ago, I saw a conversation between

**00:14:22** Hank Green and MKBHD where they were talking about the fact that once they upload videos,

**00:14:27** they delete everything. The only thing that exists is what's uploaded to YouTube. They don't keep

**00:14:32** anything. Yeah. I mean, I'm just going to say though, I think that does make more sense for

**00:14:37** both of them. Like I think that's a quite a reasonable workflow. I think there's a little

**00:14:41** bit of a difference in the content thereof. I do want to keep the originals of everything.

**00:14:45** What surprised me about MKBHD was that surely he needs B-roll footage sometimes.

**00:14:51** So maybe he saves a little bit of that. I don't know, but that, that was a surprise to me. But

**00:14:55** you know, like as I've said many times, the only shows that I keep the project files for is this

**00:15:01** one. I don't keep anything more than like a month or two after a month or two, I delete all the

**00:15:06** project files and then just carry on with my other shows except for Cortex. I have every logic

**00:15:11** project of Cortex. I never thought it was going to be useful until more text. It was so useful.

**00:15:17** So you needed to remaster everything for more text. Yeah, remove all of the ads from the entire

**00:15:21** back catalogue, which is a feature of More Text by the way. If you go to getmoretext.com,

**00:15:25** not only do you get additional content for every episode and no ads, you get an entirely ad-free

**00:15:30** remastered back catalogue. It's just higher audio quality. Yeah. And, and it's for that same

**00:15:35** reason that I do want to keep all of the video projects. I do understand it. Like I'm not saying

**00:15:39** that you're wrong for doing it. It was just interesting to me, but I think Dropbox is not

**00:15:43** the place for that. Yeah. I mean, maybe I need to figure out something else, but the,

**00:15:48** the problem of different people's versions, even for the old stuff, getting out of sync

**00:15:54** is a non-trivial issue for if you ever do need to reconstruct what is the current state of this

**00:16:00** thing. I'll have to think about it, but all of that is just to tell you why and how I'm recording

**00:16:07** to you from the beta currently. And is also another continuing step of the saga of my writing

**00:16:15** computer, which I think maybe the lesson learned is that the writing computer should not also be

**00:16:22** the server for every file you have. Really? And also a standing desk computer.

**00:16:30** It's almost as if, if you were going to try and sequester a machine to do one thing,

**00:16:34** you don't make it do everything. It's funny that really. Yeah. I think, I think that's the lesson

**00:16:39** we've all learned today. This episode of Cortex is brought to you by Muse. Muse is a tool for

**00:16:46** thought on iPad. It gives you a spatial canvas for your research notes, your sketches, screenshots,

**00:16:52** bookmarks, PDFs, and so much more. The Muse team believes that deep thinking doesn't happen in front

**00:16:58** of a computer. So Muse turns your iPad into a space inspired by your desk, letting you be

**00:17:04** personal, creative, and even a little bit messy. You can put anything on a Muse board. You can pull

**00:17:11** in relevant information from the web, email, Twitter, Slack, your files, notes, or photos from

**00:17:16** your phone, and then just arrange it however you like. Muse lets you sift and sort through it all,

**00:17:21** helping you find new patterns and insights. There are times when I come across user interfaces that

**00:17:27** I just enjoy playing with, and Muse is one of those. It does some things that feel really

**00:17:32** natural for how you want to interact with your iPad, making like this bridge between physical

**00:17:36** and digital. You're able to freely place things wherever you want, and you can move them around,

**00:17:41** how you see fit, make them bigger, smaller. It's super intuitive and fun to use, and it's an app

**00:17:46** where you're actually using both hands at the same time, which I really enjoy. Like this is a tool

**00:17:51** that I'm going to be using when brainstorming product ideas in the future. It also feels like

**00:17:56** really perfect for creating things like mood boards. Visit Muse app. That's M-U-S-E-A-P-P.com

**00:18:04** to learn more and download Muse for free today. That's Muse app.com to download Muse for free.

**00:18:10** Go there now, Muse, because deep thinking doesn't happen in front of a computer.

**00:18:15** A thanks to Muse for their support of this show and Relay FM.

**00:18:19** Cortaxins, we are once again, as we have for the last two years, taken this time to raise money for

**00:18:26** St. Jude's Children's Research Hospital from now throughout September, which is Childhood Cancer

**00:18:31** Awareness Month. I want to tell you a little bit about St. Jude and why it's a special place and

**00:18:35** why we think it's deserving of your donations. So this is our third consecutive year of supporting

**00:18:40** the life-saving missions of St. Jude's Children's Research Hospital. It's quite simple. They find

**00:18:45** cures, they save children. St. Jude is leading the way that the world understands, treats, and

**00:18:51** defeats childhood cancer and other life-threatening diseases, but they cannot do it without the help

**00:18:56** of people like you. Because of generous donors, families never receive a bill from St. Jude for

**00:19:01** treatment, travel, or food because all a family should have to worry about in these situations,

**00:19:06** in these times. It's just helping their child live. For context, the average cost to treat just one

**00:19:12** child of acute lymphoblastic leukemia, the most common form of childhood cancer, is $203,074.

**00:19:20** To make this possible, it's a lot of money. It's so much money. That is a breathtaking amount. It's

**00:19:25** so much money. And to make this possible, about 80% of the funds necessary to sustain and grow

**00:19:32** St. Jude must be raised each year from donors because this is an incredibly expensive thing.

**00:19:38** And the great thing about St. Jude really is not only do they treat the children,

**00:19:43** they're also a research hospital. So the things that they learn can be used for future cancer

**00:19:49** patients. And one of my favorite things about St. Jude is this knowledge. They share it with the

**00:19:54** rest of the world, the entire science community they will share this knowledge with. And that's

**00:19:58** what I love about them. It is one place. It is in Memphis, Tennessee, which is where my co-founder,

**00:20:04** Stephen Hackett, lives. And we are particularly tied to St. Jude, like emotionally, because one

**00:20:10** of his children had treatment at St. Jude and saved his life. And through our first two fundraising

**00:20:16** campaigns, the Relay FM community has raised over $800,000 for the mission of St. Jude.

**00:20:22** And this year, we want to cross one million. So you can help us by donating at st Jude.org

**00:20:27** slash Relay today. This year, people who donate over $100 will get exclusive Relay FM sticker,

**00:20:33** thanks pack at the end of the campaign, just as a little thank you from us. Let's cure childhood

**00:20:38** cancer together. Million dollars this year. I want to do it. Like we want to, we've set our goal

**00:20:45** at $333,333,33, because it's the third one. But when we hit $196,000 raised, we've done a million

**00:20:55** over three years. And when you think about it, like, it's an incredible amount of money.

**00:21:00** It's breathtaking. It's a drop in the ocean, really. But looking at that, that's like five

**00:21:05** children whose life could be saved from that money. And that's kind of an incredible thing.

**00:21:10** I also think the thing that you mentioned about the research being shared is just

**00:21:15** much less common in science than people might think it is. It really is so uncommon. It's shockingly rare.

**00:21:21** The reason we talk about it is because they are abnormal in what they do here, right? Like,

**00:21:27** they share their science. They don't keep it to themselves and try and make money from it.

**00:21:32** They share it. So you, but you attempting to do a calculation there of like children per

**00:21:37** hundred thousand dollars. I think that argument doesn't apply very well to St. Jude precisely

**00:21:43** because of this fact that they share the knowledge that they're able to get, which has,

**00:21:48** it has a big multiplying effect for dollars donated. And I cannot believe that Relay is

**00:21:54** approaching a million dollars for St. Jude. Like it's an unbelievable number. And I think it's

**00:22:01** great. Like I really, I really hope that we get there this year with that fundraiser. It's just,

**00:22:08** it's a lot of money and it really shows the generosity of all of the Relay listeners.

**00:22:14** Yeah. And it really is. And we have continued to be blown away by every year. And I hope that

**00:22:19** people will continue to donate is at stjude.org slash Relay, where you can donate today.

**00:22:24** And of course we're continuing the tradition as part of this campaign through September.

**00:22:29** We're going to be holding podcastathon three. It is happening on September 17th from 12 to 8 p.m.

**00:22:37** Eastern time. We're doing two hours more this year. It's an eight hour podcast. We've done six

**00:22:45** years, the first year we were supposed to do six years, the second year last year, but we did seven.

**00:22:49** I presume you mean six hours. You said we did six years the first year.

**00:22:55** Wait a second.

**00:22:56** It just feels like six years when you're alive.

**00:22:58** Six hours the first year, seven hours the second year, because we were close to meeting our goals.

**00:23:04** So we just kept going until we did it. Oh, that's right. Yes, that's right. I remember.

**00:23:08** We're doing eight hours this year. We have so many things planned. We have multiple sets of plans.

**00:23:15** Maybe we're remote. Maybe we're in person. We actually don't know at this point,

**00:23:19** but I've got the balloon room standing by in case I'm going to be here in mega studio again.

**00:23:25** I'm super excited. We're going to have tons of guests, loads of great stuff planned.

**00:23:29** That's going to be on September 17th from 12 to 8 p.m. US Eastern time at twitch.tv slash relay FM.

**00:23:36** If you go to twitch.tv slash relay FM now and click the follow button, you'll be

**00:23:40** a lot of when things go live. So I'm super excited about the podcastathon and to be once again

**00:23:45** raising money for such an incredible cause. Please donate at St. Jude dot org slash relay.

**00:23:50** So we have mentioned Cortex animated a bunch of times in the past. These are wonderful videos

**00:23:55** created by H.M. Bhutet that we put on our YouTube channel, the Cortex YouTube channel.

**00:24:00** And we know every month they send us a video and we take a look and then we approve it and we

**00:24:06** upload it. They're delightful. It's delightful. Fantastic. They're all just incredible. And I'm

**00:24:10** so pleased that we're able to make these happen with them. They do just a superb job.

**00:24:14** This time they were like, this one's going to take me a little bit longer. We're like, all right.

**00:24:19** And so they posted the video with some flashbacks in it. I don't want to spoil it all, but it's

**00:24:24** worth watching basically because if you remember on our previous episode, me and Gray were like

**00:24:29** dumbfounded with discovering the effective executive because we could not remember this book

**00:24:34** at all. And I still don't remember it. It was on your Kindle. Yeah. We're like, what is this?

**00:24:39** But in the episode for the effective executive at the end of it, you say,

**00:24:44** don't let it tap to Blue Manor and cast forget on me again. If I suggest it again,

**00:24:49** you have to remember Mike that we've already read it. And then I say, well, I will remember

**00:24:54** because there was something quite unique about this book. So I don't know exactly what has

**00:25:00** happened here, why we were convinced we needed to remember this book and then didn't. And that's

**00:25:05** also is that a magic joke? I was about to say that is a magic reference. I enjoy this on several

**00:25:12** levels because this is this is clearly the thing that I would do every couple of years, which is

**00:25:18** just, oh, I can't I can't get back into magic. But let me just let me just read up a little bit on

**00:25:24** what's going on in the magic. Let me just think about it a little. Let me just think about it.

**00:25:27** That's the same. Yeah. I'm not going to I'm going to push this button, but I'm going to put my

**00:25:32** finger on it and see if it has a little give before it clicks, you know, like that kind of

**00:25:36** that kind of thing. And so I enjoy this because the very fact that I would say that sentence

**00:25:41** indicates to me that I was at one of the heights of perhaps trying to get sucked back in, but then

**00:25:46** backing away. But so, yes, I like it because I made a magic reference about neither of us should

**00:25:52** let a spell be cast upon us so that we do not remember this book. You are then confident that

**00:25:58** you will remember at the very least and then flash forward whatever it is a couple of years.

**00:26:03** Neither of us had any memory of any part of this. Still don't remember it. I have no memory of this

**00:26:09** book. No, I don't I don't remember a thing about this book. I have a I have a guess. My my best

**00:26:17** guess is that it was one of those books where we said something like, oh, it must have been really

**00:26:24** influential at the time because we've heard all of the ideas in other places. And so it makes the

**00:26:30** original seem really boring and unnoteworthy, even though maybe it was the thing that set the trend

**00:26:37** at the time. That's my best guess. Is this a foreshadowing of today's episode, Greg? Is this a

**00:26:42** foreshadowing? I don't know what you could possibly mean. That's my best guess about because here's

**00:26:50** here's the thing. Think about any kind of media. The best things are the best things. The worst

**00:26:57** things are also kind of the best in their own way because at least you can remember them. And the

**00:27:02** worst things are the ones that are actually dead in the middle and just boring. The true worst things.

**00:27:07** Yeah, the true worst things are the things that score five out of ten rather than one or ten out

**00:27:14** of ten. Five is the worst. Ten is the best. One is the next best as far as the order of things.

**00:27:21** So that's my guess about the effective executive. What could make it so completely forgettable? But

**00:27:28** I don't know. I have no idea. This episode of Cortex is brought to you by our good friends

**00:27:35** at Memberful. Memberful is the easiest way to sell memberships to your audience used by the biggest

**00:27:41** creators on the web. Generate sustainable recurring income while diversifying your revenue

**00:27:47** stream. You might have heard us talk about Mortex which is part of the Relay FM membership program

**00:27:52** but what you might not know is that Memberful is the platform that we use to power it all.

**00:27:56** Now make it super easy for us to generate that extra revenue stream whilst also delivering

**00:28:00** bonus content to our members. I really love being able to use Memberful. It's so easy for us to

**00:28:06** integrate with the platforms that we use and make it super easy for our listeners to get additional

**00:28:11** content and to have ad removes in Mortex. Memberful make it so easy for us to keep track of the people

**00:28:16** that are signing up and to integrate with other platforms and systems like Discord so we can have

**00:28:21** that integration there as well. Maybe you're already producing content and relying on advertising or

**00:28:26** other means of income. Memberful makes it easy to diversify that income with everything you need to

**00:28:31** run a membership program of your own including custom branding, gift subscriptions, Apple Pay,

**00:28:36** free trials, private podcasts and tons more while leaving you with full control and ownership of

**00:28:41** everything that relates to your audience, brand and membership. If you're a content creator,

**00:28:46** Memberful can help you monetize that passion. Get started for free at Memberful.com-Cortex

**00:28:51** has no credit card required. That's Memberful.com, M-E-M-B-E-R-F-U-L.com-Cortex. Go there right now and

**00:28:58** check it out. It could be the start of something exciting. Our thanks to Memberful for their

**00:29:03** support of this show and Relay FM. All right, Cortex Book Club Time. Cortex Book Club Time.

**00:29:09** Thinking Fast and Slow by Daniel Kahneman. I don't know if you knew but he was a Nobel Prize winner

**00:29:14** grade, did you know that? I thought this book won the Nobel Prize. That's what the cover leads me to

**00:29:20** believe. We have a bit of a problem with this book. Mike, you don't know what I think about this book.

**00:29:30** You don't have any idea. No, I said there is a problem with this book. There is a problem. Oh,

**00:29:35** okay. The problem with this book for me happened on the Reddit thread of our last episode. Okay.

**00:29:44** Because we had a bunch of Cortexons say, oh boy, that's a dense book. And my brain said,

**00:29:52** I don't want to read this anymore. So I really, really struggled to get started with this one.

**00:30:01** So what you're saying is when I requested that we pushed back the recording date of this episode by

**00:30:06** week, you had no complaints about that because you probably hadn't even started the book. It helped me

**00:30:12** massively. I'd started it, but I've not gotten very far at all. And then there was like a whole

**00:30:18** week where I couldn't listen because we were traveling a little bit. And I was so happy

**00:30:25** because otherwise I didn't know what I was going to do. Let's just say it's not a book where you

**00:30:31** want to listen to all 20 hours in one day, for sure. That would not be a pleasant experience.

**00:30:37** Did you try and do that? No, no, no. Oh God, no. No, I didn't do that. But I partly needed to push

**00:30:43** back the date for similar reasons where I was looking at the number of things I needed to do

**00:30:48** between then and the recording date and the number of hours I had left in the book, which was something

**00:30:54** like 17 at that point. And I thought we were at a very similar place at that point, to be honest.

**00:31:03** I thought I'm going to have a real problem. The Cortexons were not wrong. This is an incredibly

**00:31:08** dense book. And it left me with a feeling which I cannot believe I felt where I missed

**00:31:16** the batshit banana stories from the other books. The things that would make me the most angry

**00:31:24** when we come to the show, like this is so annoying. Why are you wasting my time with these stupid

**00:31:29** examples? They were the things I ended up missing because the problem with this book is for most of

**00:31:36** it, I cannot attach to it because it's so dry. It is so dry and dense. And there's just so much

**00:31:46** stuff. It is not a book to try and read quickly. And I actually think this is really not a book

**00:31:53** for audio. Yeah. So what you're saying is, is you missed the, uh, the Emith revisited style stories.

**00:32:02** I went to a magic hotel. I think I kind of did, which is so weird. But what I've realized is like,

**00:32:08** what I want from these books is good information and things that like give some kind of emotional

**00:32:15** response to me. Right. Like riding on a motorcycle with your 17 children in Hawaii. All on the back.

**00:32:21** Right. All on the back. And you go, wait, how does that work? I don't understand.

**00:32:24** But at least it gives me like, I can imagine things or whatever. Like Daniel Kahneman loves

**00:32:31** an experiment more than any other human being alive. I feel like everything is an experiment.

**00:32:38** Did this experiment, did this experiment, looked at the pupils dilating, did this experiment. Hey,

**00:32:44** what about this experiment? Like there's so many of them and it's not, it doesn't, it doesn't grab me

**00:32:50** in the same way. I found it for that reason kind of really hard to try and get through.

**00:32:57** Yeah. So, oh, Mike, I've got some things to tell you about those experiments later. Oh,

**00:33:02** I will also agree that, so when we discuss these books, you normally read the audio book and I'm

**00:33:08** always like, Oh, Mike, it's a terrible mistake. You shouldn't read the audio book because these

**00:33:12** sorts of books are made for, you have to be able to skim them. And because my schedule in the next

**00:33:19** month has radically changed at the last moment, I found myself extremely short on time in the last

**00:33:25** two weeks. And I realized I'm going to have to go through this book as an audio book. I just don't

**00:33:31** have the time to sit down and read through it. And very quickly I realized, oh, this, this isn't just

**00:33:41** normally the situation where these books are not good in audio form. This book is particularly

**00:33:49** brutal in audio form. I found myself in this constantly frustrated situation where I knew the

**00:33:57** only time that I could get through it was when I was doing other things. So I could listen in audio

**00:34:02** form while simultaneously knowing that I could be getting through the book easily five times faster

**00:34:09** if I was actually reading it because for reasons we'll get into later, a lot of this stuff I just

**00:34:16** heard before or found completely unremarkable. And there was one part in particular where I did drop

**00:34:24** out and I read two chapters because I'm like, I'm willing to bet I know what's in these chapters.

**00:34:29** And so I was like, let me just quickly jump over to the actual book. It's like, okay, chapters three

**00:34:34** and four reading in quotation marks, but actually skim reading very quickly and like blasting through

**00:34:40** that section. But that was the only part where I was able to do that. And then I had to get back

**00:34:43** into the audio book and yeah, it is a brutal book in audio form. I think it's a particularly bad one

**00:34:52** because like you said, there are, well, I do have complaints about some of his stories because

**00:34:59** I think that there are stories in this book, but they're all the same kind of story that I find

**00:35:04** really infuriating where he tells you a little bit about some colleague who like did this other thing.

**00:35:11** And that to me is, I don't know, it's just an infuriating Lee.

**00:35:15** And I had to put this, but this little bit like he's really making sure to give credit and make

**00:35:20** all of his colleagues sound great. And so he constantly includes references to like, Oh,

**00:35:25** this is, this is from my genius colleague who he's so much smarter than me. We work together

**00:35:30** on this thing, but it's mostly him. And like, he's so, like, I don't care. I don't care who

**00:35:35** did the thing. Like I just care about the idea. I don't really care that this one came from Chicago.

**00:35:40** And this one came from the university of Illinois and like, Oh, well that team at Illinois is great.

**00:35:45** Like I don't care about that at all. But I do think that that is a, I'm going to put it this way.

**00:35:51** I think it is a side effect of defensive writing on the part of an academic

**00:35:59** who is trying to write a popular book. It's like, this is, this is like

**00:36:07** the popular writing version of citing where the work comes from. So he's trying not to just put

**00:36:14** in a little footnote that says like Smith et al University of Hawaii. He's instead trying to tell

**00:36:21** you a little bit about the people at the University of Hawaii. But, you know, just like when you watch

**00:36:26** the behind the scenes, uh, for making a movie, it's like, Oh, spoiler, everyone's just got great

**00:36:31** things to say about everyone else. It's the same thing here academically. Like I didn't hear one

**00:36:36** bad thing about one colleague. I found that kind of stuff infuriating and also just made it very

**00:36:41** difficult to listen to because it's like, I know if I was reading this, as soon as I would hit one

**00:36:46** of those paragraphs, I would just jump right to the next paragraph and be like, yeah, yeah. Just

**00:36:50** tell me the thing. I don't care about the person behind the thing. So I did do something with this

**00:36:54** book that I haven't known before. I didn't read all of it. Well, I listened to the first half

**00:36:59** in entirety. It's cut into a bunch of sections, this book, and in about section three,

**00:37:06** it really lost me. I'm getting to wine a little bit later on. So I then started basically just

**00:37:14** jumping around. I would listen to something and get what I think is the main idea. And then once

**00:37:19** he started going into all of the experiments that he'd done to prove his point, I would jump forward

**00:37:23** to the next section. So I feel like I was still getting the main ideas, but I wasn't sitting

**00:37:30** through the supporting materials, which again, this is a very normal thing of these types of books,

**00:37:34** but I think I prefer the presentation style of the fake person than the, let me tell you about

**00:37:40** the experiment. So I didn't listen to all of it. Yeah. The fake person is worse, but less boring,

**00:37:48** which has some redeeming characteristics. It goes back to that good worst and true worst thing

**00:37:54** that we were talking about. Right. Yeah. I was like, I'm trying to remember what it is. I just,

**00:37:59** just recently I did the thing that very rarely happens where I

**00:38:03** hate read a book. Like a book made me so angry that I finished it.

**00:38:12** God, I can't remember what it was. This is just a couple of months ago. It doesn't happen very

**00:38:16** often, but when it does, it's an odd experience of like, I hate this book so much, but I'm going

**00:38:21** to finish it. But you know what? That's an experience. You know, I felt something, whereas

**00:38:27** with the boring stuff, it's just, it's so much worse in a completely unremarkable way. So

**00:38:37** I've got so many complaints. It's hard to know where to start.

**00:38:40** Can we talk about the actual good thing of the book? I want a sandwich just a little bit.

**00:38:44** Okay. Yes. Let's talk about the good thing in the book. And then I will tell you

**00:38:49** my story about this book. Awesome.

**00:38:51** Go ahead. And then I have a bunch of more complaints.

**00:38:55** Look, the thing about this book, the reason this book is successful is because it has something

**00:38:59** which is genuinely very good, which is system one and two. This is the thing that makes this book

**00:39:05** what it is. This is the reason why this book is in so many businesses. It's the reason why

**00:39:11** when you join an advertising agency, they will give you this book. Like this is a very normal

**00:39:15** thing that happens. It's effectively saying that our brains work in one of two ways. There is system

**00:39:24** one, which is automatic responses to things. Like for example, if you hear a loud noise,

**00:39:30** you'll immediately go like you turn and look at it. Simple things like driving a car with no

**00:39:35** traffic on a route that you know, basic sentence structure, all of that kind of stuff. These are

**00:39:40** just simple things that our brain can do automatically. And then there are system two,

**00:39:44** which are things that take more effort, things that need orderly steps, things that you have

**00:39:49** to pay attention to, like focusing on the voice of a particular person in a loud environment takes

**00:39:56** effort. Filling out a form that you're unfamiliar with takes effort. Parking your car in a narrow

**00:40:02** space, rightly. These are things where like you must focus on them. So the book at first focuses

**00:40:09** on this a lot as it gets later in the book tangentially relates back to it, which is very

**00:40:13** strange to me because it feels like the entire book should be about this. But then it seems to

**00:40:17** like go off in these weird areas, which is like, Oh, well, by the way, that's a part of system one.

**00:40:22** It's like, all right, thanks. I really like this idea. I like a lot of where it comes from. I like

**00:40:29** how it can be used and is used a lot in marketing and stuff like that. Right. Like system one kind

**00:40:37** of leads on subliminal messaging and that kind of stuff. Right. Like these are the things that

**00:40:42** like people will take advantage of to try and like just get these ideas in your head and our

**00:40:47** system ones can be tricked. Like one of the key examples, this is actually a pretty good one.

**00:40:52** If somebody told you to think of the word eat and then shows you the letters S O then space and P,

**00:40:58** you would immediately think of soup. But if they told you the word clean and then showed you the

**00:41:04** same thing, you would say soap, like simple stuff like that. I liked it. I liked this idea. And

**00:41:09** there are some other parts that come from it. Like part of system two can be like being in a state of

**00:41:15** flow when you're working, when you're like really concentrating and you're in it, like all this sort

**00:41:19** of kind of stuff. I found it really interesting, but that was kind of the entire book for me.

**00:41:23** Yeah. I think if I were trying to distill down the valuable thing here is that I feel like he

**00:41:31** never quite says so clearly, but the basic idea is a little bit like if you need to make a decision

**00:41:39** that matters, you should notice when you haven't actually thought about it. Yeah. That, that like

**00:41:47** by default, your brain always wants to use the fast way of thinking. One of the ways they do

**00:41:53** press and search I like is saying that like your brain instinctively tries to find a system one

**00:41:59** answer to a system two question. Like would this person be good for this job? And you immediately

**00:42:06** look at them as and try and judge them based on their appearance. Like that is a system one answer

**00:42:11** because the system two answer is actually doing some research on this person. And like, and our

**00:42:17** brains try and make these impressions very quickly. So it doesn't have to work. Like he refers to

**00:42:22** system two as lazy, which I like. Yeah. Yeah. I think it is a good idea to have people realize

**00:42:29** that the logical part of their brain is in some ways a smart, but lazy slacker. And you know,

**00:42:38** like you need to rouse it at certain moments when it really matters and be like, Hey, pay attention,

**00:42:44** do the thing that you do. And he has some good examples in there of when are you more likely to

**00:42:54** fall for this? And I think the best two for me are familiar. They're related, but they're familiarity

**00:43:00** and availability that you tend to go with things that you have just heard a bunch. So this is,

**00:43:08** this is like with marketing, like this is the whole idea of how advertising works in many ways

**00:43:12** is just repeatedly expose people to the same idea. And the contents of that idea doesn't really

**00:43:21** matter. It's just that people will then tend to prefer whatever that idea is over alternatives

**00:43:28** that they are less familiar with. So if you're buying a car, be aware that you're, you're going

**00:43:35** to be tending towards brands. You have heard more versus brands you have heard less. And that's not

**00:43:43** always a logical thing to do. And then availability is a similar sort of thing where you just tend to,

**00:43:50** when thinking of things, you think of the most salient examples in your mind of a thing. So

**00:43:58** stuff that is emotionally resonant will come to mind first, say either books you really liked or

**00:44:04** books you really hated, like they're easier to remember than say dry books that might be filled

**00:44:09** with a lot of great information or whatever. So like, I think those two are useful to try to catch.

**00:44:17** Oh, am I just coming up with a reflexive answer? I'm just saying a thing that I have heard lots

**00:44:23** of people say, or I'm remembering a case that is emotionally salient or that was recent. I'm not

**00:44:32** thinking of what is the typical example of this case. So I feel like that's how I would try to

**00:44:40** encapsulate what is in the book. I think the things you were touching on there could cause cognitive

**00:44:45** ease, which is when our brains make logical jumps because we're familiar with something, even if the

**00:44:50** answer is not correct. But our memory of thinking that we know something will suffice, right? It's

**00:44:56** just like a thing that you couldn't know, but because you've experienced something in the past,

**00:45:00** you will just give an answer to it. And then exposure effect, which is the more we see

**00:45:04** something, the more we are likely to feel positive about it. Right. There's this one part that I'd

**00:45:09** like to, which is talking about the way that our brains expect things differently and how this can

**00:45:14** go from system two to system one. Like if something unexpected happens to you, you kind of deal with

**00:45:20** it with system two, because you have to try and work out what on earth is happening. But then once

**00:45:25** it's happened once, if it happens again, it becomes much more of a system one thing. And you're more

**00:45:30** likely to expect it to happen. And he uses an example of he bumped into a friend in Italy,

**00:45:37** and it was like a big thing and then bumped into him in London. And it was like, well, I see this

**00:45:41** guy, John in different places. So it's not weird to me. The reason this is resonated is I have a

**00:45:47** friend like this. His name's Matt. And multiple times I have bumped into him in places that seem

**00:45:53** really strange once was at a sporting event in New York City. And it was like a huge thing. It was

**00:46:00** like, oh my God, I can't believe you're here. Like we're sitting two rows away from each other. Like

**00:46:03** how wild is this? And then a couple of weeks ago, we were staying at a hotel in London and I saw him

**00:46:09** there and it wasn't such a surprise. Right. Because I've experienced it. Like, oh yeah, Matt's my friend

**00:46:14** that I see in various places around the world unexpectedly. Yeah. I like that example because

**00:46:18** I think people can understand that one quite well that like just by the nature of life, there is

**00:46:24** going to be that person who you seem to bump into more frequently than other people. And it rapidly

**00:46:28** becomes, oh yeah, that's the person who's everywhere in your brain. It doesn't become remarkable at all.

**00:46:34** But you have taught me things like this before of like selection effects where like I can see that

**00:46:41** we're probably quite similar people. So we end up being in similar places. There's just a higher

**00:46:46** percentage of chance that I will see him because we like the same kinds of things.

**00:46:52** Right. So if I'm going to see anyone, it will be Matt. Yeah. I mean, you have already stumbled upon

**00:46:57** one of the things that I find quite frustrating with this book, which is I think a conflation of

**00:47:04** like pure math, which he's often talking about with the reality of social situations. Yes. Which

**00:47:11** I feel like he just does not acknowledge in any way. I was losing my mind at one part of this book.

**00:47:20** Okay. I can't wait to find out if it's the same one that I lost my mind over. But before we get

**00:47:25** there, I was just looking through my notes and I realized there's one other idea, which I think

**00:47:29** is worth saying that comes out of the book. It's a single paragraph I've heard it before, but I still

**00:47:34** think it's like, this is always a good idea to hear. He's talking about how people remember what

**00:47:40** they have done. And so the example that he uses is like, when you ask couples, what percentage of

**00:47:47** the housework do you think you do? The total will always be over 100%. Right. Because both people

**00:47:53** will say, oh, I do 60% of the work and my partner only does 40% of the work. Or if you are in a

**00:48:00** group work situation in school, it's the same thing. The total amount of work that was done is

**00:48:05** 300% because each of the five people think that they did 40%. Who was the leader of this project?

**00:48:11** We all were. Right. Exactly. I think this is one of those ideas that should be constantly hammered

**00:48:19** into people's minds that you were more aware of the things that you do than you are aware of the

**00:48:27** things that other people do. That is not to say that you can't be in a marriage where one person

**00:48:33** is a total slacker on the housework. It's not saying everyone does the same thing, but you should

**00:48:38** just be aware that by default, your brain is exceedingly aware of every tiny thing that you

**00:48:46** have to do and is almost completely oblivious of all of the things that everyone else has to do.

**00:48:53** I really do think that one of the prime areas for this is the employer-employee relationship.

**00:49:01** It's very easy for employees to imagine that their bosses do nothing and that they do all of the work.

**00:49:09** It's just an interesting situation and it's useful to keep that in mind. I think it's useful

**00:49:16** in work life when you're on a team. You're like, oh, I'm doing everything. Are you really? Or in

**00:49:22** a relationship, oh, I'm the one who does all the work in this relationship. Do you really? Maybe

**00:49:27** it is the case, but it is much more likely that you're just over remembering your own contributions.

**00:49:34** This is actually something that I do think about a lot with other people is just try to remember.

**00:49:41** You always overestimate your own contributions to whatever a partnership is in any way. That was

**00:49:49** one of the few notes that I had for like, this is a great idea. It's one paragraph in the book,

**00:49:53** but I gave it a pink highlight to show to myself, this is the most important highlight.

**00:49:58** He talks about what you see is all there is, which I liked, which is a system one behavior,

**00:50:05** that you just see something and you take in what you see and you make your judgment on it.

**00:50:10** That's that without actually taking in any sources or information. One of the things that this,

**00:50:16** which I really liked, is the halo effect. They give this example of you'll meet somebody at a

**00:50:22** party and you're talking to someone at the party and you really like them and you think they're

**00:50:25** interesting. And then later on at that party, someone tells you about a charity that they're

**00:50:30** involved in and asks, do you know anybody who you think might be interested in donating?

**00:50:34** And then you immediately go back to person one and think they probably would because they seem

**00:50:38** like a nice person, but you do not know them at all. Like you just assume they are good and

**00:50:43** generous and kind because you like them. And I found that as like such an interesting point

**00:50:50** for the modern world today, like how many judgments and assessments we make about people

**00:50:56** just because we like them without knowing everything about them. And I think this works

**00:51:00** in multiple ways. I think the people need to be much more aware of this these days,

**00:51:05** that people are complicated and there's a lot to them that you do and do not know.

**00:51:11** And that it can be helpful to try and remember that when making judgments good and bad.

**00:51:15** Yeah. Yeah. I think he lightly touches on it in the halo effect section, but it's another one of

**00:51:20** these really under talked about ways that people influence you is just through their sheer physical

**00:51:26** attractiveness is a kind of halo effect that someone who is physically attractive gets

**00:51:32** overrated on all sorts of good qualities in this comical.

**00:51:36** So generous. So kind.

**00:51:39** Physical attractiveness is this kind of thing that is just impossible not to halo effect

**00:51:45** people over sort of talking about social realities, though. I do think one of the key

**00:51:50** things about the halo effect, though, is like this only applies, though, when you don't really know

**00:51:58** the person that like this is where like social reality kicks in of like, yeah, well, yeah,

**00:52:02** you can start to make judgments about people when you actually know them. But it is it is useful to

**00:52:07** be aware that like if you meet an attractive person who is paying you attention, your

**00:52:14** your fast thinking part of your brain is going to be like, this person's great at everything.

**00:52:19** They should be my partner. And also, they should be the new dean of this college.

**00:52:25** And they probably do all of these amazing things. Like you just it's useful to be aware of that when

**00:52:31** you're in one of those situations, like hold back judgments in the immediate moment until you have

**00:52:36** actually gathered some more information about the person. I'm just looking through here. Do I have

**00:52:40** anything else that I think is particularly good from this section? No, like it is after this

**00:52:47** section where things start to get rough for me. But there are some interesting parts to it.

**00:52:52** I particularly liked something called the availability cascade, which is when like the

**00:52:58** media will jump on a thing, making it like a circus, because the more you cover it, the more

**00:53:03** people care and they overweight unimportant things because people like to learn about them.

**00:53:08** And this is like stuff that directly appeals to system one. This just I like to this. It just

**00:53:13** puts something into words, which we've spoken about before, which I've been dealing with over

**00:53:18** the last year or two, like just about the way that news is covered and what's covered and what's

**00:53:22** important and what isn't. Yeah, I highlighted that section as well. But my note with that is

**00:53:27** terrible name for this phenomenon. Availability cascade. It's horrific. It doesn't make any sense.

**00:53:32** Availability sounds like a good thing. An availability cascade sounds like cornucopia

**00:53:38** of delight. Cascade is a good word, but availability doesn't fit with even the example he gave.

**00:53:45** Right. But it just it sounds more like it's a good thing than it's a bad thing. Yeah.

**00:53:50** Oh, I'm so free. How available am I? Oh, man, I've got a cascade of availability. This is amazing.

**00:53:57** There is a part in the beginning of the book where he he talks about this idea,

**00:54:02** which I think I have a tendency to underrate. But I do think that he's right about that giving

**00:54:08** specific vocabulary to certain ideas is helpful in terms of thinking about those ideas. I think

**00:54:15** it's particularly funny in this book because I would rate Kahneman as very under average with

**00:54:22** actually coming up with good names for the concepts that he's talking about.

**00:54:25** What makes it even worse is he doesn't stop naming things.

**00:54:29** They're all like 2000 different things in this book. This is one of my issues with it is

**00:54:36** it's too much branding. Like what Kahneman really has is like four books in one here because like

**00:54:44** he has the one really good idea and then like a bunch of others and it's like just a dartboard

**00:54:49** of naming stuff constantly. Here's the thing. We don't like Kahneman's book very much, but I mean,

**00:54:55** dude won the Nobel Prize for something like for this book.

**00:55:00** For this book. It's very smart, very smart. Yeah. No doubt. So I think he's trying to do he's trying

**00:55:06** to do a survey of knowledge in some way and of a lot of things that he was involved in. But this

**00:55:12** is where I need to tell you a story. You're not going to like about this book, Mike. Are you ready?

**00:55:17** Yeah. This episode of Cortex is brought to you by Squarespace, the all in one platform to build

**00:55:22** your online presence and run your own business from websites and online stores to marketing tools

**00:55:27** and analytics. Squarespace have got you covered because they combine cutting edge design and

**00:55:32** world class engineering to make it easier than ever for you to establish your home online and

**00:55:37** make your ideas a reality. Squarespace has absolutely everything that you're going to need

**00:55:42** to create a beautiful modern website of your own. You start with their wonderful, professionally

**00:55:46** designed templates that use drag and drop tools that you can take advantage of to make your own

**00:55:51** customizing the look, the feel, the settings, even products that you have on sale with just a few

**00:55:56** clicks and all of Squarespace's websites are optimized for mobile. Your content automatically

**00:56:01** adjusts. It's going to look great on any device. I really love that Squarespace has inbuilt

**00:56:05** analytics. So that's all just really easy to do. And that iPad app and iPhone app, that's so good.

**00:56:10** You can go in and view the important stuff you need. Also, you can publish content. You can even

**00:56:15** make changes to your website right from their apps as well. With Squarespace, you'll get unlimited,

**00:56:20** free hosting, top of the line security and dependable resources that are there to help you

**00:56:24** succeed. There's nothing to patch or upgrade. They have award-winning 24-7 customer support.

**00:56:29** You can even grab a unique domain name and take advantage of SEO and email marketing tools to get

**00:56:34** your ideas out there to the world. With Squarespace, you can turn your big idea into a new website,

**00:56:40** showcase your work with their portfolio designs, publish a blog post, promote your business,

**00:56:43** announce an upcoming event and so much more. Head to squarespace.com slash cortex for a

**00:56:48** free trial today with no credit card required. And when you're ready to launch, use the offer

**00:56:52** code Cortex and you'll save 10% of your first purchase of a website or domain. That's squarespace.com

**00:56:58** slash cortex. And when you decide to sign up, use the offer code Cortex to get 10% of your first

**00:57:03** purchase and show your support for this show. A thanks to Squarespace for the continued support

**00:57:07** of Cortex and Relay FM. Okay, so here Mike is my experience with reading this book. So I'm listening

**00:57:14** to it and like we said, it starts out with, here's the idea. There are these two ways of thinking,

**00:57:21** you know, and that's like chapters one and two is getting you started on the book.

**00:57:25** As it goes on, one of the first things he talks about is ego depletion. This idea that you have

**00:57:32** a finite amount of willpower. You can only expend it on so many things and that's partly because

**00:57:38** of the fact that type two thinking is taxing. I particularly enjoyed how he spent a really long

**00:57:44** time making sure that you believed him, that mentally difficult work made you tired. I don't

**00:57:50** know about you, but I found that section a little bit like he was telling me running on a treadmill

**00:57:54** would make me tired. He's like, did you know if I had you do mental math really fast in the lab,

**00:58:01** you couldn't do it indefinitely. Give this man a second though, O'Pryze. And he was, he was

**00:58:11** obviously really chuffed with this particular test he came up with about adding one to numbers,

**00:58:16** to sequences of numbers really quickly because he insisted many times like don't just read this

**00:58:21** part of the book. You got to try this. Did you try it? No, of course not. I was walking around

**00:58:25** listening to an audio book. No, I didn't either. I was like, I'm just going to wait for him to get

**00:58:29** to the result because I'm not interested in doing the sums. Thank you. Yeah. And guess what? The

**00:58:34** result was this will make you real tired. Like, yeah, duh. That's why I didn't want to do it.

**00:58:39** I was system wanting all my way through that section, dude. I can see where this almost going,

**00:58:46** Kahneman. Catch me. Yeah. Lazy slack earth system two looks up as like, nice try.

**00:58:56** But so, so he starts talking about ego depletion, which for various reasons, we don't need to get

**00:59:00** into because I think ego depletion is like a whole other thing for another time. But the fact that

**00:59:05** he's talking about ego depletion raises a little bit of a yellow flag in my brain and I go, hmm,

**00:59:12** uh-oh. It's like, oh, well, whatever. Let's just, we're going to keep reading. Then we get to chapter

**00:59:18** four, which covers a topic called priming. And this is where I thought, oh no. And I bailed out

**00:59:30** of the audio book to read the physical book because I thought I didn't know how this is going to go.

**00:59:35** And so skim through the section on priming, which did not like for reasons that we'll get to.

**00:59:43** I thought, okay, it's getting a little worse here. I'm going to jump back into the audio book

**00:59:49** and keep listening. And it just kept going on and on with all of these experiments in the social

**00:59:55** sciences that you were talking about. Kahneman never saw or participated in a behavioral

**01:00:02** economics experiment that he didn't want to tell you about. And it's like, here are all of these

**01:00:07** experiments. I do feel like I've lived his entire career in the last couple of weeks.

**01:00:13** It's a book that makes you feel like you're the other person for sure, because you even have to

**01:00:18** participate in office chitchat between colleagues where he tells you about what this colleague

**01:00:24** thought and then you, the colleague was surprised and thought about it some more. And because he's

**01:00:28** such a clever clogs, he obviously realized his mistake because he's smarter than me, the author,

**01:00:32** or whatever. Oh, by the way, this is one of the most often cited papers in the field.

**01:00:39** I mean, he, like I do get the impression he's quite a prolific dude, but also in that section,

**01:00:44** it always strikes me how even super popular papers, their citation numbers are real low.

**01:00:51** Like 400. It's like, all right, bud. I think it's not that many.

**01:00:56** Right. But this actually leads directly into the problem that I have, which is like, oh,

**01:01:00** the most popular behavioral economics paper is cited 400 times. Like it's actually quite,

**01:01:05** that's just quite a small number. It should raise some red flags in your brain. But so as I kept

**01:01:09** reading, I kept coming across all this behavioral, um, behavioral economics and behavioral science

**01:01:14** and psychology experiments. And at one point, probably about a quarter of the way into the book,

**01:01:19** I thought I have to check when this book was published. So go look. So the book is published

**01:01:25** in 2011 or 2010. And so this book was published right before a thing called the replication crisis

**01:01:42** came through like a, like a hurricane to destroy the social sciences. Are you familiar with the

**01:01:49** phrase, the replication crisis? Is this a thing you've ever heard? I have never heard of this

**01:01:54** before. No. Okay. Yeah. So I think more people should know about the replication crisis because

**01:02:01** it is a big deal in the modern world, but I'm also slightly sad to tell you about it, Mike,

**01:02:08** because I know it will make you sad. Can I read from Wikipedia, which is a thing that I seem to

**01:02:12** be keep doing recently? Yes. Go right ahead. The replication crisis is an ongoing methodological

**01:02:18** crisis in which it has been found that many scientific studies are difficult or impossible

**01:02:22** to replicate or reproduce. The replication crisis most severely affects the social sciences

**01:02:28** and medicine while survey data strongly indicates that all of the natural sciences are probably

**01:02:33** implicated as well. The phrase was coined in the early 2010s as part of a growing awareness of the

**01:02:38** problem. The replication crisis represents an important body of research in the field of

**01:02:42** metascience. All right. Early 2010s. Yeah. Yeah. Replication crisis has been on my radar for a

**01:02:48** really long time, and it's been quite an interesting thing to sort of follow how the

**01:02:54** scientific world has attempted to deal with this. But as the description there says, there are two

**01:03:00** particular areas that are just destroyed by this, and it is social sciences in particular and

**01:03:08** medicine. And what the replication crisis, you can summarize it as saying, an enormous percentage

**01:03:18** of these studies that you hear about, like in this sort of book where they say, we did an experiment

**01:03:24** and we had a basket of food and we put eyeballs above the basket of food and people stole from

**01:03:29** it less. These experimental results either don't replicate, which means when other people try to

**01:03:36** do the same experiment, they do not get the same results, or they have literally never been

**01:03:41** attempted to be replicated, which tells you almost nothing about the validity of the statement. And

**01:03:49** the absolute epicenter of what started the replication crisis was all of the social science

**01:03:56** work on priming, because priming, this whole thing in chapter four, is this idea that kind of like

**01:04:04** spread through the greater society. Like if you show people images of older people, they'll walk

**01:04:11** more slowly down a hallway, right? Or you can make people act more virtuous if you have them swear on

**01:04:19** a Bible, like this concept of priming, that like you're putting ideas into someone's head and then

**01:04:25** they will act more like the ideas that you just put in their head. And this whole field was just

**01:04:30** destroyed of like, none of this is real. None of this replicates. You cannot prove that this effect

**01:04:38** exists, or if it does exist, it's so incredibly infinitesimally small that the results you're

**01:04:46** getting can't possibly be real. So this is the replication crisis. It seems like he's involved

**01:04:52** in this, Conlon. So, yeah, like, I don't know what the deal is. Like, I, so let me say this. So,

**01:05:01** once we got to the section of priming and we continued with all of these like really cute,

**01:05:06** sort of media friendly experiments afterward, I just found myself in this position of,

**01:05:13** it is incredibly difficult to take anything in this book at its word. I'm so pleased that you say

**01:05:19** this. Why do you say that? I was getting so angry at this section. Which section in particular? There

**01:05:24** are these like situations where he creates fake people and personality profiles. We can get to

**01:05:30** the fake people. I hate the fake. All right, great. Cool, cool, cool, cool, cool. Come back.

**01:05:33** Come back to that. Sorry. Can you show you what you're going to say? We're not even at the fake

**01:05:38** people. Okay, cool. Cool, cool, cool. Like we're, we're at the, we're at the real science part,

**01:05:44** which is before all of the fake people. So I think Daniel Kahneman established a lot of like

**01:05:52** the impression that I get from some of the stuff that he talks about in this book is I feel like

**01:05:56** he did a lot of the foundational work in the basic concept of the irrationality of human decision

**01:06:03** making. And like I have absolutely no argument with him there. Like I had this really weird

**01:06:09** experience reading the book where it was, Hey, Daniel Kahneman, I'm totally on board with a lot

**01:06:18** of the ideas that you're expressing. I think that the environments around a person has huge amount

**01:06:23** of impact on what they actually do. A concept we've discussed on the show many times. So like you can

**01:06:28** influence your environment and your environment influences you. I'm totally on board for people

**01:06:35** don't make rational decisions all the time. And a huge number of ways that people just

**01:06:41** think lazily and it's, it's useful to try to put language and terms that express the ways that

**01:06:49** people think lazily. It's like, I'm on, I'm on board with you here, dude. But the problem is

**01:06:56** almost everything that you're using to back this up is like scores very high on my bullsh** ometer

**01:07:05** and that bullsh** ometer is not uncalibrated because this book is right at the heart of one

**01:07:11** of the biggest problems in the scientific world in the last 10 years. I have a suspicion that

**01:07:19** part of the reason this book is so popular is it must have been one of the last books published

**01:07:28** before it would have become very difficult to publish a book filled with all of these examples.

**01:07:35** So that it is actually the book that contains the maximum density of examples of these kinds of

**01:07:42** stories, because I think even a year or two later, more editors might have flagged this up of like,

**01:07:50** Hey, how sure are you about this priming stuff? Like, have you looked into this? One of my other

**01:07:55** complaints is, is I do think the book lacks actionable things to do with some of this.

**01:08:00** Like there's a lot of stuff that's just extremely unactionable. But one of the things I've really

**01:08:04** become an increasing fan of over the years is try to try to quantify your thinking in terms of bets.

**01:08:10** And I was, you know, we were getting ready for the show this morning. I was trying to think like,

**01:08:15** how confident am I in making statements about the failure to replicate of studies in this book?

**01:08:24** Right? Like, you know, I'm not an expert in this field, you know, I don't know. But I thought I

**01:08:28** would easily take an even money bet that at least 45% of the experiments mentioned in the first half

**01:08:40** of the book are wrong. Like I would, I would happily place a large amount of money on that

**01:08:45** bet. And wrong in the sense that they either don't replicate, or they have never been attempted to be

**01:08:52** replicated, which is basically worthless in the social science. Like a single paper that says,

**01:08:56** we got this crazy result is literally worthless from a mathematical perspective. Like it just

**01:09:02** tells you nothing except, Hey, maybe you should do another one of these. So I have to limit it

**01:09:07** to the first half of the book, because I exploded when we got to the fake people and

**01:09:14** just could not deal with it. So please literally had to take a walk.

**01:09:19** Because that was when I couldn't take it anymore. That was when I then started going through. It's

**01:09:24** really interesting. He has a new book out. And I wonder what that's like, like, with this stuff in

**01:09:29** mind, like, they had a book come out this year, I think called noise. I don't know anything about

**01:09:34** it. Okay. So I just, I don't know anything about it, but that title sounds a little bit like it's

**01:09:38** trying to talk about some of the replication crisis. Because I just want to mention something

**01:09:42** really quickly here, because I think the replication crisis has been actually quite

**01:09:48** damaging to the wider world in a bunch of ways, because you do get media reports or stories about

**01:09:57** like how people are under certain circumstances or how people act or what people do, or like,

**01:10:03** look at this wacky experiment, where we get the wrong results. And I do think this stuff kind of

**01:10:08** just permeates society as this background knowledge of like, oh, we all know that people

**01:10:13** will be greedy under these circumstances or people will cheat under those circumstances.

**01:10:18** And like, I've looked into these papers sometimes and they just don't replicate

**01:10:23** and they just get repeated as true ideas. I don't think this book's gonna make you happier.

**01:10:28** Oh, no. This is from Amazon. Wherever there is human judgment, there is noise.

**01:10:34** Imagine that two doctors in the same city give different diagnoses to identical patients,

**01:10:39** or that two judges in the same court give different sentences to people who have committed matching

**01:10:43** crimes. Now imagine that the same doctor and the same judge make different decisions depending on

**01:10:47** whether it is morning or afternoon or Monday. Oh my God. Okay. So this is the, right. Okay.

**01:10:53** So this is, he's actually hitting one right there, which I used to think was true and then

**01:10:58** looked into it more and it is not true. And it has to do with judges giving harsher sentences

**01:11:04** right before lunch is like this concept that- He references that in the book.

**01:11:07** ... when people are being hungry. Yeah. So like, that doesn't replicate as far as I am aware. Like,

**01:11:12** that paper failed to replicate when done with other things. So the reason I thought that the

**01:11:16** title noise would be related to this is because, so here's the fundamental problem with the

**01:11:25** replication crisis. If you have, say, in America, I don't know how many behavioral economics

**01:11:35** students there are or psychology students there are trying to get their PhDs, but you have people

**01:11:39** who need to do experiments and you have lots of them who are doing experiments. You know full well

**01:11:48** just from like the mathematics of large numbers that some of those people will conduct an

**01:11:54** experiment and they will get extremely convincing results that variable A is related to variable B,

**01:12:04** even though they're not related at all, just by chance because there's just a large number of

**01:12:10** people here. An example I used to do back when I was a teacher and you do some basic statistics

**01:12:15** is I'd have a class of 20 students and you have everyone stand up and everyone gets to flip a coin

**01:12:22** and if they flip heads, they get to stay standing up and flip again. Well, in a class of 20 people,

**01:12:28** you're basically guaranteed you're going to get one kid who's really surprised they flipped heads

**01:12:33** four times in a row and like that's just basically statistically is very likely to happen,

**01:12:39** but what's not likely to happen is that when you do it a second time, the same kid flips heads

**01:12:45** four times in a row, right? That kid wasn't really good at flipping coins or whatever.

**01:12:51** So the replication crisis is interesting because in some ways it's a side effect of there are way

**01:12:56** more people doing science now than there were in the past. And so one of the problems that you have

**01:13:01** to deal with is when you have lots of people doing experiments, you know that some of them

**01:13:09** are going to be really wrong, but also have shockingly convincing data, which is why you

**01:13:15** need to run it again because it's the equivalent of someone publishing a paper that says,

**01:13:22** holy, I flipped a coin and it came up heads 10 times in a row. I must be amazing at this,

**01:13:29** right? Like it's the mathematical equivalent of that. The other slightly more technical problem,

**01:13:33** which is not really worth getting into, but the bar for I don't really want to get into is that

**01:13:40** people will know it's called like the P hacking. It's this probability metric that's used of like,

**01:13:45** how good does your data have to be to be published in a respectable peer reviewed

**01:13:49** journal? The threshold is not set very high. It's set so that you can basically be guaranteed that

**01:13:57** one in 20 papers can't be correct in a journal is roughly where the threshold is set, which is

**01:14:03** really appalling when you realize, oh, an edition of a journal may have 40 papers in it. So two of

**01:14:12** them before we even do anything, you can be very confident are wrong without even having to look

**01:14:19** at any of the data because you just know where the threshold has been set for what will we accept

**01:14:25** to publish in this paper. And that's like the best case scenario because the journals are only

**01:14:31** picking from papers that obviously have really convincing data, but those papers are produced

**01:14:37** by statistical outliers when they perform their experiments. So it's a huge problem in the field

**01:14:43** and it's why after the priming stuff and when it just kept continuing onward, I was like,

**01:14:48** I'm having a real hard time with this book in this dual way of like, I believe your fundamental

**01:14:54** thesis, but goddamn did like this book get published at the exact wrong year to include

**01:15:02** the maximum amount of almost certainly non-replicable experiments. This makes me feel

**01:15:09** so much better about how I felt about this book. Oh, okay. I thought you would be crying when you

**01:15:14** heard about the replication crisis. Okay. I mean, that's the whole thing that I want to look into

**01:15:19** a bit more, but it seems deeply unsettling, but in a kind of tantalizing way, which is interestingly

**01:15:25** kind of exactly what these books are like, all right. Like they are, it's like tantalizing. So

**01:15:30** you just want to believe it. Cause like, it was a point, like I was, I was talking to Adina last

**01:15:35** night about this cause she asked me what I thought about it. And it was like, it was like a part of me.

**01:15:39** It's like, I don't know who this book is for or why. Like it's not really a business book.

**01:15:45** It's not really a self-help book. No, no, it's not really a book about science, but it's kind of

**01:15:51** accepted by all of them because it's like catnip to all of those different, especially like the

**01:15:55** businessy types of things, because there's interesting stuff in here, but every time it

**01:16:00** would get to a point where he would start to give examples and explain his interesting idea,

**01:16:06** I would become more infuriated by the overall experience because some of the stuff, it was like

**01:16:13** the, it's the very worst of these types of books where it's like, I'm going to tell you a thing,

**01:16:21** then tell you everybody's wrong except me. Yeah. And in other books, people do this, right? This

**01:16:27** is very normal in these types of books, but it's not usually being presented to me as science.

**01:16:34** Yes. Okay. I had, I had a thought that I was going to keep to myself, but you've expressed

**01:16:39** a similar feeling, so I feel less bad about it. Part of the reason I never read this book is

**01:16:46** it was hugely recommended to me, which I often just find a sort of yellow flag for

**01:16:52** recommendations in general of like, you know, when a thing is overwhelmingly recommended,

**01:16:57** I can be really confident. I won't like it. For example, Ready Player One, like everyone in the

**01:17:02** universe recommended it to me and was like, I can guarantee you, I will not like that book.

**01:17:06** This book had an additional layer, which is the people who recommended it to me

**01:17:15** would fall into a category that I think this book is kind of catnip for, which is a little bit of

**01:17:22** an elitist, aren't I smarter than everyone? Right, right, right. And I think that this book

**01:17:30** has that kind of weaved through it all the time. It's like, it's a little bit set up for,

**01:17:38** oh yeah, I know all about this stuff. I wouldn't fall for this kind of stuff, but look at how

**01:17:45** other people fall for this kind of stuff. And I don't have it highlighted, but there were a few

**01:17:50** little sentences that just really rubbed me the wrong way where he's like, so when we make policy

**01:17:58** for people, we need to keep in mind that they're thinking with their emotional brains.

**01:18:03** When he starts talking about governments, it's like, luckily some governments are doing things

**01:18:09** the way I think they should be done. Hopefully they'll all come on board one day.

**01:18:15** Yeah. Okay. So I'm glad it wasn't just me, but it's like, there's a quality of

**01:18:20** elite college educated superiority that some people have when recommending this book. And it's one

**01:18:28** of the things that always put me off the book. And it's like, boy, it is in here. It bugged me.

**01:18:35** It bugged me a number of times. So let's talk about the fake people. We've got to talk about

**01:18:39** the fake people. Tell me about the fake people, Mike, because I have to hear what you think about

**01:18:43** this because this is where I lost it to. So he creates two people. One is Tom Dubb,

**01:18:49** one is Linda, and the Linda one is really controversial. And he's actually listed as so,

**01:18:54** which I appreciate, like it to a point where it is known as the Linda problem. After publishing

**01:19:00** the paper, that's hilarious. Linda is the exact moment I checked out. Yeah. So in a nutshell,

**01:19:06** creates a fake person creates a personality profile about them, and then wants you to guess

**01:19:11** what jobs that they would be good at. Then says that all of your guesses are wrong.

**01:19:18** So what he explicitly does is creates a person who you are 100% expected to suggest that they

**01:19:25** would do this type of job. And he goes, no, no, they'd be good at another one. But you've created

**01:19:31** this fake situation and told me to think a certain way. And then when I said, yeah, I believe you,

**01:19:38** you said, no, you're wrong. And I hate stuff like this. You created this completely fake situation.

**01:19:44** Same with Linda, right? Creates this person like profile was like, there's no way that they could

**01:19:49** be a bank teller. It's just impossible. Like, no, it's not impossible. They could be. And I find

**01:19:54** this so annoying because it's like, I'm so smart. You are so stupid. Or like, there's another part

**01:20:00** in the book as well. This is much later on, where he's talking about experts that all experts are

**01:20:08** wrong, because they cannot actually predict the future. Like people are paid to forecast things,

**01:20:16** but there's no way that they could know them because it's the future. So they're all wrong.

**01:20:20** And it's kind of like, I'm not saying that he's incorrect. But by his own logic,

**01:20:28** what he has just said is wrong. Because he cannot actually know. And I really get annoyed when

**01:20:34** these types of books wrap themselves in either A, these falsehoods and tells you you're stupid

**01:20:39** for believing them, even though they forced you to believe them, or B, makes these grand sweeping

**01:20:45** statements that undo everything the statement has said, like it's eating its own tail, right? Like,

**01:20:52** you can't trust anyone, everyone's wrong. But you can trust me except everyone's wrong. And like,

**01:20:58** these two parts just, it really, unfortunately reduced my overall feeling about this book.

**01:21:07** And I really wish that these, as most of these books, it was just half the size. And then he

**01:21:13** could have got out what he wanted to say, put system one and two in, give me some more about that,

**01:21:18** and left it there. Because everything past that point really undermines the work, I think.

**01:21:25** Yeah. The thing about experts, again, because what is true has been a repeated topic on this

**01:21:31** podcast. It's frustrating because I think in the what is true topic, the danger that you constantly

**01:21:40** have to avoid is becoming cynical and just reflexively going, oh, I can't believe experts,

**01:21:47** experts are dumb. You can have an interesting conversation about under what circumstances does

**01:21:55** it make sense to trust expert advice? What are the constraints and what are the incentives that

**01:22:00** are acting upon an expert? And that gives you a way to frame people's advice. A very classic

**01:22:10** example is something that comes up in the past year. You say regulatory agencies, how much should

**01:22:16** you trust a regulatory agency? And you're like, well, the more that that agency, the people making

**01:22:21** decisions have something on the line personally, you should take that into account for trusting of

**01:22:28** that. Or it's like, there are all of these different ways to think about that. But a dismissal

**01:22:35** that is also then couched in, except me as the expert is just, it's the worst kind of thing.

**01:22:43** I think it encourages a kind of cynical ism that is not helpful for actually solving problems.

**01:22:51** And also tells you sort of implicitly, because you've read this far in this book,

**01:22:57** you obviously agree with all of this. So when I say you can trust me, I'm also saying you can trust

**01:23:03** you who trusts me. You're the real expert here. I do want to just add on something that you said,

**01:23:08** because you made a good point about if they have something on the line. But I think that

**01:23:12** can actually lend a little bit to what he's saying. And I just wanted to also suggest like that.

**01:23:18** We have an expert, yeah, they probably, well, they definitely can't predict the future.

**01:23:22** But they know more about it than you do. And so if one of us is going to try and make a decision,

**01:23:28** maybe it's that person. And that was the thing that annoyed me about this book, where it's kind

**01:23:32** of like, no experts know nothing. I don't know anything. There's no way they can predict anything.

**01:23:37** I was like, Yeah, I know, we all know this, like, we'll know that these people cannot predict the

**01:23:41** future. But if you spent years studying something, you maybe have a better gut reaction than me,

**01:23:47** rando individual who's just rolling up having read a news article in The Guardian, right? Like,

**01:23:53** I don't think anybody is suggesting that people can accurately predict the future. But if we're

**01:23:59** going to try and base something on something, at least try and put some logic behind it. And I

**01:24:04** find it really annoying that he, as you say, right, it's like, yeah, we shouldn't do that to anyone,

**01:24:10** except what I have to say, just to get back to the fake people thing. All right, so I just want

**01:24:16** the listeners to really understand what we're saying here. So just to pull up part of this,

**01:24:20** I'm just going to read a little bit of the Tom W one. So he presents you with this description

**01:24:26** of a person, Tom W, who's not a real person, is a constructed person for this experiment here,

**01:24:34** right? He says, Tom W is of high intelligence, although lacking in true creativity. He has a

**01:24:41** need for order and clarity and for neat and tidy systems in which every detail finds its appropriate

**01:24:46** places. His writing is rather dull and mechanical, occasionally livened by somewhat corny puns and

**01:24:53** flashes of imagination of the sci fi type. Now, before we even get one sentence further,

**01:25:00** the note I wrote down in this section of the book is I feel like I'm having a stroke. Like,

**01:25:06** how am I supposed to understand what is happening here? Like, I don't know about you. I literally

**01:25:16** can't conceive of this the way he's trying to ask me to conceive of this. It's like,

**01:25:24** it's not a real person. Okay. So do I need to pretend it's a real person?

**01:25:29** If it is a real person, who's giving me this information? God, like, is this information

**01:25:36** 100% accurate? Yeah, because this is like, oh, you have to judge this personality profile,

**01:25:41** which is a personality profile that doesn't exist. No one can write this about someone.

**01:25:46** Yeah. It's like every, every one of those sentences is thrown into immediate confusion.

**01:25:53** If, if I am quite literally doing what the whole book is about, hey, think about this seriously.

**01:25:58** Like don't just make a quick judgment. Think about it seriously. But the moment I have to

**01:26:02** think about it seriously, I feel like I can't read. Like I can't, I can't absorb this in any way

**01:26:09** because the whole thing just falls apart. And then yes, well, the thing that, that both of

**01:26:13** us find annoying is he, he then asks you to rank out of nine categories, you know, which of these

**01:26:19** categories do you think that he's most likely to work in? And then he goes, LOL, no, you're wrong.

**01:26:24** He's not likely to be a librarian. He's likely to be a farmer because there's more farmers than

**01:26:29** librarians. Yeah. Oh, I hate that. There's more farmers in the world. Oh, good. Great. That's

**01:26:35** excellent. But are they this type of person? Like let's be realistic here. Like I know what you're

**01:26:41** trying to tell me, but the world doesn't work on probabilities. It's not, no. This, this is what I

**01:26:48** meant by constant confusion of math problems and social situations. And it, the thing that made me

**01:26:54** think of is like, I remember in high school, one of the standardized math things that we had to

**01:27:00** learn was these, I really quite liked them, but they were logic puzzles, but they would be presented

**01:27:04** as a series of sentences. And so they would say something like, uh, John wears red every day that

**01:27:13** starts with a T. He only wears blue on every other day and he'll never wear yellow on a Sunday.

**01:27:21** If it's a Tuesday, what color is he likely to be wearing? And step one of solving any of these

**01:27:27** problems is you go, okay, these words don't have anything to do with reality. And you have to just

**01:27:32** turn them into mathematical statements. And then it's very easy to solve. Right. But this, so what

**01:27:40** Kahneman, the trick that he's pulling here, he is explicitly asking you to solve a social problem.

**01:27:46** Here's a personality description of a person. What job do you think that they would like to do?

**01:27:51** And then he's pulling the rug out from under you going, LOL, I actually only wanted a statistical

**01:27:56** answer. And it's like, you didn't ask me for that. Yeah. You didn't ask me for that. I was walking

**01:28:03** around in my pre-cortex time. I was trying to really articulate like, why does this make me

**01:28:10** so mad? Like, but I couldn't put it into words. And then it finally dawned on me of like, I know

**01:28:15** what he's doing. So this is my metaphor for this section. Let me describe a student for you. She's

**01:28:21** the smartest girl in school and she loves books. She's great at memorizing long lists of things.

**01:28:27** And she's about to be sorted into a Hogwarts house. Which house do you think she's going to be

**01:28:34** sorted into? Did you guess Ravenclaw? LOL, no, she ended up in Gryffindor. She ended up in

**01:28:42** Gryffindor because there's more brave people than smart people. Now don't you feel stupid? Oh my

**01:28:48** God, it's in theory. Like this whole thing is fake. It's not real. That's what it just dawned on me.

**01:28:53** Like that's what it is. I am being judged for guessing people's Hogwarts house incorrectly

**01:28:59** based on fictional descriptions of fictional people. And he's giving me a lesson on like,

**01:29:05** well, you know, the stereotype of Ravenclaw students is that they're smart, but actually in

**01:29:09** the book, it's not really mentioned very often that they're smart. So jokes on you, like you

**01:29:14** really fell for something here. Like I don't, like it's maddening. It's absolutely maddening. And like,

**01:29:20** I actually find it more maddening because there is a good idea underneath this. It's just presented

**01:29:29** in the worst of all possible ways. I want to again, I'm going back to Wikipedia because that's

**01:29:34** apparently what I do on this podcast now. So this is known as the conjunction fallacy and some part

**01:29:40** of it and it's going back to the Linda problem. I just want to read it out just so people that

**01:29:45** haven't read the book couldn't like, because we're just getting so upset now. Right. So Linda is 31

**01:29:51** years old, single, outspoken and very bright. She majored in philosophy as a student. She was deeply

**01:29:56** concerned of issues of discrimination and social justice and also participated in the anti-nuclear

**01:30:01** demonstrations, which is more probable. One, Linda is a bank teller. Two, Linda is a bank teller and

**01:30:07** is active in the feminist movement. The majority of those asked choose option two. However, the

**01:30:11** probability of two events occurring together in conjunction is always less than or equal to the

**01:30:16** probability of either one occurring alone. So the idea is she's more likely to be a bank teller than

**01:30:22** a bank teller active in the feminist movement because the probability you have to ignore the

**01:30:26** fact that you were told, you were told clearly in such a way that would suggest that she would be

**01:30:33** active in the feminist movement. And that is so angering to me because I honestly, genuinely

**01:30:40** believe this person is more likely with real, like the way we believe about the world kind of thinking

**01:30:50** to be a bank teller active in the feminist movement than just a bank teller because people

**01:30:56** are not math. Yeah. So I mean, here's the thing. I will disagree with you on that. That's fine.

**01:31:02** Right. But the, the, this is why I think he's, it's particularly bad at explaining this concept. Now,

**01:31:09** like I think that the Linda one is, is less bad because the fundamental thing that he's trying to

**01:31:16** convey is true on a mathematical perspective. Yeah. The Tom one is worse, I say. Right. The Tom

**01:31:21** one is worse because it's just you guessed wrong about this person's job. Like that's, that's what

**01:31:27** I mean with the Ravenclaw houses, but like also with the Tom one, the thing that's frustrating to

**01:31:33** me about it is like, okay, so let me, let me translate this into the useful idea. Useful idea,

**01:31:38** people is don't bet against the base rate unless you have a really good reason why you think this

**01:31:46** time is different. So all that means is say like, I think this is a really useful idea for trying to

**01:31:54** make predictions as you say, Oh, I want to, I want to predict something. Well, if I didn't know

**01:32:01** anything about the details of this particular situation, but it's like a class of situations.

**01:32:09** So you might say, what's the chance that the CEO at a big tech company will get replaced

**01:32:16** within the next year, let's say, and you want to place a bet on that. You can lose yourself very

**01:32:23** easily in like, Oh, here's all these things that I know about Apple and this might have an effect

**01:32:28** or this might have an effect. I think this is super important. This thing, like you can get lost

**01:32:33** in those specifics. And this is the argument against experts in some ways is you can become

**01:32:39** too obsessed with the details. The very starting question should be what is the likelihood in any

**01:32:47** given year that a tech CEO is replaced as CEO based on the last 10 years of data. And that should be

**01:32:55** your default betting position, unless you have like a really good reason why you think you may

**01:33:03** know differently this time. And like, maybe you do, maybe you don't. And so that's what the Tom

**01:33:08** question is trying to get at, but the way it should be phrased is more like, if you have to guess what

**01:33:14** someone's job is and you don't have reliable information about them, you should just guess

**01:33:21** whatever the most frequent job is and you will be correct most of the time. But it's, he just

**01:33:27** presents it in like this totally bizarre social way where if you have to take it seriously

**01:33:35** in real life, when you're really interacting with people, I think people are actually pretty good

**01:33:43** at making correlative judgments about other people. And like, this is, this is why I say like the

**01:33:49** selection effect is really undervalued in humans. That if you know a couple things about someone,

**01:33:55** you probably can estimate very well other things about them. But he's not doing that with an

**01:34:03** artificial person. Like this artificial person is just math. And like, that's just not how it works

**01:34:11** if you are really interacting with someone. Like if Tom was a real person, I'm pretty sure if I was

**01:34:18** talking with him, I could figure out pretty quickly, is he more likely to be a librarian

**01:34:24** or a farmer based on information that you get about interacting with the person rather than like this

**01:34:30** stroke inducing description of his personality. So that's why it's absolutely infuriating. And it's

**01:34:37** more infuriating because like don't bet against the base rate is a really good idea. I feel like

**01:34:43** it's an idea that I've only really become aware of in like the past five years as something that

**01:34:48** just wasn't really on my mind before of like, base rates really important. You should think about it

**01:34:53** if a decision really matters. And so that's why I'm like, when I get to the Tom section, I'm like,

**01:34:57** ah, like I can't, I can't deal with it. And thank God, this is not where I first came across this

**01:35:02** concept. This mixture of social and math, I think serves neither of them. There's one more thing I

**01:35:11** just have to tell you about. Okay. Cause I highlighted it cause this was the other time.

**01:35:16** I don't know if you have this experience, Mike, but sometimes when you're listening to a podcast

**01:35:19** or an audio book, like you can remember exactly where you were when you heard something.

**01:35:24** Yeah. Yeah. Yeah. This is, I feel this and I've heard a lot of people say this to me about my

**01:35:29** shows in the past. Yeah. So I was listening to this audio book and I was at a particular spot

**01:35:33** in London and again, had like an aneurysm on the street when I got to this point in the audio book.

**01:35:38** And I know that forever in my life, I will always think of this one corner in London as

**01:35:43** bat and ball corner. So do you remember the bat and ball section in the book? Did this strike you

**01:35:49** at all? Oh yeah. Yeah. Yeah. Yeah. Yeah. So I'm just going to read this little section from the

**01:35:55** book word for word. He's talking about system one and system two thinking, for example, here is a

**01:36:01** puzzle. Do not try to solve it, but listen to your intuition. A bat and ball cost a dollar 10.

**01:36:08** The bat costs $1 more than the ball. How much does the ball cost? Right. So I listened to that

**01:36:16** and I followed his instructions and the first number that pops into my head is 10 cents as the

**01:36:23** answer, which of course is the wrong answer. And I actually knew it was the wrong answer because

**01:36:29** I've heard this before, but following his instructions, like don't try to solve it. Just

**01:36:34** listen to your intuition. It's still the number that just pops right into my head of like, Oh,

**01:36:38** it's got to be 10 cents. That's not the way it works. Like it's actually five cents. If you write

**01:36:42** it out with some algebra and you solve it. But this is one of those sections where he sort of

**01:36:46** goes on to be like, LOL, aren't people dumb? And I had an aneurysm because he, because later on he's

**01:36:53** like, he starts talking about, Oh, how, how were people able to solve it? Like they were obviously

**01:36:59** able to overcome their system one fast thinking and, you know, really, really work it out. It's

**01:37:06** safe to assume that the intuitive answer also came to the mind of those who ended up with the correct

**01:37:11** number, but they somehow managed to resist the intuition. And I'm like, okay, well, screw you,

**01:37:18** because you didn't give me the chance to actually solve it. You specifically told me,

**01:37:23** don't try to solve this. Just say whatever pops into your head for the first time. And then this

**01:37:27** is also the mixing socialness with math. Furthermore, we also know that the people who gave

**01:37:33** the intuitive answer have missed an obvious social cue. They should have wondered why anyone would

**01:37:39** include a question to a puzzle with such an obvious seeming answer. And it's like, God damn it. Like,

**01:37:46** I would have actually solved it if you didn't explicitly tell me, like, don't try to solve it.

**01:37:52** And now I feel like you're gaslighting me. Like I should have, I should have really rethought like,

**01:37:58** Oh, but it's such an obvious answer. Like there must be something more complicated. It's just like,

**01:38:02** it's another one of these weird traps of, Oh, you got the wrong answer. But actually, I really

**01:38:08** encouraged you to get the wrong answer. And I tricked you, I fooled you and like, ha ha ha.

**01:38:13** The correct answer is this one. It's just a bunch of stuff in the book is infuriating like that.

**01:38:18** It's like, yeah, the social cue stuff, obviously. And it's also why anyone who ever does public

**01:38:23** speaking, I highly recommend you never do the thing where you ask the audience a question,

**01:38:30** where you're expecting them to give one answer, and then you're going to tell them, Oh, it's the

**01:38:33** other answer. It just never works in an audience because a real group of people, you can always

**01:38:39** feel it. They hesitate because they don't know what to do. They don't know if they're supposed

**01:38:43** to give the answer that they know that you want to give so that you can then say the other answer,

**01:38:49** or if they should pick the contradictory answer because they know from a lifetime of experience

**01:38:54** that when people ask really dumb questions with obvious answers, spoiler, it's going to be the

**01:38:58** surprising answer. Like don't ever do this. Like so him to just mention this thing about missing

**01:39:04** out on the social cue also just really flipped me out. It's like, you can't keep switching

**01:39:10** between, are you trying to solve a math problem or are you aware of a social situation and using

**01:39:15** them to bounce off of each other? So that's not a part of the book I did not enjoy.

**01:39:20** In case, like, so it took me a while, right? Like in case you're one of these people like me,

**01:39:24** it struggles with it. It's $1.05 is the cost of the bat. The bat costs $1 more than the ball.

**01:39:33** So the ball's five cents to bat is $1.05. That's how that works. I hate this. Makes me feel stupid.

**01:39:40** And I hate it. Yeah. And it's also, it annoys me because again, like with the fake people,

**01:39:49** I can't help but wonder all of these people who get this question wrong, what is the situation

**01:39:54** under which you're asking them? Like, this is something about like, Oh, we asked all of these

**01:39:57** people at really smart call. Oh yeah. Harvard, MIT, and Princeton. We asked all of these people

**01:40:02** who graduated these elite universities and they got it wrong. That's like, yeah, but what's the

**01:40:07** scenario under which you asked these people? Because it matters quite a lot. I think anyone

**01:40:12** who graduates from MIT, if you gave them this question under a scenario in which like, Hey,

**01:40:20** really think about it. I think they could get you the answer. I suspect like a lot of these wrong

**01:40:26** answers are because it's not worth anything to the student being asked to think about it for more

**01:40:32** than a second. Right. It's just part of a thing. Like it also kept having flashbacks to when I was

**01:40:38** getting my sociology degree. And as part of that, guess what? You have to participate in these exact

**01:40:44** kind of experiments. It's like, okay, I had to go into the lab sometime and answer a bunch of

**01:40:49** questions on a computer or they'd have you look at a thing and, and, you know, try to react to

**01:40:53** something. And this is also part of the replication crisis. It's like, guess what? A lot of these

**01:40:59** studies, they're not done on random people. They're done on undergraduates of psychology and

**01:41:06** sociology who are trying to get credits so they can graduate. And so, you know, when I did those

**01:41:13** experiments, number one, anyone who has participated in those things, if you're doing a degree in

**01:41:19** sociology, spoiler, you already know that whatever they say they're studying is not the thing that

**01:41:25** they're studying. That's like step one of an experiment. So you're already thinking, I wonder

**01:41:30** what they're really trying to find out in this experiment because they're asking me to solve math

**01:41:33** problems, but it's not really math problems. Like I know how this works, you know, or I remember

**01:41:37** sitting on the computer and you had to do one of these things where it's like, oh, we're going to

**01:41:42** show you certain kinds of pictures and then you move the mouse cursor up and then other kinds of

**01:41:46** pictures and we move the mouse cursor down or like you have to react and like, I didn't care,

**01:41:52** whatever, like I'm there just to get a credit. But it's like if it really mattered that I

**01:41:57** performed well at this task of like classifying different sorts of flowers quickly, you bet I

**01:42:04** could do it better if the incentive was high. So this is the other like massively conflating

**01:42:09** problem for all of this stuff. And so I just, you know, again, this is why I'm willing to bet just

**01:42:14** a huge amount of this stuff just does not check out. Even dumb little things where they're like,

**01:42:20** oh, the Harvard graduates can't get this question right. It's like, yeah, but who are you asking

**01:42:26** and how I bet it just wasn't worth their time at all to think about it, which you can sort of say

**01:42:33** is part of the idea of the book that people think fast and slow sometimes, but is also

**01:42:42** totally uninteresting as a piece of information that like if people don't care about a question,

**01:42:47** they won't think about it very much. So sorry, I got way more worked up than I thought I was going

**01:42:54** to be over this book. I mean, I was pretty worked up about it too. So I'm pleased that you are.

**01:42:58** I will say that we have spoken about this book for much longer than I thought we were going to today.

**01:43:03** I really thought we were going to have to plan more stuff and we have a lot of stuff as usual

**01:43:08** that we're not going to talk about today. Like many books, it has a good thing,

**01:43:12** it has a lot of bad things. Unfortunately, I think that the bad things that this book has

**01:43:17** is maybe more bad than the typical. Yeah, I would say that I anti-recommend this book.
