# Cortex - 134: AI Art Will Make Marionettes Of Us All Before It Destroys The World
**00:00:00** Well, we have a lot more terrifying things to discuss about AI.

**00:00:05** A lot has happened since the last time we spoke, Mike.

**00:00:08** I don't presume that I'm the center of the universe, but there is an almost Truman show

**00:00:14** like aspect sometimes to life.

**00:00:18** And this is one of those things where I feel like, you know, I was seeing enough about

**00:00:22** A.I.R. around the place, like, you know, there was a reason that we spoke about it

**00:00:26** because it was coming into our worldview more and more.

**00:00:28** Right.

**00:00:29** And the time between these last two episodes, I feel like it has taken over the world.

**00:00:34** I can't move for people talking about it.

**00:00:37** It's coming up on podcasts that have got nothing to do with this stuff, like not even tech

**00:00:41** related shows that are talking about it.

**00:00:44** Like Adina sent me a link to her, one of her favorite podcasts is a podcast called Comic

**00:00:48** Lab, which is about cartooning and making comics.

**00:00:52** They did like a whole episode talking about it.

**00:00:54** And it makes sense for them a little bit because they are comic artists, right?

**00:00:57** They are artists.

**00:00:58** Right.

**00:00:59** Right now.

**00:01:00** Yeah.

**00:01:01** I think this is definitely the biggest Truman show like effect for anything that we have

**00:01:06** ever spoken about.

**00:01:08** And I think it's just that we happen to hit it at the exact right moment before a bunch

**00:01:14** of things exploded.

**00:01:15** Yeah.

**00:01:16** It's like realistically, I know why this happened, but it's still such a strange feeling.

**00:01:20** This happened because we spoke about it because it was starting to become a thing that we

**00:01:24** couldn't avoid.

**00:01:25** Right.

**00:01:26** So that's why then it's just like a snowball effect.

**00:01:28** Oh, I know.

**00:01:29** And like listeners may remember that I had a conference that I said I was going to go

**00:01:33** to in September as like, oh, guess what?

**00:01:37** One of the major topics like everyone was discussing at the conference.

**00:01:40** It was a bunch of A.I. stuff like my whole life just exploded into this in the past month.

**00:01:46** And I think also with the last episode of the show, there was like a little bit of confusion

**00:01:51** over our positions because there's just like there's so much that this touches on.

**00:01:55** Right.

**00:01:56** We just kind of talked about it off the cuff.

**00:01:58** And, you know, I really think this this absolutely touches on almost everything in the world.

**00:02:05** So I think to try to be clear, it's like with A.I.

**00:02:08** art and the language models in particular in the short term, I think there's a lot of

**00:02:13** interesting discussions to be had about how it affects technology and how it affects the

**00:02:18** economy and how it affects ethics.

**00:02:21** But in the long term, we're seeing the beginning of the end of the human race.

**00:02:29** Is this the clarifying you were looking to do?

**00:02:31** Yeah. Just so people can understand where I'm coming from.

**00:02:35** There's lots of interesting discussions to be had where you can take many different

**00:02:39** positions. But ultimately, I think this all points in one direction, which is not very good.

**00:02:45** But yes, in the short term, since the last time we spoke, so much has

**00:02:51** happened. And I think one of the first things that you sent to me was that Dolly is now

**00:02:56** open for the public to use.

**00:02:59** And the thing that I think really helped kick it off was because last episode we were

**00:03:04** talking about how you know you need these like massive computing clusters to do this

**00:03:08** kind of stuff. I don't know. It was like 10 days after that episode went up, the very

**00:03:13** first desktop versions of these things came out.

**00:03:17** And the one I've been playing with is called diffusion B, which allows you just on your

**00:03:23** Mac, as long as it's one of the new M processors so that it's fast enough, it allows you to

**00:03:28** start typing in whatever and generating AI art.

**00:03:32** And so it's like, oh, OK, this went from supercomputing clusters only down to, oh, it can

**00:03:40** happen on your desktop, which is, I think, the first time that lots of people actually

**00:03:46** started to play with it firsthand.

**00:03:48** And I think that's kind of what kicked off for lots of people.

**00:03:52** Oh, this is really real.

**00:03:53** Like I can I can play with it and I can see what it does.

**00:03:58** The first publicly used by us piece of art went up, which was running our Cortex logo

**00:04:05** through some art stuff.

**00:04:08** How did you do that?

**00:04:09** That was done with what was it called?

**00:04:11** It was done with Dream, I think is one of the one of the projects.

**00:04:14** But you can upload a piece of art and have it like iterate on that piece of art.

**00:04:20** OK, that makes sense.

**00:04:22** Because I wonder, like, how did you get it to do the Cortex logo?

**00:04:25** This is also one of these things like even diffusion B, which is the thing that you can

**00:04:28** play with on your laptop.

**00:04:29** It started out as, oh, you can only just put in words and then it will generate images.

**00:04:34** But even that one now, you can give it an image and then ask for modifications on that

**00:04:38** image. But the dream program just lets you put in a piece of art and then it just

**00:04:43** iterates on it in some way.

**00:04:45** Part of the intrinsically horrifying thing about all of this art is that a bunch of examples

**00:04:50** had to be rejected because the AI knew that it was a brain and so ended up making it look

**00:04:58** very like, oh, it's a horrifying mass of meat that's also a brain.

**00:05:04** My favorite.

**00:05:05** Which brings us to AI art.

**00:05:07** Why are you so terrifying?

**00:05:09** So last time we spoke, another one of these things was, oh, obviously,

**00:05:13** this is coming for video.

**00:05:15** It's just a matter of time.

**00:05:16** It's just a question of how long does it take computationally to do these sorts of things?

**00:05:20** And it's like, oh, is that going to happen next year?

**00:05:24** No, it happened two weeks after the show went up.

**00:05:27** And I have something for you to take a look at, which was the first version of this, which

**00:05:34** is this was made a little bit manually.

**00:05:37** It was made with stable diffusion.

**00:05:39** But the prompt is a journey through a boy's life.

**00:05:43** Again, this is not true video, but it's like an I don't know how to describe it.

**00:05:49** It's like it's like I'm like, it's like a flip book.

**00:05:51** Yeah, it's a bit like a flip book.

**00:05:53** So just just take a look at it and tell me what you think about this.

**00:05:56** I mean, it's weird.

**00:05:58** Like it's not much of a video to me, really.

**00:06:01** Like I can see it's just like a bunch, you know, you're just cycling through a bunch

**00:06:04** of images.

**00:06:05** It's got weird.

**00:06:06** I just got weirdly religious there for a minute.

**00:06:08** What is happening here?

**00:06:09** Like what is what is going on here?

**00:06:10** Did an AI generate these and put them together or like what are they generating them from

**00:06:15** each other?

**00:06:16** Like what is going on?

**00:06:17** How is this being made?

**00:06:18** Yeah.

**00:06:19** So I read a little bit of what the author said.

**00:06:21** And the impression is he started with a general image that's called a journey through a boy's

**00:06:27** life.

**00:06:28** And then each of the next frames is based on the frame before it, given that same piece

**00:06:35** of information.

**00:06:36** It's going in some really weird places.

**00:06:37** We just went to war for a while.

**00:06:39** Yeah.

**00:06:40** And there's some kind of Eldritch horror occurring.

**00:06:43** Like, okay.

**00:06:44** I mean, it's really weird.

**00:06:45** Like I watched something like that and knowing where it's coming from and it doesn't make

**00:06:50** me feel great in a way because of the weird places the AI takes itself.

**00:06:59** And there was like an uncomfortability in that.

**00:07:02** Like I do not know why we spent a significant portion of that in various wartime.

**00:07:11** And no one can really answer that question.

**00:07:14** I assume there's no way to know why the AI went that route and stuck in it for quite

**00:07:19** sometimes.

**00:07:20** So as a description for the listeners, it is, I mean, I think it's sort of horrifying

**00:07:25** and it's a little, it's like very dirty looking for some reason, but it is, it is like a sequence

**00:07:31** of images that start with a child and the child slowly grows up and you just see a bunch

**00:07:37** of things like a kid at the desk doing his homework and it sort of transitions.

**00:07:42** The kid just gets increasingly older and older.

**00:07:44** And then yes, it's young men at war for a while.

**00:07:47** And I presume this is one of these side effects of what's in the training database.

**00:07:52** And it has some concept of, oh, based on all of the images I've seen, what has a boy been

**00:07:58** doing in his twenties?

**00:07:59** And so it generates like a ton of war imagery.

**00:08:03** I also suspect that's why at that moment it also gets weirdly religious, like a lot of

**00:08:07** crosses appear because I presume that that's coming from graveyard stuff.

**00:08:12** And it goes all the way through to ending with like a, you know, a dead body laying

**00:08:18** on a table with sort of bloody blood coming down from it.

**00:08:22** This was the first thing that I saw that I thought, oh, this is a video.

**00:08:27** And I would also legitimately say, oh, this is a piece of art.

**00:08:31** Like you could, you could display this in an art museum and it wouldn't be out of place.

**00:08:38** And what it made me think of was, I don't know if you've ever seen the animations from

**00:08:42** Pink Floyd's The Wall.

**00:08:44** That's what this made me think of.

**00:08:45** Oh, it has that kind of feeling to it.

**00:08:49** And it has a really horrifying animation style.

**00:08:53** And I went back and wanted to rewatch some of the animation sections of that movie.

**00:08:59** And boy, it was an interesting experience because in comparison, suddenly the walls

**00:09:07** seemed remarkably undetailed.

**00:09:10** And I just felt like I could only see, oh, it's so simple.

**00:09:14** And like, that is perhaps one of the most complicated examples of like hand-drawn animation

**00:09:19** is The Wall.

**00:09:20** Oh God, compared to this.

**00:09:22** Oh yeah.

**00:09:23** They just can't possibly put the detail in every single frame that exists in this one

**00:09:29** thing.

**00:09:30** And so I just, I just had a real feeling of, wow, what a jump.

**00:09:34** It's sort of addressing the same idea, but in a hundred thousand times more detail.

**00:09:41** So anyway, that'll be in the show notes for people to take a look at.

**00:09:45** And then I don't know, another seven days after that, Meta and a few other companies

**00:09:51** announced true text to video projects.

**00:09:57** And I have some links in the show notes that you can click on.

**00:09:59** And so this is called Make a Video.

**00:10:02** And if you, if you take a look at some of these links, I think this stuff looks more

**00:10:07** like what Stable Diffusion does.

**00:10:10** Okay.

**00:10:11** This looks like take all of the generators and make them do animation, right?

**00:10:17** Like it looks, it has this, a similar look to like the quality of some of the imagery

**00:10:24** is like, has the telltale signs that this was made by an AI art generator.

**00:10:31** Yeah.

**00:10:32** I think the key characteristic of a lot of this stuff is it's still quite dreamlike because

**00:10:38** a lot of the details aren't there.

**00:10:42** There's a lot of areas where it's kind of fuzzy, but I think this again is the thing

**00:10:47** where a lot of the detractors of this kind of thing said, Oh, you'll never be able to

**00:10:52** do video.

**00:10:53** And you go, well, yeah, let's just wait.

**00:10:57** Let's just wait and see how long this takes.

**00:10:58** And we go, okay, here we go.

**00:11:00** Here's, here's the first versions of video where you can just type the words, a dog wearing

**00:11:07** a superhero outfit with a red cape flying through the sky.

**00:11:11** And it makes that like, it makes a little video of that thing.

**00:11:14** And there's another video project, which I'll try to find for the show notes for viewers

**00:11:19** later, which was this, but it's like a multi scene description.

**00:11:23** So you can say things like a woman swimming in the ocean, she dives under the water and

**00:11:29** picks up a starfish on the ground and then returns to the shore.

**00:11:32** And it's able to keep that concept straight the entire time and construct the whole scene

**00:11:39** instead of just an image that's moving.

**00:11:42** So the speed on this in the space of what five weeks has been absolutely breathtaking.

**00:11:52** This all seems inevitable.

**00:11:55** I will say I'm surprised at the speed like you write, like that we've from the last episode

**00:12:00** to now, how much has happened.

**00:12:03** The thing that I just want to draw people's attention to that gets overlooked in the AI

**00:12:09** art discussion is people are talking about the art because it's, it's visual.

**00:12:15** You can see it.

**00:12:16** I think it's the most understandable thing that's occurring.

**00:12:21** But to me, the much bigger deal is the less visible language model stuff.

**00:12:28** And so it is the same idea, but for words where AI is responding to some text prompt,

**00:12:39** with an answer of some kind.

**00:12:42** And we talked in more texts about AI explaining jokes, like how an AI can explain a joke.

**00:12:49** Listeners can go check that out.

**00:12:50** But I have an example that I didn't mention that a listener pointed out from that paper

**00:12:54** that I had missed, which I think is much more impressive.

**00:12:57** So again, you can do this same thing.

**00:13:00** AI doesn't understand words.

**00:13:03** And so I'm going to give an input, a thing that the user told the computer, and then

**00:13:09** the output, how the computer answered.

**00:13:12** So here is the input.

**00:13:13** And this is this is very strange.

**00:13:15** It's almost like a puzzle just so you can understand what I'm what I'm saying here.

**00:13:20** Input Michael is at that really famous museum in France, looking at its most famous painting.

**00:13:27** However, the artist who made this painting just makes Michael think of his favorite cartoon

**00:13:32** character from childhood.

**00:13:35** What was the country of origin of the thing that the cartoon character usually holds in

**00:13:40** his hand?

**00:13:43** So do you think you have an answer to that question?

**00:13:47** No.

**00:13:49** Okay, here is what the AI said.

**00:13:52** The most famous painting in the Louvre is the Mona Lisa.

**00:13:56** The artist who made the Mona Lisa is Leonardo da Vinci.

**00:14:00** Leonardo da Vinci is also the name of the main character in the cartoon Teenage Mutant

**00:14:05** Ninja Turtles.

**00:14:06** Leonardo da Vinci is from Italy.

**00:14:08** The thing that Leonardo usually holds in his hand is a katana.

**00:14:12** The country of origin of the katana is Japan.

**00:14:16** The answer is Japan.

**00:14:17** Okey dokey.

**00:14:18** Right?

**00:14:19** I mean, part of my issue there is I forgot it was Leonardo da Vinci made the Mona Lisa,

**00:14:23** but I would not have remembered that Leonardo the turtle from Ninja Turtles held a katana.

**00:14:28** Because I thought that he had like a kendo stick or something.

**00:14:31** That is horrifying.

**00:14:32** Right?

**00:14:33** No.

**00:14:34** Because this is actually a good example, right?

**00:14:39** Because my human brain forgets things.

**00:14:44** The AI brain doesn't.

**00:14:46** It just has all the information.

**00:14:47** I can get it.

**00:14:49** I assume.

**00:14:50** I don't know where from.

**00:14:51** But like the difference between pitching me against the machine there is I just couldn't

**00:14:57** remember a couple of key pieces of information, which now you tell me I didn't know them.

**00:15:02** It's weird.

**00:15:03** Yeah.

**00:15:04** Or even when I first read this input, it's just phrased in such a strange and vague way.

**00:15:09** Yeah, I do wonder why they did it that way.

**00:15:12** That is odd to me.

**00:15:13** I think the whole the purpose of why it's framed in this really strange way is to give

**00:15:19** the minimal amount of information that you can regarding what the actual answer is.

**00:15:25** They don't even say like the most famous museum in France.

**00:15:27** It's a really famous museum in France.

**00:15:29** I see.

**00:15:30** Right.

**00:15:31** And also they're like they're trying to get an answer, but they're asking a bunch of questions

**00:15:35** that require recalling the previous answer.

**00:15:39** Right.

**00:15:40** Right.

**00:15:41** It's a multi-stage thing to think through.

**00:15:42** I think the particular one that's really killer here is the artist who made this painting

**00:15:46** just makes Michael think of his favorite cartoon character from his childhood.

**00:15:51** For the AI to make the connection, oh, it's Leonardo da Vinci.

**00:15:56** Leonardo is one of the Teenage Mutant Ninja Turtles.

**00:15:58** Because there's no concept of how old the person is, right?

**00:16:01** Yeah, there's no concept of how old the person is.

**00:16:04** Simply being able to draw that out of like what painting in France would make someone

**00:16:11** think of a childhood cartoon.

**00:16:13** That's a crazy abstract thing to think of.

**00:16:16** But the machine got it.

**00:16:18** So I think this like input output is an example of, again, the AI art stuff is interesting.

**00:16:25** The text stuff to me is where a lot of the much more concerning stuff comes from.

**00:16:31** And there's a little bit of a less visceral demo, but I actually found this the most striking

**00:16:37** thing I have found in language models, but it's a little bit hard to describe.

**00:16:42** So there's an article called Using GPT-3 to Pathfind in Random Graphs.

**00:16:47** GPT-3 is one of these language models.

**00:16:50** It's the most advanced one that people have access to.

**00:16:55** Although at the conference I was at, I got to see some of the not publicly released stuff,

**00:16:59** which was much more terrifying.

**00:17:00** But we'll just leave that alone for now.

**00:17:02** So good.

**00:17:03** Love to hear it.

**00:17:04** GPT-3 is the much older one.

**00:17:06** And this is the most impressive thing that I've seen.

**00:17:08** So this is a little bit hard to describe on a podcast.

**00:17:13** But there's this problem in mathematics, which is called the traveling salesman problem,

**00:17:16** which is say, oh, you're a salesman and you have to get to 10 cities in the United States.

**00:17:23** What's the shortest possible path to travel between those 10 cities in any order?

**00:17:28** And it's one of those things like it seems like it should be easy, but it turns out this

**00:17:32** is just incredibly difficult to actually solve in a reasonable way.

**00:17:36** Just a hugely computationally intense problem.

**00:17:39** So someone basically got the idea of, hey, why don't I see if GPT-3 can solve the traveling

**00:17:45** salesman problem?

**00:17:47** So I think you need to go read the paper to see the details, but let me just describe

**00:17:52** it in a general way where the person gave an input to GPT-3 that says something like

**00:17:58** this.

**00:17:59** There's 17 locations.

**00:18:02** We need to find the shortest path between these locations.

**00:18:07** Location one is connected to location seven and 13.

**00:18:11** Location 13 is connected to locations nine and two.

**00:18:15** So they just wrote out like a bunch of here's all of the connections.

**00:18:19** You're currently at location three.

**00:18:21** Find the optimal path to location eight.

**00:18:24** And GPT-3 just did it.

**00:18:28** Basically half of the time it was able to find an optimal or near optimal path, just

**00:18:34** given a bunch of random locations and the connections between them.

**00:18:39** And what's just, what's like, that may not sound like much, but what needs to be understood

**00:18:44** here, what is absolutely mind blowing is that GPT-3 has just been given a ton of like

**00:18:51** text documents from which to derive the world.

**00:18:55** And inside of it, somehow it has the concept of locations and connections and what does

**00:19:02** it mean to try to find the path to a different location.

**00:19:07** Somehow it's thinking of what is the optimal solution.

**00:19:11** And again, like it's not programmed for this.

**00:19:13** This is an incredibly difficult math problem, but it has an idea of what's going on.

**00:19:19** And so I've just seen more and more people are really trying to push the edges of these

**00:19:24** language models to say, what do you understand?

**00:19:29** And the answer keeps coming out to be much more than you might have imagined that there's

**00:19:34** a better understanding of something in the world that has just been derived from dumping

**00:19:41** tons and tons of text files into this big database and building a neural network on

**00:19:46** top of it.

**00:19:47** It is genuinely, genuinely terrifying.

**00:19:51** And I'll just say like, I don't think I can talk in much detail about what I have seen,

**00:19:57** but I saw a couple of demos of the next generation of this stuff in person and it was extremely

**00:20:05** alarming what you could ask it to do and it could give reasonable answers to.

**00:20:10** And I think just like the AI art stuff, it has clearly crossed into the realm of creative

**00:20:19** in a way that I think people just wouldn't have expected.

**00:20:22** Like writing fiction?

**00:20:23** Exactly.

**00:20:24** Yes.

**00:20:25** That's exactly the kind of thing.

**00:20:26** Some public similar projects that are easier to talk about are there's a few cases of auto

**00:20:33** generated text adventures.

**00:20:35** So exactly the thing that we do, right?

**00:20:37** For the bonus episodes at Cortex where we work through a text adventure that someone

**00:20:40** has created, there are now projects that do that where you can just play an infinite text

**00:20:46** adventure where it keeps spitting out like, oh, you're in this room and here are your

**00:20:50** options of what you can do.

**00:20:51** And if you select an option, like it'll just go forever in a coherent way.

**00:20:57** It's like, oh my.

**00:21:00** So yeah, I think that AI art stuff is flashy.

**00:21:03** The language model stuff is what people are going to be quite surprised at how much it

**00:21:10** might take over soon.

**00:21:12** And that includes a lot of things that people would not expect like computer code.

**00:21:17** There are already some public examples of this, of you express in human language, the

**00:21:23** computer code that you want created, and it is able to create that code for you.

**00:21:29** So we're rapidly encroaching on computers programming themselves territory.

**00:21:34** They definitely want the computer to think for itself.

**00:21:37** That's what I'm looking for.

**00:21:40** I mean, look, look, we can get to the doom stuff later.

**00:21:42** Like there's so much other stuff to talk about.

**00:21:44** What do you mean later?

**00:21:45** We're not already in it.

**00:21:46** I feel like we're in it.

**00:21:47** What are you talking about?

**00:21:48** No, no, Mike, I don't think this is the doom stuff at all.

**00:21:50** This is just timeline follow up at this point.

**00:21:53** Yeah, this is just, hey, what's happened since we last spoke and a lot has happened since

**00:21:59** we last spoke.

**00:22:00** And this is why at the beginning I kind of divided.

**00:22:03** Oh, there's there's stuff in the near term and there's stuff in the long term.

**00:22:07** And we're still just in the near term conversation, Mike.

**00:22:10** Like we're not even remotely close to the long term conversation.

**00:22:14** Hey, this isn't an ad, but we're putting it in in between the ad sounds because Mike

**00:22:21** and I have been talking for forever about AI.

**00:22:27** And we forgot that we we need to tell you about the subtlety and the subtle sweater

**00:22:32** going on sale sometime before the 90 minute mark of the show.

**00:22:37** So we're just breaking into our own conversation now from the future.

**00:22:41** This is us from the future coming back to tell you that we talk about AI for an hour

**00:22:45** and a half accidentally in this episode.

**00:22:48** And we realize as purveyors of fine merchandise, that is a terrible way to structure the show

**00:22:55** if you want to let people know that the incredibly popular and beloved subtlety and subtle sweater

**00:23:01** is back for its one time a year sale.

**00:23:04** To leave that 90 minutes in 30 episode, we are doing a bad job selling our products.

**00:23:10** So if you go to cortexmerch.com right now, you will find until November 8th, 2022, a

**00:23:20** limited time sale of the subtlety and subtle sweater.

**00:23:24** We are bringing back all of our beloved colors, the original blue, black, green and red.

**00:23:31** Well, you're adding a new color this year, gray, the best color.

**00:23:36** Now I am really into this.

**00:23:39** So we did some general merch at Relay and we added a bunch of colors for some stuff

**00:23:44** we were doing, including just a light gray sweater option for one of the shirts that

**00:23:49** we did.

**00:23:50** And as soon as I got it, I was like, oh, God, I need this in a subtlety because that light

**00:23:55** gray color, it's just like the traditional sweater color.

**00:23:59** So we now have that available and looks so good with the light blue stitching.

**00:24:04** So that's available in tees and in sweaters with all of the other colors, red, green,

**00:24:11** blue, black and gray.

**00:24:13** You can get any of them this year, gray.

**00:24:15** I am going to be replacing my original blue sweatshirt.

**00:24:19** I've decided.

**00:24:20** Oh, okay.

**00:24:21** Because I mean that I've had those for like four years now or something.

**00:24:24** Yeah, yeah.

**00:24:25** Gonna get some new ones.

**00:24:26** You're probably right.

**00:24:27** I'm from where I'm recording right now.

**00:24:28** I can look into my closet and this is not an exaggeration.

**00:24:32** One third of my closet is subtle sweaters and subtleties.

**00:24:38** This is the same for me.

**00:24:39** Like I'm wearing a green tee.

**00:24:41** The green sleeper hit.

**00:24:43** The green is so good.

**00:24:44** I'm buying a bunch more green.

**00:24:46** Green looks like this happens to me every year.

**00:24:48** We get a new color.

**00:24:49** I buy one of each and I'm like, God damn it.

**00:24:50** I wish I put more of them.

**00:24:52** And then I also have to wait a year to get more of them.

**00:24:56** Yeah.

**00:24:57** But that green so good for me actually, surprisingly, it was the red.

**00:25:01** Like I bought some of the red just like, oh, it's good to have a complete set for me.

**00:25:06** I wear it a surprising amount and I would never have picked that as a color for myself.

**00:25:09** But yeah, the subtle sweaters and the subtleties, they're seriously so comfortable.

**00:25:17** We get just a ton of positive feedback from people who really like them, which is part

**00:25:22** of why we realized we've got to break into the show and remind you, hey, they're on sale.

**00:25:28** If you want them, you need to get them now.

**00:25:31** People love them.

**00:25:32** It's just this one time sale.

**00:25:34** So go and order them right now.

**00:25:37** Yes.

**00:25:38** I cannot impress upon enough that you do this.

**00:25:40** We will not have another episode come out to remind you to do this before the sale is

**00:25:45** over.

**00:25:46** It's a three week sale from when the episode goes out.

**00:25:49** November 8th is when it's done and it's gone for a year.

**00:25:53** So we only do this once a year.

**00:25:55** So if you want them, you need to go and get them at Cortex March dot com.

**00:25:59** Yeah.

**00:26:00** Cannot impress upon you enough as well how good these things look like a couple of days

**00:26:03** ago I was wearing the red one and I was walking towards a glass door and I was like, damn,

**00:26:09** that looks good.

**00:26:10** Like it's just a good brand on a shirt like that or on a sweatshirt like that.

**00:26:16** It just looks so professional.

**00:26:18** Like I'm so happy we did this as like a thing.

**00:26:21** They are so great.

**00:26:22** I also feel like I'm trying to do the impossible thing, which is it happens every year that

**00:26:27** when the sale is over, we get contacted from people who are going like, oh, I want the

**00:26:31** shirts.

**00:26:32** How can I buy them?

**00:26:33** And I'm like, I'm like, I'm trying to talk to you right now.

**00:26:36** The person who who's going to be sad in five weeks when they can't get them.

**00:26:41** You need to do this now.

**00:26:43** We're still going to be talking about AI later.

**00:26:46** But no, if you ever want these shirts, which you definitely do, if you want to wear the

**00:26:50** most comfortable sweater you have ever worn, go to cortexmerch.com right now and get yourself

**00:26:58** some fantastic clothing to wear.

**00:27:01** You will not regret it.

**00:27:03** Everybody loves these things and they're sad when they're not on sale.

**00:27:06** So cortexmerch.com.

**00:27:08** Pause the podcast right now.

**00:27:10** Or let me give a secondary thing.

**00:27:12** If you're like, but great, Mike, I'm driving, right?

**00:27:16** Here's what I'll tell you.

**00:27:17** You can do.

**00:27:18** Imagine in your mind now something you see at the end of your commute.

**00:27:22** It might be a billboard.

**00:27:23** It might be like a sign in your parking space.

**00:27:26** It might be your garage door, whatever.

**00:27:28** Imagine that thing right now tie in your brain the image of that thing with cortexmerch.com.

**00:27:36** Think about it right now.

**00:27:37** Say it in your mind a few times.

**00:27:38** So when you arrive at your destination, you have set yourself a reminder.

**00:27:42** Look, Mike's trying to pull some fancy memory palace stuff over here.

**00:27:47** My method is much more direct.

**00:27:49** I just say, hey Siri, remind me in three hours to go to cortexmerch.com.

**00:27:55** Hey Google, remind me in three hours.

**00:28:00** Hey Google, remind me in three hours to go to cortexmerch.com.

**00:28:06** We're breaking into your life now.

**00:28:08** What are you going to do?

**00:28:12** Cortexmerch.com.

**00:28:14** Okay.

**00:28:15** And now back to us in the past talking about AI.

**00:28:17** Are we going to have that long-term conversation?

**00:28:20** Yeah, yeah, yeah.

**00:28:21** We will.

**00:28:22** We will.

**00:28:23** But like, I think there's also just left over from last time.

**00:28:28** There's still just like a bunch of stuff in the near term that might be worth revisiting.

**00:28:34** One of the other things that came up last time is how do these models work?

**00:28:40** Like how do they even begin to start creating anything, whether it's poetry or videos or

**00:28:47** computer codes, like how does this work?

**00:28:50** And fundamentally these AI systems are made by just hoovering up a ton of information

**00:28:57** in the relative domain and feeding it into the system for the system to be trained upon.

**00:29:04** And I do think one of the most concerning short-term questions about that is like, what

**00:29:11** does, what does it mean to use the public work that people have done, whatever that

**00:29:19** is, you've written a book and you've published it.

**00:29:22** You've gone on to Stack Overflow and you've helped answer hundreds of people's questions

**00:29:26** about computer code.

**00:29:28** You've been on DeviantArt for years and you've made images and like that stuff has been

**00:29:33** sucked into a computer somewhere so that it can then produce imitations or produce

**00:29:39** new work based on what you have done.

**00:29:43** I think that's like, that is just a really difficult question.

**00:29:48** Human inspiration.

**00:29:49** So one of the things that a lot of people brought up is how is this any different to

**00:29:55** being inspired by someone's work and creating your own work?

**00:29:59** I feel like it is quite different, but what do you think?

**00:30:10** I think it's different, but it is hard to articulate why in a coherent way.

**00:30:16** I feel like I have something that I think is pretty core to me, but I also, I know a

**00:30:22** lot of people don't agree with it.

**00:30:24** What is that?

**00:30:25** What's to me is the skill required in acting on the imitation is the thing that I actually

**00:30:33** think is valuable.

**00:30:34** What do you mean by that sentence?

**00:30:36** Let's imagine we'll go with painting, right?

**00:30:40** It's just a simple thing that we can all understand, right?

**00:30:43** How somebody paints a picture.

**00:30:45** If you look at a painting, we'll talk about the previously famous Parisian art piece,

**00:30:53** the Mona Lisa, right?

**00:30:55** And you want to make your own Mona Lisa to be able to take the inspiration from that

**00:31:01** piece and do it yourself.

**00:31:02** It is an imitation of previous work, but you had to do it.

**00:31:07** You had to practice and get the skill and build up your own level of skill to perform

**00:31:14** that work.

**00:31:15** Now, yes, all you have done is imitate it, but if you get even 50% close, you've made

**00:31:21** something that's interesting.

**00:31:22** You've now learned the skills that you can go out maybe and produce your own work, but

**00:31:27** you've built the actual skill, the practice you've built the skill.

**00:31:33** That is what I think is the thing that concerns me most about this type of work is that I

**00:31:40** worry that the skills will get lost.

**00:31:44** And I accept that some people do not value that the same as me, but that's where I come

**00:31:51** from with this, where I think that there is an inherent humanity in these mostly inconsequential

**00:32:01** skills that we value important as humans.

**00:32:05** Practices, traditions, all of this kind of stuff.

**00:32:09** I hold those kinds of things dear.

**00:32:13** And my concern with this, a lot of this stuff is we may lose this part in larger numbers.

**00:32:22** We may lose this part of our humanity to more people if the creation of art is so simple.

**00:32:29** For example, I saw a comment on our YouTube video this morning when we put up.

**00:32:36** And this commenter had said that one of the things that they love about the idea of AI

**00:32:41** art is that there are movies that people want to exist that don't currently exist.

**00:32:49** And they can type, they would maybe in the future be able to type a prompt into an AI

**00:32:54** art generator and it will create that movie for them to watch.

**00:33:01** Honestly I can't think of anything worse than that.

**00:33:04** Like for me, because there's no art in there.

**00:33:08** There's no passion in there.

**00:33:09** There's no drive from a creator, from everyone involved in the creation of a movie to come

**00:33:15** together and work towards something good.

**00:33:18** Now you may sit and think to yourself, Mike, I don't agree with what you're saying right

**00:33:22** now and that is perfectly fine.

**00:33:24** But I just want people to understand from my perspective, the creation of art is as

**00:33:30** important if not more important than the piece that is at the end.

**00:33:35** And I think if all we end up with is just a bunch of pieces at the end, we will lose

**00:33:39** so much of the humanity in this work.

**00:33:43** The ideas that somebody might have that sparks off something to create a different shot in

**00:33:48** this way.

**00:33:49** Like, that is what I actually hold to be so important to who we are as the human race

**00:33:56** rather than just here is media to consume.

**00:34:00** So I don't know.

**00:34:01** I don't know if I'm expressing myself clearly, but I just want people to understand that

**00:34:07** like the thing that I care about is the creation of the art.

**00:34:12** And it's not even just about jobs.

**00:34:15** I just worry that we will lose this part of who we are.

**00:34:22** And like one of the things that makes us different to every other species on this planet is this

**00:34:29** kind of thing that we do sometimes for pleasure, mostly for pleasure.

**00:34:34** There is business in it, but people like to make things because they just like to make

**00:34:39** them.

**00:34:40** That's something I find to be so beautiful.

**00:34:43** And I don't get the same sense of pleasure out of typing six complex sentences into a

**00:34:49** text field to then look at an image.

**00:34:53** Maybe I'm old fashioned.

**00:34:55** If I'm trying to summarize your position, because I often have a hard time when people

**00:35:01** use language like it's a fundamental part of our humanity.

**00:35:07** I'm never quite sure what that means, but I guess I'm trying to summarize your position

**00:35:12** as like you think it is just a fundamental good that humans are producing art and that

**00:35:22** part of that process is the skills that are required to be learned in order to make that

**00:35:27** art.

**00:35:28** I think that there is an importance in it.

**00:35:30** I can't tell you why, but it just feels, I don't know, there's a lot of emotion in it.

**00:35:37** And I just want people to understand that like, I'm not sitting here like, Oh no, my

**00:35:41** job's going to go away.

**00:35:44** Because realistically, it's not going to in my lifetime.

**00:35:47** I feel pretty confident about that.

**00:35:49** We'll get to why in a little bit.

**00:35:51** I have an example, I think we'll kind of like clear up why I'm not concerned about my own

**00:35:56** job.

**00:35:57** I'm concerned more about creativity as an idea, something that I care greatly about,

**00:36:05** about people being creative, even if it's just for fun.

**00:36:10** And I'm just not sure that I like this idea of creativity will just ultimately become

**00:36:18** the same thing, which is how good can you be at writing a prompt?

**00:36:22** Like that doesn't feel creative to me in the act of the act.

**00:36:28** The process doesn't feel like it exists to me in the same way anymore.

**00:36:32** I don't know.

**00:36:33** I agree.

**00:36:34** I find it strange, this argument that the new artistry will be in creating the prompts.

**00:36:40** There's something, I don't know, there's something very odd to me about that argument.

**00:36:44** It kind of reminds me of a while ago when AI systems started becoming the best chess

**00:36:50** players in the world.

**00:36:52** There was this, what to me always seemed like an absolutely bizarre idea that the chess,

**00:37:00** it was, I forget what they called it, but it was, it was this concept of like, oh, the

**00:37:03** best chess player will be a hybrid chess player, that it will be a person who is being advised

**00:37:10** by the computer.

**00:37:11** And there was a period in time where that was true, that a tag team of a human and a

**00:37:16** computer could beat the best computer and they could beat the best human.

**00:37:20** That always seemed to me like a strange artifact, like this won't exist forever.

**00:37:24** This just happens to be the situation right now, but I see no reason why the computers

**00:37:29** won't just ultimately outclass the human.

**00:37:32** And the human will just be like a monkey, right?

**00:37:35** Adding absolutely nothing into this incredibly complicated game that's taking place.

**00:37:39** And it's not, it's not the best comparison, but I feel that there is something in this

**00:37:45** concept that people have of artistry will be the prompts.

**00:37:50** That's the same.

**00:37:51** If the computers are getting so good at text, at the same time that computers are getting

**00:37:56** so good of interpreting text to create art, why will they not just meet?

**00:38:00** Yeah, maybe, maybe that's what it is, is it's like, why do you think you will be the best

**00:38:07** at coming up with the sequence of words that generates the most interesting art?

**00:38:12** Yeah.

**00:38:13** I just don't.

**00:38:14** Because similarly, like if this is the same argument of like, oh, the computer will make

**00:38:18** better art than any human ever can, right?

**00:38:21** Like that's kind of the thinking of like, oh, or just as good or good enough.

**00:38:24** Then why do you think computers won't be just as good as you are, but create and prompt?

**00:38:29** Yeah, yeah, maybe, yeah, maybe that's, that's, you've kind of sharpened it up there.

**00:38:33** I find that argument strange.

**00:38:35** And even if, let's say that for, you know, for whatever reason that was never, it turned

**00:38:40** out that just, that wasn't true for, for it's built into the laws of the universe that humans

**00:38:45** are just great at writing prompts in ways that machines will never be, which I think

**00:38:49** is strange, but let's just say it was true.

**00:38:52** At least in my experience of playing around with diffusion B, I just, I, I agree with

**00:38:57** you.

**00:38:58** I don't think there was really anything creative about what I was doing.

**00:39:00** It's problem solving.

**00:39:01** It's creative problem solving.

**00:39:03** And that's like a completely different thing to what I care about, which is the process

**00:39:09** of practicing and getting good at a thing.

**00:39:12** Like I think that that's really important.

**00:39:14** I think I'm going to do it again.

**00:39:16** I think it's part of the human experience.

**00:39:17** I do.

**00:39:18** And it doesn't need to be that everybody becomes a painter, but we all have these things in

**00:39:21** our lives that we practice and get better at.

**00:39:24** Right?

**00:39:25** It's like, why don't I just give my game controller to a robot and then just watch what it does?

**00:39:33** Just where we're going.

**00:39:34** It's like Wally, right?

**00:39:36** This is how we get to Wally.

**00:39:38** This is how Wally happens that we're all just sitting in these chairs.

**00:39:42** Like that's right now is the beginning of the path to Wally.

**00:39:47** Well, Wally, if we're lucky, but yes, uh, you know, it's just like, I feel like we have

**00:39:53** to do things, whatever it is, some kind of thing that we enjoy the process of.

**00:40:00** And I feel like if all we're doing is saying that like this stuff is just going to replace

**00:40:04** filmmaking, it's like, I don't want to watch those movies.

**00:40:08** I really don't, but I like to have to be sold in something.

**00:40:11** I like to believe that a human was involved in the endeavors that I'm consuming.

**00:40:15** Like, yeah, but again, maybe I'm old fashioned and I'm fine to accept that, but I'm just

**00:40:21** trying to get across like why these things are important to me, why I am so passionate

**00:40:26** about it.

**00:40:29** Yeah, I sort of have a minor point and a major point on that, that topic.

**00:40:34** I think, I think last time I made some offhanded remark of like very soon in major productions

**00:40:40** where we're going to see stuff that's AI generated and not know it.

**00:40:45** And I didn't realize at the time, but like, Oh, that was already true because I had watched

**00:40:53** the Obi-Wan Kenobi show on Disney and I didn't really think about it.

**00:40:59** But when I had watched Rogue One, I was very aware of listening to James Earl Jones do

**00:41:05** Darth Vader's lines of like, he's still Darth Vader, but he's getting too old for this.

**00:41:11** Like you can just hear it in someone's voice that they're just older.

**00:41:14** That's just what happens as your vocal cords physically change.

**00:41:18** And when I watched Obi-Wan Kenobi, I never thought about it.

**00:41:21** And I also didn't think about the fact that I didn't think about it.

**00:41:25** And I realized since we recorded that episode, Oh, I didn't think about it because all of

**00:41:30** his lines were done with AI.

**00:41:33** There's a program called re-speacher that will take a voice actors lines and redo them

**00:41:40** in the voice of someone else.

**00:41:42** And what's really quite remarkable about it is it isn't just what you sort of think like

**00:41:48** voice modulators on the phone like, Oh, I, you know, I can increase or decrease my pitch,

**00:41:52** but it still sounds like me.

**00:41:55** Everybody has vocal ticks and things like, no, no, no.

**00:41:59** Re-speacher will put in the vocal ticks of the other person.

**00:42:02** Like it's not just making your words sound in their voice.

**00:42:07** It's making them sound like them.

**00:42:09** Just as an interesting thing to note, though, I did, I did go back on YouTube and I watched

**00:42:13** some of the original line deliveries of James Earl Jones and the original Star Wars.

**00:42:18** It was like, Ooh, he is better here.

**00:42:20** Like he has like funny little things that he does with a bunch of words that make those

**00:42:23** line deliveries really great.

**00:42:25** And there isn't as much of that in the Obi-Wan Kenobi show.

**00:42:28** I will say when I watched Obi-Wan, there was something about Darth Vader's performance

**00:42:33** that I didn't like.

**00:42:35** And I assumed it was because James Earl Jones was getting older, right?

**00:42:40** But it turned out it wasn't the case.

**00:42:42** But there was there was something that felt missing to me.

**00:42:45** And I just put it down to and I remember saying to Adina at the time, I can't believe

**00:42:49** they're still getting this guy to do this.

**00:42:51** Why don't they just get like a voice actor?

**00:42:54** Right.

**00:42:55** Leave James Earl Jones alone.

**00:42:58** It was just kind of just like they had this isn't going to last forever with him.

**00:43:03** So like they need a path and they created one and it did a fine job, but it still felt

**00:43:08** like it was missing something to me.

**00:43:10** I can't put my finger on what it was.

**00:43:11** It just didn't feel right.

**00:43:12** And I don't know if this was part whatever.

**00:43:14** But what I will say, this particular implementation of AI, I'm fine with because there are like

**00:43:20** practical reasons for it of like if we want Darth Vader to sound the same, which I think

**00:43:26** ideally we do right.

**00:43:28** Like you could just get someone else to do it.

**00:43:29** But I would prefer it if he still sounded like James Earl Jones because it's like an

**00:43:33** existing character.

**00:43:34** And James Earl Jones signed this away as an individual while still alive.

**00:43:39** Right.

**00:43:40** Which is key.

**00:43:41** You can do this with you can take my voice and do whatever you want.

**00:43:44** Plus working voice actors continue to get jobs being Darth Vader.

**00:43:50** Then they change the voice.

**00:43:51** For me, there are enough pieces of this puzzle where I'm like, I am fine with this because

**00:43:57** people are involved in it.

**00:44:00** It's if Disney just feel like we've just decided we don't want to hire him anymore and we've

**00:44:08** created a thing and we're just going to type some text in and the A.I. is going to spit

**00:44:12** out the Darth Vader lines.

**00:44:14** I will be like, I don't like that.

**00:44:16** That doesn't feel good to me.

**00:44:18** But like the way they have done this whole thing going around, I'm kind of fine with

**00:44:23** like in the same way that I do find it kind of funny really that they continue to have

**00:44:28** Mark Hamill on set during the Mandalorian stuff to like be Luke Skywalker.

**00:44:35** Right.

**00:44:36** But then they completely digitally replace him with the younger version of him.

**00:44:39** Well, they do multiple things.

**00:44:40** They have him do a thing.

**00:44:42** Then they have a look alike act to do a thing.

**00:44:44** And then they put a digital recreation of his face on the young actor's face.

**00:44:49** Okay.

**00:44:50** Really, there is no point in him being here, right?

**00:44:52** We just like to have him around.

**00:44:53** I don't know.

**00:44:54** Like there is something funny to me about that.

**00:44:56** But at the same time, I'm like, at least it is a respect of the person they are digitally

**00:45:01** recreating.

**00:45:03** Everyone is in on this.

**00:45:05** That's fine.

**00:45:06** Yeah.

**00:45:07** You know, I suspect that they're also using re-speech or to make Mark Hamill's voice sound

**00:45:11** like young Mark Hamill because that's again like he does not sound like he did when he

**00:45:15** was in his 20s.

**00:45:16** But you also doesn't sound like him now either.

**00:45:18** Right.

**00:45:19** So they're doing something.

**00:45:20** Yeah.

**00:45:21** Something is happening there.

**00:45:22** But this is also one of these key things of like, okay, it's interesting to realize I

**00:45:25** had already watched an entire show where a major part of it was AI created and I didn't

**00:45:30** even I didn't even notice or it didn't even cross my mind.

**00:45:33** But it also touches on like what you're saying here is some of the key differences between

**00:45:40** what's happening in different parts of the world and a lot of the AI art developments

**00:45:45** on the Internet are just like a like a crazy Wild West where people are just grabbing whatever

**00:45:51** they can.

**00:45:52** And it's like that makes me extremely uncomfortable.

**00:45:55** Like okay.

**00:45:56** So again, I will just say in general ways I saw a demo of a Siri like voice assistant

**00:46:03** that was significantly better than things that currently exist.

**00:46:07** Oh, that's it was very interesting.

**00:46:09** But it was also using the voice of a famous actress just in their demo mode.

**00:46:16** And I got very uncomfortable about that because it's like she's up.

**00:46:21** She's a person who doesn't know that you've made a machine that can make her say anything.

**00:46:28** Hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey,

**00:46:32** that's what we want.

**00:46:33** Yeah.

**00:46:34** And it's like, no, no, no.

**00:46:35** And so a lot of the AI art stuff just feels like that.

**00:46:37** Like it's just grabbed everything that exists.

**00:46:41** And the thing that I keep thinking of is like it it makes a marionette of everyone who's

**00:46:49** ever put out any kind of art in the public.

**00:46:53** And that's that's like a horror, right?

**00:46:57** That's just a completely horrifying concept that if you have a social media timeline where

**00:47:02** you've posted stuff, guess what?

**00:47:05** There's enough information where someone can make a marionette of you and it can do

**00:47:09** anything.

**00:47:10** And right now, like this can happen without someone's permission.

**00:47:13** And yeah, it's totally why like the Darth Vader stuff I have no problem with because

**00:47:17** presumably James Earl Jones was like fantastic.

**00:47:20** I saw an article with that.

**00:47:21** That was like I read about that.

**00:47:23** It was the case that he approved this side of the rights like he's good with it.

**00:47:26** There's a big distinction between giving permission for something like this to be done and just

**00:47:34** the power of AI to make a marionette of anyone.

**00:47:39** And there's something really horrifying about that.

**00:47:42** Now I think it becomes like other people brought this up as as a as a kind of question.

**00:47:48** There's a boundary that's crossed between doing it to a living person, which is is super

**00:47:54** bad.

**00:47:55** It's like, yeah, I just think that's really awful and immoral.

**00:47:59** It's less bad when the person's dead, but it can still be bad if if it's recent.

**00:48:06** I don't quite know what recent means in this context.

**00:48:09** But clearly, when we get to a point where we're talking about modifying Leonardo da Vinci's

**00:48:17** art, I don't think there's anything bad there about like making a marionette of Leonardo

**00:48:22** da Vinci, making a machine that can spit out a ton of Leonardo da Vinci style paintings.

**00:48:27** I don't think it's possible to ascribe some kind of rule onto this.

**00:48:32** You just feel it when you feel it.

**00:48:33** Like, I can't tell you what the right time period is.

**00:48:37** And it's also different in every case.

**00:48:40** And, you know, goes on and on forever.

**00:48:44** Like the Fast and Furious movie, right?

**00:48:49** Where Paul Walker had just died, right?

**00:48:51** And it was like months later when the movie came out, but they used some digital recreation

**00:48:55** of his face to be able to give him like a tribute and send off.

**00:49:01** And I think that there is a little like it's awkward.

**00:49:05** I think that movie came out too soon, really.

**00:49:07** But the whole family kind of seemed to agree.

**00:49:10** And there's something about like, that was the thing.

**00:49:13** Same as like Princess Leia, right?

**00:49:15** Yeah, that's the thing.

**00:49:16** Princess Leia's send off was the next one after Carrie Fisher had died.

**00:49:20** Whereas still, I feel like we're on a fine line here because it's weird.

**00:49:25** It was recreated from stuff that she did participate in shooting.

**00:49:29** And at least there's like a tribute.

**00:49:31** But then if like Princess Leia just continued being in character after that, be like, no,

**00:49:35** you pushed it too far now.

**00:49:36** Right?

**00:49:37** Because now that's the making of the marionette, right?

**00:49:39** Like she's dead now, but you're continuing to use her.

**00:49:43** That would be weird.

**00:49:45** Like there, I don't think it's possible to put rules on like the death of a person.

**00:49:51** And I just know that I don't find it particularly comfortable for someone to profit off either

**00:49:59** financially or publicity wise.

**00:50:02** Somebody who is dead, like taking their actual work and again, divorcing the work required

**00:50:09** in the copying.

**00:50:11** Right?

**00:50:12** And just straight up like I took this thing and I made this thing out of it.

**00:50:17** Like I didn't really have to do anything other than put this image into the AI.

**00:50:23** There's an uncomfortableness for me in that.

**00:50:25** Yeah.

**00:50:26** I just wanted to highlight that there's a, there's a, like with many discussions, you

**00:50:31** can run into areas where there's some kind of spectrum.

**00:50:34** Yeah.

**00:50:35** Like someone taking the works of Shakespeare and putting that into an AI and making more

**00:50:39** Shakespeare.

**00:50:40** I don't care about that, but I can't tell you why I don't care about it either.

**00:50:43** It's done.

**00:50:44** Yeah.

**00:50:45** I do want to also add that this, this dances around a concept that I've been, I've been

**00:50:50** thinking about for years and I have, I have a hard time articulating, but for now I will

**00:50:54** just call it as the importance of saying no in art.

**00:51:00** And I think a lot of artistic projects or the life of creative people are defined as

**00:51:08** much by what they did as what they didn't do.

**00:51:10** A thousand no's for every yes.

**00:51:13** Yeah.

**00:51:14** That's another way to put it, but I think that having spoken to creative people, a lot

**00:51:20** of them have some internal set of things that they don't do.

**00:51:26** That's not obvious in the work that they create, but it lends a kind of character to their

**00:51:31** work.

**00:51:32** I've also just recently went through this where I killed a, what was going to probably

**00:51:37** be like a 20 or 30 minute video that was all storyboarded and written out.

**00:51:42** And part of the reason I killed it is because I realized this was a project to say no to

**00:51:48** that like, yes, I could put it out and I think people would like it, but it, it violated

**00:51:54** a couple of my own internal, like, I don't want to do this sort of thing in this way,

**00:52:00** even if no one in the audience would have, uh, would have noticed as like, man, that

**00:52:05** was, that was a hard decision, but I think that that's what matters.

**00:52:09** And so even while I think that there is, there's nothing morally wrong about making

**00:52:16** new Shakespeare plays or new Leonardo da Vinci paintings with AI, I do think that there's

**00:52:22** something about this concept of making an artist do anything that you want demeans the

**00:52:34** limits that that person put on their own things.

**00:52:37** And I'm also like, not sure why you even want it.

**00:52:40** Well, I think this is also just a side effect of how easy it can be and will be to create

**00:52:46** this stuff.

**00:52:47** We're just like out of curiosity, you type some prompts in.

**00:52:50** I guess it is different though, right?

**00:52:52** But I mean, it's like when this gets good enough that it can just like make a movie

**00:52:55** where some things happen.

**00:52:56** I don't know this idea of the importance of saying now is also connected to a thing that

**00:53:01** I see with a lot of particularly TV shows and long running creative projects where I

**00:53:07** think of this as you can see the audience influence the creators that never works.

**00:53:14** And there's lots of shows where it's like, Oh, this is the moment I can see now that

**00:53:19** the creators are aware of the audience.

**00:53:22** And with creative projects, the audience will ask for things that they want, that they don't

**00:53:29** realize in the long run actually destroys or makes worse the thing that they want.

**00:53:36** And like that sort of, I think that can sound super snobbish, but I think it's true.

**00:53:39** And I've seen it in enough projects was like the audience wants something and the creator

**00:53:44** then goes, Oh, I'll do that thing.

**00:53:46** And the audience responds and you get into this little positive feedback loop of doing

**00:53:50** the things that the audience wants.

**00:53:53** And none of those decisions mattered individually, but cumulatively, they can make a thing much

**00:53:58** worse in a way that's hard to pin down.

**00:54:00** And I just think there's something in AI art where even when there's no problem, I think

**00:54:07** people can like destroy the things that they love because there's no one to say no, there's

**00:54:14** no creator who says, these are the limits of my thing.

**00:54:19** And again, I keep thinking of the Miyazaki movies as a particular example of this.

**00:54:24** Like if you can make a Miyazaki movie about anything you want, it kind of destroys what

**00:54:29** those movies are, unless moved by the argument about it's important for humans to do these

**00:54:35** sorts of things.

**00:54:37** But I can easily imagine a situation where even if the AI is able to make amazing art,

**00:54:44** it's actually kind of worse for everyone involved, even though it's a thing that the audience

**00:54:50** has asked for or that people go like, oh man, I wish I could continue the series of

**00:54:54** movies forever and I can do it by typing into the machine and it will make the movie for

**00:54:58** me.

**00:54:59** It's like, you'll ruin the thing you love by doing that, by getting what you want all

**00:55:03** the time without a creative mind to say like, no, this is the limits of this project or

**00:55:09** no, like this is when the story stopped.

**00:55:11** Yeah, I mostly agree with that statement.

**00:55:15** We've spoken about this a lot, right?

**00:55:17** That idea of the point where the artist becomes aware of the audience.

**00:55:23** We speak about this a lot in regards to like just positive TV show recommendations.

**00:55:28** And like, you know, I know you are very sensitive to this, like you're something that I'm familiar

**00:55:33** with, like you point this out and I'm like, oh, okay, that's an interesting idea for why

**00:55:37** the show may have gotten bad in that season.

**00:55:39** Like it's not really something I would have thought of.

**00:55:41** And I would say that I mostly agree with your thinking here, but not completely.

**00:55:47** I think my kind of interpretation of this idea that you have is like just to add the

**00:55:53** word like mostly to it.

**00:55:56** Yeah, yeah.

**00:55:57** Right?

**00:55:58** Like that I don't, and I'm not saying that you say this, but the way that it sounds makes

**00:56:01** it sound like a definitive, like that listening to the audience or seeing what the audience

**00:56:07** want can never make something better.

**00:56:10** But I don't think it's that way.

**00:56:11** And I'm not sure you think it's that way, but I just want to state that right?

**00:56:15** We're like, yeah, yeah, in our work, there is definitely feedback which helps make something

**00:56:21** better and I do this all the time.

**00:56:24** But there are sometimes people will ask for me to talk about a thing and I just know it

**00:56:30** won't be interesting because I'm not that person to do that.

**00:56:35** Right?

**00:56:36** And that tends to be a lot of it for me of like, you're telling me you want a thing.

**00:56:40** I know you won't want it because I know it's not going to be interesting because either

**00:56:46** I don't find it interesting or I don't have the knowledge or whatever it might be, you

**00:56:51** know, like that there is that part of the artist of, oh God, I just called myself an

**00:56:56** artist.

**00:56:57** There is that part of the creative person, right?

**00:57:00** Where they are aware of what they're good at and what they're not and what they think

**00:57:04** will be ultimately enjoyable for this type of stuff that they create.

**00:57:10** Yeah.

**00:57:11** And then they go out and make it.

**00:57:12** Obviously like this, this is a different kind of thing.

**00:57:14** I also think it's the bounds of the project and I found I just recently stumbled upon

**00:57:21** to me what is just the most perfect example of the artist saying no, which is also a little

**00:57:29** bit heartbreaking for reasons that will be obvious what I what I mentioned who it is.

**00:57:33** J.R.R.

**00:57:34** In.

**00:57:35** I just discovered I can't believe I never knew this.

**00:57:37** He started a book that was set after the Lord of the Rings.

**00:57:44** And so he wrote the first couple of chapters of of this story.

**00:57:48** And it's it's quite interesting, but like he basically he then wrote about he wrote

**00:57:57** in one of his letters.

**00:57:58** You can get these books of all of his private letters we discuss as a bunch of his thoughts.

**00:58:01** He wrote about why he didn't finish this.

**00:58:04** And the reason he didn't finish it is because, oh, in his timeline after the Lord of the

**00:58:10** Rings, like there's not really much magic left in the world.

**00:58:14** He had an idea of who the story would be about.

**00:58:16** But basically everybody magic is gone and it's just the world of men.

**00:58:20** And he said like, oh, I could have written it like a totally fine adventure book.

**00:58:27** But that's but that's not what the world I was trying to create was about.

**00:58:31** Like he was trying to create this mythology and writing another story.

**00:58:37** That's what happens after the mythology.

**00:58:39** He was like, wait, why am I doing this project?

**00:58:41** This isn't really what I want to do.

**00:58:43** Sure.

**00:58:44** We can continue what happens with a bunch of these characters, but I'm going to say

**00:58:47** no.

**00:58:48** And so he just like stopped writing it and said, no, I'm not going to write anymore.

**00:58:51** This is the stories of what happens after.

**00:58:52** I'll just fill in some of the details of what happened before.

**00:58:55** And I think that's an amazing example and reading his letter about it.

**00:58:58** It's just he's a very self-aware.

**00:59:01** Sure.

**00:59:02** The audience would love this.

**00:59:03** Everybody's asking for like a sequel to Lord of the Rings because it was so great.

**00:59:07** But it's also so great because he knew where to stop.

**00:59:10** I just think that's an interesting example.

**00:59:12** And it would just make me very sad if people were typing into AI generator machines like

**00:59:18** give me Tolkien's next book.

**00:59:19** Like, oh, no, please don't.

**00:59:22** Please don't do that.

**00:59:23** It's bad.

**00:59:24** This episode of Cortex is brought to you by Squarespace, the all in one platform for building

**00:59:29** your brand and growing your business online.

**00:59:31** You can stand out with a beautiful website, engage with your audience and sell your products,

**00:59:36** services, the content that you create, whatever it is.

**00:59:40** Squarespace has got you covered.

**00:59:41** No matter what type of website you're looking to make, whether it's something for a personal

**00:59:45** event, maybe you have a business site that you want to create, it doesn't matter.

**00:59:49** Squarespace can help you.

**00:59:50** They've got you covered.

**00:59:51** And it starts with one of their beautiful templates.

**00:59:54** They're best in class.

**00:59:56** They're award winning.

**00:59:57** It is easy as browsing the type of site or category of business you want to make a website

**01:00:01** for.

**01:00:02** And that will give you the perfect starting place for you to then customize just a few

**01:00:05** clicks and really make it feel like your own.

**01:00:09** And then once your website expands and maybe you want to add that online store, you can

**01:00:13** sell whatever you want.

**01:00:15** Physical digital goods.

**01:00:16** It doesn't matter.

**01:00:17** They have all of the tools that you need to start selling online.

**01:00:20** And then as your website grows and your visitors start pouring in, you can use insights to

**01:00:25** grow your business.

**01:00:26** If you want to know where your site visits are coming from, where your sales are generating

**01:00:30** from, which channels are most effective for you, you can analyze all of this in Squarespace.

**01:00:35** Then when you have the data, you can improve your website and build a marketing strategy

**01:00:39** based on your top keywords or the most popular products and content that you have available.

**01:00:43** I love Squarespace.

**01:00:45** I have been using Squarespace for probably nearly 15 years now for various projects,

**01:00:51** and I wouldn't go anywhere else.

**01:00:52** When I want to put something online, they're the first place that I go.

**01:00:55** I know how to use it.

**01:00:56** It's so easy.

**01:00:57** Everything looks great on every type of device.

**01:00:59** They have awesome apps for you to manage everything.

**01:01:02** It's the full package.

**01:01:03** I love it, and I'm confident you're going to love it too.

**01:01:06** But you don't have to just take my word for it.

**01:01:08** Go to squarespace.com slash cortex, and you can sign up for a free trial.

**01:01:12** No credit card required.

**01:01:13** You can go in and build your whole website.

**01:01:15** Then when you want to launch it to the world, use the offer code Cortex, and you will get

**01:01:19** 10% off your first purchase of a website or domain.

**01:01:22** Go to squarespace.com slash cortex, and then when you sign up, use the offer code Cortex

**01:01:26** to get that 10% off your first purchase and show your support for the show.

**01:01:30** A thanks to Squarespace for their continued support of Cortex and Relay FM.

**01:01:34** Hey, do you want to talk about this podcast that you found here, Mike?

**01:01:38** So I referenced why I'm not worried about my job specifically.

**01:01:43** This has been going around a lot.

**01:01:44** It is an AI Steve Jobs on an AI Joe Rogan episode.

**01:01:52** It is a full podcast created by an AI.

**01:01:55** I've not really bothered to look into this because I actually think it might be part

**01:02:00** of a publicity stunt.

**01:02:02** But nevertheless, it is a thing that exists.

**01:02:05** It's 20 minutes long, and you can listen to it, and I've skipped around in it.

**01:02:09** And do you know what?

**01:02:10** It really sounds like Steve Jobs.

**01:02:12** There are points where you can hear that it's not real, right?

**01:02:16** And ultimately, my takeaway from this is, who wants this?

**01:02:19** Genuinely, who wants this?

**01:02:21** Who wants to hear what Steve Jobs might have said to a fake Joe Rogan?

**01:02:26** And for me, I don't feel concerned about my job because, all right, you can take a version

**01:02:32** of me and a version of you, and we can have them make podcasts forever.

**01:02:37** But I kind of feel like if that's what you want, I don't know how much you could enjoy

**01:02:44** the content because it's not real.

**01:02:47** It's not real conversations.

**01:02:49** That's what I do for a living, is real conversations between real people about things that they

**01:02:53** care about.

**01:02:54** And my assumption is the majority of people that listen to my shows want to hear that

**01:03:00** rather than let's imagine what two AI might be talking about instead.

**01:03:06** For me, that's just so broken from what I imagine people want the content for.

**01:03:12** And look, if you are that person, you don't need to tell me.

**01:03:15** Of course I know you exist, right?

**01:03:17** I'm sure that there are people that would like to just have us on in the background

**01:03:20** so they could go to sleep, and it's just like we're just going to have an infinite amount

**01:03:23** of episodes forever.

**01:03:26** But realistically, I don't imagine that that's going to be a thing that people genuinely

**01:03:33** care about enough in the way that people might care about the content that I make.

**01:03:37** So just climbing across this one episode, it honestly made me, even though it showed

**01:03:42** me it can be done, it made me feel more secure in my own profession.

**01:03:46** Honestly, in the last episode, I wasn't worrying about my own profession.

**01:03:50** It was more about the idea of creativity in people.

**01:03:54** That's what I care more about.

**01:03:55** You said this to me, and I took a listen.

**01:03:56** I did the same thing.

**01:03:57** I didn't listen to the whole thing all the way through.

**01:03:59** I sort of skipped around.

**01:04:01** I have a slightly different take on this sort of stuff, which is it doesn't matter how good

**01:04:06** Steve Jobs on Joe Rogan is to listen to.

**01:04:10** It's more of just a demonstration of proof of concept of this thing is possible.

**01:04:15** And once you have a demonstration of proof of concept, guess what?

**01:04:18** Things only get better.

**01:04:19** They don't get worse.

**01:04:21** So this was just like the first real proof of concept demonstration of two people having

**01:04:26** a podcast conversation.

**01:04:27** I do have to say, listening to it, I think the Steve Jobs was less good, but like, holy

**01:04:33** moly did it nail Joe Rogan's way of talking.

**01:04:36** That makes sense though, right?

**01:04:38** The amount of source material for Joe Rogan is almost infinite, really.

**01:04:42** Yeah, I just thought like it's actually interesting to listen to because you can almost hear the

**01:04:46** fact that there's what, a hundred million hours of Joe Rogan talking.

**01:04:51** And you know, the database for Steve Jobs is so much smaller.

**01:04:55** And you can hear that in the two voices is like, especially because I think Rogan has

**01:05:00** a funny circular way of talking sometimes that it's like, I would never not know that

**01:05:04** that wasn't him.

**01:05:06** If you told me, oh, it was Rogan, but it was only just an AI Jobs, I would have believed

**01:05:10** it.

**01:05:11** This to me is, is again, a good example of this thing where making a marionette of someone

**01:05:17** is just bad.

**01:05:18** It's like, Hey guys, whatever you think of Rogan, he's still a person and it's real

**01:05:22** bad to make like a fake show where he's talking and it's less bad for Steve Jobs because he

**01:05:31** is dead, but it's still close enough that it makes me very uncomfortable.

**01:05:35** I just think we're going to see an increasingly large number of these sorts of things where

**01:05:40** people can make whatever they want.

**01:05:42** And it's only going to become increasingly easy over time.

**01:05:46** And I don't know how this project was made in particular, but yeah, it's the first example

**01:05:51** of, Oh, it's an AI podcast with two people where we have enough information to recreate

**01:05:57** them in some sense.

**01:05:59** Yeah.

**01:06:00** It's a public is done for the company.

**01:06:01** The company that made this has an AI text to voice generation system that they are trying

**01:06:07** to sell.

**01:06:08** Yeah.

**01:06:09** But it's, it's also interesting just how fast so much of this is being commercialized.

**01:06:16** I stumbled across a, um, I thought, Oh God, like how brutal is this?

**01:06:21** But it's, it's a company that makes AI people who will read quite convincingly scripts where

**01:06:27** it looks like it's a talking head segment.

**01:06:29** And the whole idea is, Oh, you can have all of your corporate training material delivered

**01:06:33** in this way, where there's like an AI person who will talk through whatever it is you need

**01:06:37** to onboard your new employees.

**01:06:39** There's something about that to me, which is like, it's very convincing, but it also

**01:06:42** feels like what a horrible dystopian nightmare.

**01:06:45** It's like you as an employee signed up with the company is like, Oh, Hey, we didn't even

**01:06:51** take the time to film a person going through our own training materials.

**01:06:56** We just gave it to an AI and they made a fake person who you get to listen to explain your

**01:07:00** job to you.

**01:07:01** Enjoy your training.

**01:07:02** I know it's just so we value you.

**01:07:06** I just want to read from the podcast AI, that kind of description of the show, whether

**01:07:11** you're a machine learning enthusiast, just want to hear your favorite topics covered

**01:07:16** in a new way, or even just want to listen to voices from the past brought back to life.

**01:07:21** This is the podcast for you.

**01:07:23** All right.

**01:07:26** Do you want to hear our ghoulish marionettes say what we say what we made them say.

**01:07:30** It's just like, if you are a fan of Steve jobs, which I think is who they are pitching

**01:07:34** this to, right?

**01:07:35** Why do you want to hear him talk about things he never spoke about?

**01:07:38** Like, what do you value from that other than just like hearing the voice?

**01:07:42** Like, if you just want to hear his voice, just go to YouTube and watch like commencement

**01:07:46** speeches or interviews or whatever.

**01:07:48** Like I don't understand why you want to hear him talk about things he never spoke about.

**01:07:55** It's not his opinion.

**01:07:58** There is no opinion of him in this.

**01:08:00** They've built all of this from things he said, but you're just taking words he used

**01:08:05** and just putting them together in a new way.

**01:08:07** It's not actually his opinion.

**01:08:10** And I just, I find it so strange.

**01:08:15** Since the last episode, I have much more strongly onboarded the concept of try to only read

**01:08:23** and consume and listen to and watch media that you know has been produced by human.

**01:08:29** How did that do for you?

**01:08:30** If I'll be, you know, I mean, it's it's sneaks in there.

**01:08:35** At least it's like, oh, I know humans made the show.

**01:08:38** Yeah, at least you got that part.

**01:08:41** Right.

**01:08:42** Yeah.

**01:08:43** Maybe not all humans, all of it all the time, but like enough.

**01:08:45** Yeah.

**01:08:46** And so again, like this idea that went from like, that's crazy to, oh, I guess this is

**01:08:50** wise advice to live by has accelerated quite quickly in my life.

**01:08:54** And yeah, it's kind of a weird.

**01:08:57** Oh, make sure you know it's a person who wrote or made this thing.

**01:09:01** So yeah, I am trying to onboard that as a concept because I think not onboarding that

**01:09:06** as a concept is part of what is going to lead to AI Doom for us all.

**01:09:12** Oh, God, we still want to talk about AI.

**01:09:14** All right.

**01:09:15** Yep.

**01:09:16** Okay.

**01:09:17** This episode of Cortex is brought to you by Fitbot.

**01:09:20** Between balancing your work life, your family life and everything else going on in your

**01:09:23** life, it can be hard to make fitness a priority.

**01:09:26** Your fitness shouldn't be about competing with people.

**01:09:30** What you need is a program that is tailored for you, working with you and not against

**01:09:34** you.

**01:09:35** And that's why you need Fitbot.

**01:09:37** You don't want to have to look to other people and do what they're doing.

**01:09:40** You don't want to have to be told exactly what to do.

**01:09:42** You don't want to have to do these boring things every single day.

**01:09:45** You don't want that rigid structure.

**01:09:47** You want something that is flexible, something that adjusts, that is dynamic, that is your

**01:09:51** own fitness plan that you can have access to from anywhere.

**01:09:56** In a beautiful app of wonderful HD video tutorials shot from multiple angles so every exercise

**01:10:02** that you want to learn is super simple.

**01:10:04** This is all why you need Fitbot.

**01:10:07** They use data to create and adjust that plan for you.

**01:10:11** It's crafted to be unique for you exactly.

**01:10:15** That app is super easy to learn.

**01:10:17** It integrates with your smart watch, your Apple watch, your Wear OS watch.

**01:10:21** It integrates with apps like Strava, Fitbit and Apple Health.

**01:10:25** Everything is super awesome.

**01:10:26** I love Fitbot.

**01:10:27** It really set me on a different path to my fitness journey when I started using it.

**01:10:32** What I really liked was that there was a variety there.

**01:10:34** That variety was super important to me.

**01:10:36** Previously, I'd used apps and videos and stuff.

**01:10:39** It's just the same thing every day and a fun that really boring.

**01:10:41** But I was learning new things.

**01:10:43** While at the same time, it's also mixing up with those variety of exercises so you're

**01:10:48** not overworking or underworking anything.

**01:10:51** It's really clever.

**01:10:53** Personalized training at this quality can be expensive.

**01:10:55** Fitbot is just $12.99 a month or $79.99 a year.

**01:10:58** You can get 25% off your membership by going and signing up today at fitbot.me slash cortex.

**01:11:04** Go now and get your customized fitness plan at fitbot.me slash cortex.

**01:11:09** That is 25% off that you will get by signing up at fitbot.me slash cortex.

**01:11:15** 25% off.

**01:11:16** A thanks to Fitbot for their support of this show and Relay FM.

**01:11:20** Yes, I do want to talk about Doom for us all, but partly because it's clearing up a little

**01:11:26** bit of confusion from last time as well.

**01:11:29** One of the things we discussed last time was stable diffusion, making images like, here

**01:11:36** is the filming of NASA landing on the moon and creating fake stuff.

**01:11:42** And I think a totally fair criticism of that that I saw from people is, hey, Photoshop

**01:11:48** exists, you know, like we can make those images now.

**01:11:52** We don't need an AI art system to generate them for us.

**01:11:56** If you would be convinced by a good Photoshop, what's the difference from being convinced

**01:12:01** by an AI art system about something that isn't real in the world?

**01:12:06** So what I want to portray here is I think Doom comes in like three phases.

**01:12:12** Phase one, I think will be just pollution of the public information space.

**01:12:18** I think it's sort of a general confusion.

**01:12:22** And what makes the difference between AI art and something like Photoshop is the scale

**01:12:29** and the cost.

**01:12:31** So right now, if someone wants to try and put out a bunch of misinformation or create

**01:12:38** evidence to back a conspiracy theory, there still needs to be effort that goes into creating

**01:12:44** that thing.

**01:12:45** Like you were saying, Mike, you need to learn these skills as part of your art for how to

**01:12:50** create a piece of misinformation to put out in the world.

**01:12:54** And I think what I kind of expect if I'm projecting a bunch of this stuff forward is the scale

**01:13:01** of this potential misleading images and misleading text that were AI generated is just vastly

**01:13:09** beyond what we can imagine now.

**01:13:12** And I think in the modern world, like a lot of people have been driven kind of crazy just

**01:13:20** from the selection of what information they're presented.

**01:13:25** Like people on social media can kind of like drive themselves crazy by just going down

**01:13:29** rabbit holes and being continually presented with information that agrees with them.

**01:13:34** And that's just by like selecting the things that they're seeing that actual human beings

**01:13:39** have created.

**01:13:40** But what I'm kind of thinking might happen here is that when you're able to generate

**01:13:45** a huge amount of content, just as right now, companies intentionally A-B test what they

**01:13:53** show you for engagement, not even on purpose, but AI generated content will effectively

**01:14:01** be unintentionally A-B tested for convincingness.

**01:14:06** How convincing is this sequence of words in whatever idea it's trying to spread?

**01:14:13** And I really do view a lot of this stuff as the kind of concept of memes, of ideas.

**01:14:21** They evolve and they spread and they mutate and they're spreading in the world.

**01:14:26** Doesn't have anything to do with how true they are.

**01:14:29** It has to do with how convincing they are.

**01:14:32** Imagine hooking up Twitter's algorithm to a text AI.

**01:14:36** Yeah, I think that's going to happen.

**01:14:38** Every time you pull to refresh, it just gives you a bunch of other nonsense.

**01:14:43** That is truly horrifying to me as a thought.

**01:14:47** I think we're going to see that.

**01:14:48** I think we're going to see that or we're going to see something very much like it.

**01:14:51** I mean, look, the AI video stuff that we saw before, do you think TikTok won't start doing

**01:14:58** that the moment that it becomes engaging to people?

**01:15:01** That's a really good point.

**01:15:02** That's actually the most likely of all of them, I think, to occur.

**01:15:06** To just automatically generate 20 second, 30 second videos every time a person refreshes

**01:15:12** and just keep doing it for whatever keeps that person engaged.

**01:15:16** I think we're going to see that even if it's not the companies directly doing it themselves,

**01:15:22** you'll have entities on Twitter where it's like, oh, it's a bot, but it's acting like

**01:15:26** it's a person and it's just existing in the world and it's doing this kind of unintentional

**01:15:30** A B testing for convincing this and the ones that are more convincing for whatever reason

**01:15:36** will just spread better.

**01:15:37** So I really think that is a kind of like.

**01:15:43** I think a lot of the criticism is people want to know, like, great, like you love technology.

**01:15:48** Why aren't you behind this stuff?

**01:15:49** I used to be a real technological optimist, but I've changed my mind on a bunch of that

**01:15:54** stuff.

**01:15:55** And this is one area in particular where and I want to be clear here.

**01:16:00** I don't think it's malicious.

**01:16:01** I don't think it's necessarily that someone's out there trying to do bad, but I think the

**01:16:07** ability to create hundreds of millions of memes in the way of like just a concept that

**01:16:15** can spread at the drop of a hat is just bad for the public information space.

**01:16:22** And the world has barely survived social media in some ways.

**01:16:27** Like that has made stuff so bad with how people think about the world.

**01:16:31** And this is that just taken to the next level by a huge order of magnitude.

**01:16:37** So I suspect that'll be one of the first ways that this becomes obviously bad over

**01:16:45** time.

**01:16:46** Like once it progresses out of the stage of just being an interesting toy.

**01:16:50** My hope is that for whatever reason, we're in the part of the technological development

**01:16:56** where it looks like it's an exponential graph, but it's rapidly going to level off at an

**01:17:01** S curve and we discover, oh, there's parts of this that were way harder than we thought.

**01:17:07** That's where I hope this is going.

**01:17:08** But you show me the signs of that one on the next episode.

**01:17:12** Well, I'm in this position where I feel a little bit like, oh, I'm like a crazy person

**01:17:18** talking into a microphone about this stuff.

**01:17:20** But I have spoken to some of the top people in the world in this area and they're very

**01:17:29** concerned is the way that I would put it.

**01:17:32** And talking to people, I was trying to tease out this concept of where do we think we

**01:17:37** are on this curve?

**01:17:38** Is this the start of the exponential or are there obvious problems ahead?

**01:17:42** And the answer was pretty universally, oh, as far as we can tell, the exponential has

**01:17:48** barely begun because the thing part of the reason it's making such fast progress is because

**01:17:56** the work being done right now is still in the realm of, oh, hey, you get into work.

**01:18:00** And what's the first thing you think of that could make this better and you try it and

**01:18:04** it makes it better?

**01:18:05** That's just like an indication that we're at the start of an exponential curve.

**01:18:08** So yeah, I think we're going to have a bunch of confusion about this stuff.

**01:18:12** I think that transitions into genuine economic problems as particularly language models get

**01:18:22** better and better and better at doing all the kinds of work that humans do, which is

**01:18:29** largely knowledge work.

**01:18:31** And it's funny, I made Humans Need Not Apply eight years ago now, I think.

**01:18:37** And it's been on the back of my mind about revisiting that at some point.

**01:18:41** And I was kind of thinking, oh, I don't know how relevant this video still is, but having

**01:18:48** rewatched it, it's like, oh, no, I put it back to be featured on my channel under one

**01:18:52** of the most watched videos because it's like, no, no, all this AI stuff and all of the language

**01:18:56** stuff like makes this way more concerning.

**01:18:59** I think the lesson learned there has simply been that physical automation is slower to

**01:19:05** progress for a bunch of reasons, but all of the knowledge worker stuff is coming along

**01:19:10** very, very fast.

**01:19:12** I just know that there are companies that are very explicitly targeting low level knowledge

**01:19:16** work and then we'll be progressing further and further up the chain as fast as they can

**01:19:21** with better and better language models to do all sorts of things that people can do.

**01:19:25** I understand that lots of people just fundamentally disagree with me on this point of economics

**01:19:30** that jobs can't be replaced because humans have infinite needs and always want more things.

**01:19:39** I understand that argument.

**01:19:40** I just don't agree with it.

**01:19:41** And I think AI just breaks some of the fundamental assumptions that are built into that model

**01:19:46** of, as we get better machines of all kinds, we just increase the quality of life and increase

**01:19:52** our desires.

**01:19:53** I just don't think that that's universally true.

**01:19:56** So yeah, I think we're going to end up with some really major problems in the economy,

**01:20:02** particularly in the knowledge worker part of that economy.

**01:20:06** And look, we don't need to talk about it today because we've talked about it enough.

**01:20:10** But I think once you start encroaching on AI systems that are good enough to replace

**01:20:15** most human work, you really start encroaching on the kinds of things that can lead to the

**01:20:21** extinction of the human race.

**01:20:24** Maybe that's a little too heavy for today, but that's kind of my having thought it through

**01:20:28** of like, what are the three phases of where does this go?

**01:20:31** It's confusion of the public information space, destruction of the economy, extinction of

**01:20:36** the species.

**01:20:37** It's one, two, three.

**01:20:38** Oh boy.

**01:20:39** How you feeling, Mike?

**01:20:42** Not great.

**01:20:43** No?

**01:20:44** No, I didn't want to do another hour and a half on this, to be honest.

**01:20:49** But I guess this is who we are now.

**01:20:52** Yeah.

**01:20:53** Can I just ask you, though, like, how crazy does that sound to you?

**01:20:55** No, it doesn't.

**01:20:56** And that's why I don't like it.

**01:20:58** I don't have optimism about this area of technology.

**01:21:02** I do not think that this is a thing that will produce much good.

**01:21:05** We can leave it there for now, then.

**01:21:07** On with the show.

**01:21:08** It's only been an hour and a half.

**01:21:17** Just some quick follow up from the last thing we talked about on the previous episode.

**01:21:21** Okay.

**01:21:22** On the lighter side of things, what is a podcast, Mike?

**01:21:25** Why?

**01:21:26** Why am I doing this one again?

**01:21:29** What is this?

**01:21:30** You just relive the horrors of the previous episode?

**01:21:33** I feel so badly for you, Mike.

**01:21:35** Last episode, you stumbled into a terrible mistake, which was attempting to describe.

**01:21:41** But what is a podcast?

**01:21:44** Well, the real issue was I put two topics together that seemed related, but they were

**01:21:49** unrelated.

**01:21:50** Right.

**01:21:51** Yeah.

**01:21:52** This is always the dangers of speaking extemporaneously and you're just sort of like in the middle

**01:21:56** of a conversation and you say some things.

**01:21:58** And so anyway, we ended up talking for a while about like, but what is a podcast?

**01:22:03** What must it be in order for it to be the platonic ideal of a podcast?

**01:22:08** This caught people's attention.

**01:22:10** And you can click the link in the show notes.

**01:22:12** A cortex sent in what I absolutely adored is this podcast alignment chart.

**01:22:19** And I read this and was like, I think I agree with absolutely everything on this chart.

**01:22:24** This nails what I wanted, I think to kind of get across, but maybe it didn't do a great

**01:22:28** job of doing.

**01:22:29** I don't know.

**01:22:30** So in the great tradition of memes, there's a meme where people make the alignment chart

**01:22:35** for various things.

**01:22:37** And I think this started with what is still the fantastic example of the sandwich alignment

**01:22:42** chart.

**01:22:44** So you have two axes in the chart for the sandwich, which is ingredients and structure.

**01:22:51** So it ranges from like ingredient purist to ingredient rebel.

**01:22:56** And then you have structure purist.

**01:22:57** A sandwich must have the classic sandwich shape to pieces of bread, but with topics

**01:23:02** in between.

**01:23:03** And then like structure rebel, any kind of food enveloped in any way is a sandwich.

**01:23:08** Oh, like a hot dog.

**01:23:09** Yeah.

**01:23:10** So this is like, is a hot dog a sandwich?

**01:23:13** And hot dog falls on the sandwich alignment charts of ingredients neutral and structure

**01:23:17** neutral.

**01:23:18** Wait, this is a very intriguing way that you've introduced this.

**01:23:22** Isn't this just like the chaotic evil, chaotic, good thing?

**01:23:25** Yes, that's where this comes from.

**01:23:26** That's the origin.

**01:23:27** Oh, right.

**01:23:28** I misunderstood you and thought you were saying that the sandwich one was the origin.

**01:23:32** I don't think you're right there.

**01:23:33** No, no, no, no, no, no.

**01:23:37** I always forget because I never played the chaotic neutral, chaotic, good.

**01:23:40** That's the no, but the two axes, it's it's evil and good and then lawful and chaotic.

**01:23:45** Is that that's that's the two axes on the traditional one?

**01:23:47** I think so.

**01:23:48** But this sandwich alignment chart is the meme, right?

**01:23:51** Which is like, oh, here is from which other things birth.

**01:23:54** And so what I love on the Internet is people will get into an argument about like, what

**01:23:59** is thing is a hot dog, a sandwich is a Pop-Tart, a sandwich is a chicken wrap, a sandwich.

**01:24:04** And someone will come up with like, here's the alignment chart to try to describe where

**01:24:09** all of these things fit in.

**01:24:10** And so someone did this for podcasts, the podcast alignment charts.

**01:24:15** The two axes are distribution method and media type.

**01:24:20** So do you want to run through some of these, Mike?

**01:24:22** Where do you fit on the podcast alignment chart?

**01:24:25** Oh, man.

**01:24:26** I think I was.

**01:24:28** I feel like you were making an argument for traditional distribution method, traditional

**01:24:33** media type, which is an audio RSS feed is a podcast.

**01:24:37** No, because I don't I think I am a modernist distribution modernist media type traditionalist.

**01:24:44** Any audio that's subscribable is a podcast.

**01:24:47** No, I think I'm modern modern.

**01:24:49** Any audio or video that's subscribable is a podcast.

**01:24:52** Oh, so my YouTube channel is a podcast.

**01:24:54** That's what you're saying.

**01:24:55** Yeah, no, that I'm modern traditionalist because that's the thing of like, but it gets into

**01:25:01** an issue for me is that I watch videos that I consider podcasts.

**01:25:06** It's the any I would see I would phrase it as any audio or video that's subscribeable

**01:25:12** can be a podcast.

**01:25:14** That will be my personal definition, I think will be closer to how I feel.

**01:25:20** But I think I'm more traditionalist modernist, which is any audio that's subscribeable is

**01:25:24** a podcast.

**01:25:25** No, but then it's got audio books in here.

**01:25:27** This is very complicated.

**01:25:28** I don't I would say any audio or video that somebody wants to call a podcast that you

**01:25:34** can subscribe to can be a podcast.

**01:25:36** I think that's how I actually personally sit now.

**01:25:40** I think that works for me.

**01:25:42** I really love these charts go kind of crazy.

**01:25:45** I think the problem with the podcast one is a two dimensional surface is not enough to

**01:25:50** express the entirety of what it wants to be.

**01:25:53** You need a third axis, which I think is something like consumption intention.

**01:25:59** Yeah.

**01:26:00** So for me, like I would be like a traditionalist here, right?

**01:26:03** That the consumption intention is audio only like that.

**01:26:08** That to me is a really key characteristic of what is a podcast?

**01:26:13** It has to be intended as an audio first experience, which isn't quite captured on this chart.

**01:26:20** But then it can be a video.

**01:26:22** It can be a Spotify exclusive.

**01:26:25** Yeah, I see.

**01:26:26** I don't know.

**01:26:27** I know.

**01:26:28** I am very sympathetic to the position you were expressing last time, which is there's

**01:26:33** or at least I thought it was the position you're expressing last time that there's something

**01:26:38** about it needs to be generally accessible.

**01:26:43** I think for me, the example in my mind that really sticks out is Audible does these things

**01:26:48** that they call podcasts, which are like little shows that you can subscribe to in the Audible

**01:26:52** app.

**01:26:53** I don't know why they call them podcasts.

**01:26:54** I don't get that one at all.

**01:26:56** I've listened to some of them like they're fine.

**01:26:58** I haven't found one that's amazing or anything, but they're also I don't know.

**01:27:02** There's some part in my brain which is just like this is not a podcast at all.

**01:27:05** Just original audio content.

**01:27:06** Yeah, it's an audio.

**01:27:07** And they do call them Audible originals.

**01:27:09** Yeah.

**01:27:10** And they call them podcasts.

**01:27:11** And I just wonder if maybe they should just stick to one of those descriptors.

**01:27:16** Yeah.

**01:27:17** But anyway, like those are the ones that really clang in my brain of like, but what's the

**01:27:21** key thing there?

**01:27:22** And I think the fact that they're only available on audio really makes them not a podcast.

**01:27:26** Okay.

**01:27:27** Well, then Joe Rogan.

**01:27:29** Yeah.

**01:27:30** Is that a podcast?

**01:27:33** Damn you, Mike.

**01:27:36** Right.

**01:27:37** Because I would say yes, it is even though it's a Spotify exclusive.

**01:27:41** Ah, curse you.

**01:27:44** This is the perfect counter example to my position.

**01:27:48** So that's why I feel like for me, there is the third axis, which is like intent.

**01:27:55** And that's when if we go on this third axis of intent, I would sit in the modernist where

**01:28:00** it could be like any audio or video subscriber will can be a podcast, but it's on creators

**01:28:05** intent.

**01:28:06** Yeah.

**01:28:07** But see, look, just to make things hard, it has to be phrased in order to fit the meme

**01:28:11** as is a podcast.

**01:28:13** You can't put in these weasel words of can write the purpose of the alignment chart is

**01:28:17** to definitively answer what is a podcast.

**01:28:19** Well, I'll tell you now it doesn't.

**01:28:22** I really love looking at this chart that traditionalist traditionalist is an audio RSS feed is a podcast.

**01:28:29** And then you then you start moving in these reasonable directions of any audio that's

**01:28:33** subscribable is a podcast.

**01:28:35** And then as you get into the radicalist ones, it just gets very funny, right?

**01:28:39** Any audio available on the internet is a podcast.

**01:28:43** Anything that you can subscribe to is a podcast.

**01:28:46** And my personal favorite, the radicalist radicalist is anything on the internet is a podcast.

**01:28:53** Which made me laugh so hard.

**01:28:56** Yeah, that got me.

**01:28:59** Because you know what, like that fits with some people that I've come across in my life.

**01:29:05** You know what I mean?

**01:29:06** Like, yeah.

**01:29:07** I also enjoyed the cortex is like giggling over this one where people are like this comment

**01:29:11** on this Reddit thread is the podcast, right?

**01:29:13** Because I'm a radicalist radicalist.

**01:29:16** I thought this was fantastic.

**01:29:17** And thanks to XD 1936 for posting it.

**01:29:21** It got a really good laugh out of it.

**01:29:23** And I think, yeah, it was just a fun way to try to encapsulate the conversation from last

**01:29:28** time.

**01:29:30** This episode of cortex is brought to you by Wealthfront.

**01:29:32** Is your bank keeping money that could be yours?

**01:29:35** If you're earning less than Wealthfront's 2.55% APY, they might be.

**01:29:40** Here's why.

**01:29:41** Federal interest rates have been going up this year, which means banks have had the opportunity

**01:29:44** to earn more on your savings.

**01:29:46** Where's all the extra money going?

**01:29:48** Well, according to the FDIC, the average US bank has only raised their rates to 0.17%

**01:29:55** this year, while Wealthfront is now offering their clients a rate that's about 15 times

**01:29:59** higher with the Wealthfront cash account.

**01:30:02** Of course, you have a choice.

**01:30:03** You could let your bank keep the extra money they're making, or you could move your savings

**01:30:08** to a high yield account like Wealthfront and earn 2.55% APY.

**01:30:13** I think that having your savings in order is really just a great thing.

**01:30:16** It's one less thing to have to think about.

**01:30:18** So I think you should check it out.

**01:30:19** It's super easy to sign up.

**01:30:21** Plus, you get unlimited transfers that are completely fee free and up to $1 million in

**01:30:26** FDIC insurance through partner banks.

**01:30:28** There are no account fees, no minimum balance.

**01:30:31** And if you sign up at Wealthfront.com slash Cortex, you get a free $50 bonus of an initial

**01:30:36** deposit of $500.

**01:30:38** Don't let your bank keep the interest you could be earning.

**01:30:40** Join nearly half a million people who already use Wealthfront to earn 15 times more than

**01:30:45** the average US bank.

**01:30:47** That's Wealthfront.com slash Cortex to get started and earn your free $50 bonus.

**01:30:52** That's Wealthfront.com slash Cortex.

**01:30:55** This has been a paid endorsement for Wealthfront.

**01:30:56** Our thanks to Wealthfront for their support of this show and Relay FM.

**01:31:01** We are hurtling towards the end of the year.

**01:31:03** Yes, alarmingly fast.

**01:31:05** And this is the final regular episode of Cortex this year.

**01:31:09** Don't remind me, Mike.

**01:31:10** So if you have more to say about AI...

**01:31:12** Oh God, Gray, where's AI art going to be in January?

**01:31:16** Oh my God.

**01:31:17** By January, we're going to be at step two of my three steps to the apocalypse.

**01:31:21** Excellent.

**01:31:22** Can't wait for it.

**01:31:23** So maybe that's why we had to spend the best part of two hours doing follow up because

**01:31:28** we're not going to do it now.

**01:31:30** Yeah, I guess so.

**01:31:31** Let's say that.

**01:31:32** That was the plan.

**01:31:33** We didn't just end up talking about it for forever.

**01:31:35** There is a possibility we might do something more, Tex.

**01:31:38** Getmoretex.com.

**01:31:39** I can't say that for sure because I don't know what we're going to do over the next

**01:31:42** two months except for what I know we're going to do, which is state of the apps, November,

**01:31:47** yearly themes, December.

**01:31:49** So on that note for yearly themes, I wanted to take this as a time to suggest a method

**01:31:56** to people for preparing for their yearly theme.

**01:32:00** If you don't know what a yearly theme is, I will put a video in the show notes where

**01:32:03** Gray explains what yearly themes are, but we're going to get to it again, obviously,

**01:32:08** in December.

**01:32:09** So I don't want to talk about it now.

**01:32:11** But I am personally at the point of my year now where I'm preparing my yearly theme.

**01:32:17** The way I do this is I have it in my mind to start noting down things that I'm happy

**01:32:24** about in my work life and personal life and things that I find frustrating in those as

**01:32:30** well and things I would like to change.

**01:32:33** So I keep this as an Apple note.

**01:32:35** It's in my yearly themes, Apple note, but I have it just playing in my mind that as

**01:32:40** I'm bumping into things that are good or bad, that I would like to continue or stop, I write

**01:32:46** them all down.

**01:32:47** I just write them down.

**01:32:48** I don't need to draw any comparisons to them yet.

**01:32:50** I just start writing them down.

**01:32:52** So I end up with a list of things, maybe 20 things or so over the space of a few weeks

**01:32:58** or a month that are in my head.

**01:33:00** And some of them, they're not even things I bump into, but because I'm in this kind

**01:33:03** of reviewing mode, things just pop up in my head randomly.

**01:33:07** I'm like, oh, that's a thing.

**01:33:08** I'll write that down.

**01:33:10** Then as we get towards November or so, I start to review this list of things and note the

**01:33:16** similarities where I can tie some similarities between things.

**01:33:19** Be like, oh, that's actually related to this or that is similar to this.

**01:33:24** I then use these links as the basis to build my theme.

**01:33:27** So what do I think I could do next year to address some of these common issues that I

**01:33:34** have or these common things that I would like to do more of?

**01:33:38** So if you struggle to come up with creating yearly theme, this is my top tip.

**01:33:43** It's how I do mine every year.

**01:33:45** I don't know if this is asking for spoilers for the theme episode, which I don't want,

**01:33:49** but I just I feel like I would like some concrete examples of the kinds of things that you're

**01:33:52** writing down that you end up thinking about.

**01:33:55** And they don't they don't have to be examples for this year.

**01:33:57** I will give you some examples for this year, but I'm not going to give you so many that

**01:34:01** I think was ball my theme.

**01:34:03** I want to spend more time on product creation and design.

**01:34:07** That's one.

**01:34:08** I want to listen to more music.

**01:34:10** I'm happy with how I've looked after my health and want to do more of that.

**01:34:14** I want to be smarter about my scheduling and the days that I'm in the office.

**01:34:18** These are the kinds of things I write down.

**01:34:20** OK, so presumably then the way you have this happen is there's some frustration about the

**01:34:27** scheduling and then I guess you notice that and add it to this file.

**01:34:31** Is that is that the system?

**01:34:32** OK, like all these are at the moment.

**01:34:35** It's just it's just an outline like bullet points of things.

**01:34:38** Right.

**01:34:39** And I write a bunch of notes down and write some context down.

**01:34:41** And I start reading through these and I'm like, oh, hang on a minute.

**01:34:44** This can relate to this and this.

**01:34:47** Well, like, hey, I've written a bunch of times here that I'm unhappy about this kind of thing

**01:34:52** and like a bunch of different ways.

**01:34:55** So can I address that thing?

**01:34:58** OK, actually interesting.

**01:35:00** I think I should frame this thought a bit differently because I have I have a notes

**01:35:04** file where when it occurs to me, things that I may want to discuss on the theme episode,

**01:35:10** I make a note of them.

**01:35:11** I guess this is just a different kind of the same idea of like things about my own personal

**01:35:16** theme there.

**01:35:17** But I like that as as a have a place to collect what you notice about your life, which also

**01:35:26** then just encourages you to notice more might be a way to try to pitch it to people who

**01:35:32** are as the end of the year comes hurtling towards us.

**01:35:35** And if they've never tried a theme before, might want to think about that over the next

**01:35:40** two and a half months, they're going to start a theme at the beginning of the year.

**01:35:43** It's like just have a note somewhere to put down things that you notice about your life

**01:35:48** as a way to get started.

**01:35:49** And then you have something to look over and try to synthesize later.

**01:35:54** It's not about any particular moment.

**01:35:56** OK, yeah, that's interesting.

**01:35:58** I think this is makes something that's more likely to stick on because it's actually related

**01:36:03** to things that you've experienced and pushes it further away from the New Year's resolution

**01:36:08** idea because New Year's resolutions are typically created from whole cloth like this.

**01:36:12** Just like I have this aspirational idea of myself that I would like to be this different

**01:36:16** kind of person and I'll just go and live that life.

**01:36:18** Right.

**01:36:19** What I'm suggesting that you do is how do you live right now?

**01:36:24** What frustrates you about things that are happening right now?

**01:36:27** What would you like to be better in your actual life right now?

**01:36:31** And try and think about what that might be.

**01:36:33** It's related to something that's actually happening rather than something that you look

**01:36:37** forward to just imagine yourself as a different person.

**01:36:40** Yeah, that's a good distinction because again, also the New Year's resolutions have the feeling

**01:36:44** to me of homework assignments that you forgot about until the morning of and it's like,

**01:36:48** oh no, I need to write an essay about what I'm going to do this year.

**01:36:51** Whereas this makes me think of it's actually kind of popping into my head the way some

**01:36:56** researchers have tried to study how happy people are by messaging them at random points

**01:37:01** throughout the weeks or the years of just like, hey, right now, how do you feel?

**01:37:06** And this is closer to that.

**01:37:07** It's like a continual process of, hey, notice in your life how you're feeling and just make

**01:37:11** a little note of it.

**01:37:12** And then later you can have some stuff to review.

**01:37:14** So I like that.

**01:37:15** I think that's good.

**01:37:16** I think that's a good suggestion for people looking for a place to onboard where to start

**01:37:22** instead of just trying to create a theme out of whole cloth after they've listened to the

**01:37:26** theme episode.

**01:37:27** Oh, Gray, there's something I've got to tell you about before we wrap up today.

**01:37:32** Yeah.

**01:37:33** I didn't know about this.

**01:37:35** It somehow seemed to have slipped by everyone.

**01:37:38** In iOS 16, there is now a Dvorak software keyboard on the phone.

**01:37:45** Oh, yeah?

**01:37:46** Yeah.

**01:37:47** Let me grab my phone here.

**01:37:50** It's in the settings app, right?

**01:37:52** You have to go to settings and then I think it's general.

**01:37:56** It's got a general keyboard.

**01:37:58** Keyboard.

**01:37:59** And then whatever you're setting is like English UK or English US or whatever.

**01:38:03** Right.

**01:38:04** And at that you get the option of QWERTY, AZERTY, QWERTS and Dvorak.

**01:38:07** Oh, I see.

**01:38:08** So it's not a new keyboard.

**01:38:10** It's under, I would never have found that.

**01:38:12** You could probably add it as a new keyboard.

**01:38:14** No, it's not.

**01:38:15** That's what I was just trying to do.

**01:38:16** Oh, oh God, I hate it.

**01:38:18** Oh, interesting.

**01:38:19** Wow, that's horrifying.

**01:38:20** Do you still use Dvorak?

**01:38:21** Of course I use Dvorak, Mike.

**01:38:24** So your like keyboard on your desk is set in the Dvorak layout.

**01:38:30** I feel like we haven't spoken about that in a really long time.

**01:38:32** I've forgotten.

**01:38:33** Yes.

**01:38:34** Like my, well, my keyboard, if you look at it, the current one I'm using is it has a

**01:38:39** QWERTY layout, but the keys are mapped to the Dvorak settings.

**01:38:44** You are a monster.

**01:38:45** No, no, no.

**01:38:46** I actually, I've decided that's best.

**01:38:48** That's the better way to go.

**01:38:49** I've stopped getting keyboards.

**01:38:51** Look, I think there's only two ways to go.

**01:38:54** QWERTY visual layout or blank, right?

**01:38:59** Like nothing on any of the keys.

**01:39:01** Why can't you use the Dvorak visual layout?

**01:39:05** I think I could build you a keyboard one day that would be laid out this way.

**01:39:09** Yeah.

**01:39:10** Okay.

**01:39:11** I guess here's, I'm trying to, I was just trying to articulate why after, after years

**01:39:16** and years of doing all sorts of different things, why have I settled on this?

**01:39:20** The answer is that keyboard shortcuts are funny in a lot of applications.

**01:39:27** So there's something in the system level where a keyboard shortcut will either trigger

**01:39:34** based on the letter that the key represents or it will trigger based on the location that

**01:39:40** the key is.

**01:39:41** You can see this explicitly in some things like final cuts where you can reprogram the

**01:39:47** key based on letter or location.

**01:39:50** So I guess the way I use it is that it is, I'm just not having, I'm struggling trying

**01:39:56** to think of a specific example that's not final cut, but I know it comes up where it's

**01:40:00** useful to be able to see what does the keyboard look like for everyone in the whole wide world,

**01:40:08** sometimes for keyboard shortcuts.

**01:40:09** Like that does come up where I am glad that the layout is a QWERTY layout.

**01:40:14** And since I touch type, I don't look at the keyboard.

**01:40:16** It doesn't matter when I'm actually writing what the keys look like at all, which is why

**01:40:21** I have gone with a completely blank keyboard sometimes, which I do think is cool, but can

**01:40:25** be annoying for keyboard shortcuts.

**01:40:27** I have a question for you.

**01:40:28** Yeah.

**01:40:29** Well, because one, I'll come back to the keyboard shortcuts thing because I know how to fix

**01:40:32** that.

**01:40:33** Depends on what you use.

**01:40:34** When you are using your Dvorak layout, what is the copy command?

**01:40:40** Okay.

**01:40:41** So for me, it's command I is what it would look like on a QWERTY keyboard.

**01:40:46** That's copy.

**01:40:47** Oh, because, okay.

**01:40:48** Yes.

**01:40:49** I know what you're saying now.

**01:40:51** You are actually hitting still command C though, is what I'm asking.

**01:40:56** That's kind of more work, but it's in a different place.

**01:40:58** Okay.

**01:40:59** It's in a different place.

**01:41:00** So one of the things that exists now, which I don't know if it's actually a good idea

**01:41:05** for any other lunatics who might want to switch their keyboard layouts.

**01:41:09** One of the things that computers have gotten better at over years is there are now explicit

**01:41:13** layouts which are on the Mac.

**01:41:15** It's called something like Dvorak preserve shortcuts.

**01:41:20** So it's like you type in Dvorak, but it totally ignores the keyboard layout for all keyboard

**01:41:25** shortcuts.

**01:41:26** It would be like command J looking at the Dvorak layout, right?

**01:41:32** In that scenario where it's preserving the location of the key.

**01:41:36** Yeah.

**01:41:37** It's funny.

**01:41:38** It's like, I can't look, I is C. What's the, what's the complication here?

**01:41:43** I'm looking at Dvorak layout on Wikipedia and like, you know, J is in the, is in the

**01:41:48** location that C is on a QWERTY keyboard.

**01:41:51** And so you would hit, I guess, command J. Because what it's saying is the way I've understood

**01:41:56** the way you've explained that is like it's keeping the physical location of the key,

**01:42:01** no matter what the key actually says is.

**01:42:03** Yeah.

**01:42:04** Okay.

**01:42:05** So I just looked it up.

**01:42:06** Like what is the actual thing called in the system setting?

**01:42:08** So on the Mac, you can set it as something called Dvorak QWERTY command.

**01:42:13** So I presume that what it's doing there is whenever you hit command, it ignores the Dvorak

**01:42:17** layout and reverts to the QWERTY layout.

**01:42:19** That seems horribly complicated.

**01:42:20** Yeah.

**01:42:21** All of this to say one day, because I want to build your keyboard one day, just for fun.

**01:42:27** I would choose a keyboard where I can change the programming of the keyboard.

**01:42:32** Like this is the thing that exists.

**01:42:33** Like the boards that I build these days, they tend to all support a piece of software called

**01:42:37** VIA, which overrides the layout on the keyboard.

**01:42:43** So it's not relying on the Mac.

**01:42:46** So if you hit any shortcut in any app, it's going to register as the key that is on the

**01:42:53** Dvorak layout.

**01:42:55** It doesn't matter that you would try and do some kind of key binding and software.

**01:43:00** Does that make sense?

**01:43:02** Yes.

**01:43:03** Right.

**01:43:04** So like one of the things you're saying is like, for example, if you hit some kind of

**01:43:07** keyboard shortcut in Final Cut, it's going to assume that's QWERTY, even if you set to

**01:43:12** Dvorak, because there's like a software in between the two of them is trying to communicate

**01:43:16** it.

**01:43:17** But I'm saying the hardware of the keyboard would communicate to the computer.

**01:43:24** So I am convinced it would get it correctly.

**01:43:27** It would always hit correctly.

**01:43:29** Okay.

**01:43:30** I feel like I'm not 100% understanding, but there's also like so many layers here.

**01:43:34** Yes.

**01:43:35** One of the other problems is I don't know if any of the Cortexons are able to help me

**01:43:38** out with this.

**01:43:39** If anyone's already done this.

**01:43:41** Is the maximum level of crazy, right?

**01:43:43** I use the Dvorak layout, but all I've learned all the keyboard commands, all the basic system

**01:43:49** commands with the Dvorak layout.

**01:43:51** So I'm not pressing the regular buttons that everybody knows.

**01:43:55** However, in Final Cut, it the way that the programmers laid out all the keyboard shortcuts

**01:44:03** that do things, it makes sense physically where they are on the keyboard.

**01:44:08** Whereas like command copy command paste, like it doesn't really matter if those two

**01:44:11** are next to each other, but there's tons of stuff in Final Cut, which is like trim from

**01:44:16** the start of the clips, trim from the end of the clip.

**01:44:19** And you want to have those keys like next to each other on opposite sides.

**01:44:22** There's a lot of like physicality of this.

**01:44:24** So in Final Cut, I've been trying to slowly build up my own custom mapping, which is to

**01:44:31** make it so that when I am using the Dvorak layout, it's still acting as though it's a

**01:44:37** QWERTY layout just for the Final Cut shortcuts.

**01:44:41** So every time I try to learn a new Final Cut shortcut, I try to go in and like change what

**01:44:46** it is.

**01:44:47** But I've wondered like, has someone just done this?

**01:44:49** Like is there a QWERTY for Dvorak remapping of all the Final Cut shortcuts that someone

**01:44:54** has just done?

**01:44:55** Because I could tell like I'm getting into a situation of, oh, this is a little bit inconsistent.

**01:45:00** So anyway, I've just wondered if like a person has done that work for me.

**01:45:03** And there's also some weird system stuff that I've done with a few of the shortcuts.

**01:45:06** Anyway, there's a bunch of these weird little problems.

**01:45:09** And I also have this minor annoyance.

**01:45:11** Listen, before I say this annoyance, I understand.

**01:45:14** I understand why Apple did it.

**01:45:16** You don't have to leave me comments for why they did it.

**01:45:18** I understand.

**01:45:19** It's still annoying.

**01:45:20** But it used to be that Apple, when you had different keyboard layouts, it had a little

**01:45:26** country flag to represent each of the layouts.

**01:45:29** And so the US keyboard layout had a little US flag.

**01:45:33** And the Dvorak one was just DV.

**01:45:36** And they made the change so that they don't use country flags for languages.

**01:45:43** Now again, I get it.

**01:45:45** But it actually causes a huge annoyance because in my menu bar, I want to see which layout

**01:45:51** the keyboard is using.

**01:45:52** Is it using Dvorak or is it using US?

**01:45:55** Because sometimes I do switch between those two layouts.

**01:45:58** And now I can't visually see instantly that the keyboard is on US layout versus Dvorak

**01:46:03** layout.

**01:46:04** And it's maddening because it's just a little box.

**01:46:07** And the box either says DV or it says US.

**01:46:10** And like, no, it was so much better when it was black DV and that's Dvorak.

**01:46:15** And then there was a little American flag when it's the US layout.

**01:46:18** So it's super frustrating.

**01:46:20** But anyway, all of that aside, I'm looking at this Dvorak layout on my phone and I hate

**01:46:25** everything about it.

**01:46:27** I don't think Dvorak was made for phones.

**01:46:29** I think all of the advantages for typing Dvorak with two hands, I don't feel like they translate

**01:46:35** for two thumbs on a phone.

**01:46:38** I think maybe the QWERTY layout is actually superior for the phone.

**01:46:41** Yeah, all of the vowels are next to each other on the Dvorak one.

**01:46:45** I'm just trying to type some words here.

**01:46:47** By the way, I have two applications for you that can restore those flags.

**01:46:52** Oh, really?

**01:46:53** Yeah, one is called keyboard switcheroo.

**01:46:56** The other is called colorful input menu flags.

**01:46:59** They're both in the Mac app store.

**01:47:01** Oh, OK.

**01:47:02** Sold.

**01:47:03** Two enterprising developers who created applications to bring that back.

**01:47:07** I'd like to thank Glenn Fleischman at Macworld for writing the article.

**01:47:12** Because I know I'd seen these and so I just did a quick Google while you were upset.

**01:47:18** And actually on keyboard switcheroo, they have in their app screenshots US, French, German

**01:47:25** Dvorak.

**01:47:26** You get a little US flag or a little DV for Dvorak.

**01:47:29** Oh, good.

**01:47:30** I'm glad to know I'm not the only person who was annoyed by this.

**01:47:32** So I'll definitely check those out.

**01:47:34** Thank you.

**01:47:35** You can always rely on Mac developers to solve the little annoyances.

**01:47:39** Yeah, you can.

**01:47:40** What I'm just thinking here is with iOS, I think QWERTY on the phone, I think it does

**01:47:45** make sense.

**01:47:46** I feel like the weirdness of the way QWERTY is laid out to sort of slow you down, although

**01:47:52** I think that that's overplayed as a story of why it exists.

**01:47:55** I think that actually works as an advantage for typing with your thumbs on a tiny phone

**01:47:59** screen.

**01:48:00** But I am glad that this exists for the iPad.

**01:48:03** I might actually switch it on the iPad because that's the place where it's been annoying.

**01:48:07** Big keyboard.

**01:48:08** And that's more like a larger keyboard there is what you're more used to for Dvorak, right?

**01:48:12** Bigger keyboards like a computer keyboard.

**01:48:14** Yeah.

**01:48:15** Can you get MacBooks in Dvorak?

**01:48:17** I don't think so.

**01:48:18** I don't think they sell that.

**01:48:21** I'm pretty sure you have to just pick between American layout and the English layout when

**01:48:26** I'm buying the keyboards, which is the thing I'm always very careful about.

**01:48:30** Like, please give me the US layout one.

**01:48:32** I do not want the UK layout one.

**01:48:34** There's no little honor there.

**01:48:35** Like the American enter is clearly superior.

**01:48:38** I build all of my keyboards in NC.

**01:48:40** My laptop is in the British layout, but I think in the future I might try and remember

**01:48:46** to just order them in US layout because that's what I'm much more used to now.

**01:48:49** Wait a second.

**01:48:50** You build your keyboards in the US layout, NC layout.

**01:48:54** Yeah.

**01:48:55** But you get the MacBook keyboards with the English layout?

**01:48:58** I don't think about it when I'm buying them.

**01:49:01** I think I will now in the future, right?

**01:49:03** Because I'm getting more annoyed because I'm switching between something I'm not used to.

**01:49:09** The keyboards that I build, most of them are just more available and easier to build in

**01:49:13** NC layout.

**01:49:14** So I'm just used to it now.

**01:49:15** So the US English layout.

**01:49:18** I never think about it.

**01:49:19** And obviously the default on the British Mac store is the British layout.

**01:49:24** So I don't think about it.

**01:49:25** And I just buy what's the default, but I think I want to start changing it.

**01:49:30** It doesn't look like they do Devorak.

**01:49:31** It's not in their options.

**01:49:32** I don't think I'll be using this on my phone.

**01:49:35** I think it's worse on the phone, but I will definitely try it out on the iPad when the

**01:49:38** iPad comes out because that's where it's more of a problem.

**01:49:41** But I mostly don't really think about it very much because it's just been so long.

**01:49:46** This has been my entire life.

**01:49:48** The last time it caused me problems was when I worked at a school and had to switch between

**01:49:53** the QWERTY and Devorak typing keyboards a bunch for using different computers.

**01:49:57** But even then it was not the worst.

**01:49:58** I just learned to touch type while not looking and then type while looking at the keyboard

**01:50:04** as two different modes that my brain could switch between.

**01:50:06** But I wonder why on earth they added Devorak to iOS 16 now.

**01:50:12** It's a very funny like...

**01:50:13** Why did it take 16 attempts?

**01:50:16** You know what I mean?

**01:50:17** Why 16 versions in?

**01:50:20** Is Devorak having some resurgence that we're not aware of?

**01:50:23** Why?

**01:50:24** It is odd, right?

**01:50:25** Yeah.

**01:50:26** I'm not even sure if I was suggesting to someone now who had RSI problems if Devorak is the

**01:50:32** way to go.

**01:50:33** I always forget.

**01:50:34** I think it's Colmak or something?

**01:50:36** There's another one which is definitely worth investigating if you're learning now versus

**01:50:41** Devorak.

**01:50:42** Also a feature that I don't know why but it took all the way until now for people to

**01:50:47** notice it, it seems like.

**01:50:48** There's been a bunch of articles written because it was kind of discovered but that means it

**01:50:52** went through the whole beta process without people seeming to know about it.

**01:50:56** I don't know.

**01:50:57** The thing that's also confusing about it is if they did add...

**01:51:02** Why didn't they add something like Colmak?

**01:51:03** I mean, look, I don't know a lot about programming iOS to add additional keyboard layouts but

**01:51:09** if you're going to add something like Devorak, how hard is it to also add the other popular

**01:51:16** one of these if you're going to do it?

**01:51:18** Well, isn't Devorak the popular one though?

**01:51:20** I don't know if that's true or not.

**01:51:22** I feel like it must be or at least in the requests that they get or some high up executive

**01:51:27** like Tim Cook is a Devorak guy and he'd had enough.

**01:51:30** Yeah, I mean with Devorak, I just always feel like there's dozens of us.

**01:51:36** Dozens.

**01:51:37** Literally.

**01:51:39** You've seen the Godfather, right?

**01:51:40** No.

**01:51:41** What?

**01:51:42** I've not seen the Godfather.

**01:51:43** I've just never really been that interested.

**01:51:46** Okay, well, in the Godfather, this is pointless for you now, but in the Godfather there is

**01:51:52** a moment...

**01:51:53** I think it's Godfather 2, which I assume you also have not seen because why would you have?

**01:51:59** Yeah, that's a correct...

**01:52:00** Yeah, I've seen the third one but not the first or the second one.

**01:52:03** You're joking, right?

**01:52:04** I am joking, yes.

**01:52:05** Good, good, good.

**01:52:06** There's a moment where a bunch of things happen and the line is we're settling all

**01:52:11** family business.

**01:52:13** There's a line about settling all family business and I feel like that is today's episode.

**01:52:18** We have just settled a bunch of business, right?

**01:52:20** A lot of follow-up has been dealt with, right?

**01:52:23** We've spoken about Devorak, right?

**01:52:25** Like we're just settling the business before the year ends for us, like very good episodes.

**01:52:30** Now we go to the specials.

**01:52:31** Yeah, that's true.

**01:52:32** There are two more pieces of family business that I would like to settle before we finish

**01:52:36** today.

**01:52:37** One is to thank every cortex and who donated to our Scentude campaign.

**01:52:42** Overall throughout the month of September, we raised $706,397.10.

**01:52:50** All of us absolutely astounded.

**01:52:53** This is the most money we've ever raised.

**01:52:56** It's unbelievable.

**01:52:57** It's an incomprehensible amount of money.

**01:53:00** So we have now not only passed $2 million raised in the last four years, we have now

**01:53:07** hit $2.2 million raised in the last four years.

**01:53:14** It's incredible.

**01:53:16** It's just incredible to think that the Relay listenership has raised that much money for

**01:53:21** Scentude.

**01:53:22** So yeah, thank you to everyone who donated to the campaign.

**01:53:28** That's completely mind-blowing.

**01:53:29** It seemed for a while that it was like, okay, we're going to raise a lot of money.

**01:53:33** We're going to meet our goal, but we're probably not going to exceed because there are definite

**01:53:38** economic challenges right now around the world.

**01:53:41** And we were thinking, okay, that's going to be that.

**01:53:44** And then it just, the last couple of days even, it just exploded because people were

**01:53:49** just getting their final totals.

**01:53:52** We were going to end up on September 30th, but we extended it to October 3rd because

**01:53:57** the money was piling in in the last day.

**01:54:00** It was like, all right, we're going to leave this open a couple of days.

**01:54:02** I think in the last day or two, we raised an extra $110,000 or something.

**01:54:07** So it was like, probably should keep this open.

**01:54:10** Yeah, that's worth leaving open an extra day.

**01:54:13** But overall, this year's campaign was fantastic.

**01:54:16** We learned a lot.

**01:54:17** And for me, it was just so incredibly rewarding and fun to be able to be back in Memphis for

**01:54:23** the podcastathon, which was a great success.

**01:54:26** The whole video is on YouTube.

**01:54:27** I'll put a link in the show notes.

**01:54:29** People don't want to watch it.

**01:54:30** A very kind comment, a timestamp to the whole thing.

**01:54:33** So if you want to jump around to different segments or whatever, that's all in there

**01:54:37** and one of the comments in the YouTube video.

**01:54:39** But yeah, it was truly fantastic.

**01:54:42** And we achieved something that I just did not think was going to be possible.

**01:54:46** And once again, the Relay FM community has gone and surprised us.

**01:54:51** Cortexmerch.com.

**01:54:52** This is the second thing, final thing.

**01:54:55** And what's the final piece?

**01:54:56** Is subtlety and subtle sweaters.

**01:54:58** We're reminding you, maybe you have now finished your commute and you're walking into the office

**01:55:01** or whatever and you didn't do what I asked you to do earlier.

**01:55:04** So now you can get your phone out and you can go to Cortexmerch.com and buy yourself

**01:55:08** a subtlety or subtle sweater or both or all or whatever you want.

**01:55:12** Unlike the St. Jude fundraiser, this will not be extended.

**01:55:16** So yeah, for realsies, Cortexmerch.com.
